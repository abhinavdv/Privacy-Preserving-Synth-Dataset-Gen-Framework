{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning LLM with Differential Privacy\n",
        "\n",
        "This notebook fine-tunes the **Llama-3.1-8B** model using **Low-Rank Adaptation (LoRA)** and **Opacus for Differential Privacy (DP)**. The goal is to train the model while ensuring privacy protection by preventing it from memorizing sensitive data.\n",
        "\n",
        "Key Steps:  \n",
        "- Load the training dataset (`train.jsonl`) containing structured prompt-response pairs.\n",
        "- Apply **LoRA fine-tuning**, which optimizes training efficiency while keeping parameters manageable.\n",
        "- Use **Opacus** to implement Differential Privacy (DP-SGD), adding noise to gradients to protect individual data points.\n",
        "- Track and log experiments with **Weights & Biases (wandb)** for reproducibility.\n",
        "- Save the fine-tuned model for later inference and evaluation.\n",
        "\n",
        "This ensures that the synthetic data generated later maintains **privacy, realism, and usability**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XXHWXGdJNhKu"
      },
      "outputs": [],
      "source": [
        "# RUN_MODE = \"test\"\n",
        "RUN_MODE = \"main\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "akOhzOdxoE-q"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "TRAIN_JSONL = \"data/train.jsonl\"\n",
        "TEST_JSONL = \"data/test.jsonl\"\n",
        "file_paths = [TRAIN_JSONL, TRAIN_JSONL]\n",
        "\n",
        "for path in file_paths:\n",
        "    if not Path(path).exists():\n",
        "        raise FileNotFoundError(f\"Error: {path} does not exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jMHg6ywOp9um"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "\n",
        "# folder_path = \"wandb\"\n",
        "\n",
        "# # Delete the folder and all its contents\n",
        "# shutil.rmtree(folder_path)\n",
        "\n",
        "# print(f\"Deleted folder: {folder_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwPM3hdJr1ez"
      },
      "source": [
        "## Requirements and dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HZof-WuyHdF2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install opacus\n",
        "# !pip install -U bitsandbytes transformers accelerate\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8Ga73J2NITU",
        "outputId": "60eae1db-d89a-4225-dbbe-992819a4b746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (12.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pynvml) (12.570.86)\n"
          ]
        }
      ],
      "source": [
        "!pip install pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCRKTux3HJuH",
        "outputId": "c6108aa7-a1ba-48d6-ff96-152c1de9ad45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy version: 1.26.4\n",
            "Using device: cuda\n",
            "GPU Device: NVIDIA L4\n",
            "Available GPU memory: 22.17 GB\n"
          ]
        }
      ],
      "source": [
        "from random import sample\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)  # Should print \"1.23.5\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.amp import autocast, GradScaler  # Import automatic mixed precision tools\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from opacus import PrivacyEngine\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Set up device - prioritize GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Print GPU info if available\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NSOP969tMhsE"
      },
      "outputs": [],
      "source": [
        "## Clear GPU cache and storage\n",
        "torch.cuda.empty_cache()  # Frees unused memory\n",
        "torch.cuda.ipc_collect()  # Collects shared memory used in multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "06uZqEV3rGAe"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5eyCXarG33",
        "outputId": "5f49182e-46d9-40a6-c10a-98321c8bdc74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged in successfully!\n"
          ]
        }
      ],
      "source": [
        "# Retrieve token securely\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"Logged in successfully!\")\n",
        "else:\n",
        "    print(\"Hugging Face token not found. Please set it in Colab.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8WJhCIZMuRB"
      },
      "source": [
        "## CPU and GPU util functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebizwy4ZMnlk",
        "outputId": "b5123f91-9553-4ff4-a021-6be4e02481be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”¹ CPU Stats:\n",
            "\n",
            "ðŸ”¹ RAM Stats:\n",
            "\n",
            "ðŸ”¹ GPU Stats:\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetUtilizationRates, nvmlSystemGetDriverVersion, nvmlDeviceGetName, nvmlShutdown\n",
        "    nvmlInit()\n",
        "    NVML_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NVML_AVAILABLE = False\n",
        "\n",
        "def get_cpu_stats():\n",
        "    \"\"\" Get CPU usage stats \"\"\"\n",
        "    cpu_usage = psutil.cpu_percent(interval=1)  # Get CPU usage %\n",
        "    cpu_freq = psutil.cpu_freq().current if psutil.cpu_freq() else \"Unknown\"  # CPU Frequency\n",
        "    num_cores = psutil.cpu_count(logical=False)  # Physical Cores\n",
        "    num_threads = psutil.cpu_count(logical=True)  # Logical Cores\n",
        "    print(f\"CPU Usage: {cpu_usage}%\")\n",
        "    print(f\"CPU Frequency: {cpu_freq} MHz\")\n",
        "    print(f\"Physical Cores: {num_cores}\")\n",
        "    print(f\"Logical Cores: {num_threads}\")\n",
        "\n",
        "def get_ram_stats():\n",
        "    \"\"\" Get system RAM stats \"\"\"\n",
        "    ram = psutil.virtual_memory()\n",
        "    print(\"Total RAM:\", round(ram.total / 1e9, 2), \"GB\")\n",
        "    print(\"Available RAM:\", round(ram.available / 1e9, 2), \"GB\")\n",
        "    print(\"Used RAM:\", round(ram.used / 1e9, 2), \"GB\")\n",
        "    print(\"RAM Usage:\", ram.percent, \"%\")\n",
        "\n",
        "def get_gpu_stats():\n",
        "    \"\"\" Get GPU stats if available \"\"\"\n",
        "    if not NVML_AVAILABLE:\n",
        "        return {\"Error\": \"pynvml not installed. Run: pip install nvidia-ml-py3\"}\n",
        "\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "\n",
        "    for i in range(num_gpus):\n",
        "        handle = nvmlDeviceGetHandleByIndex(i)\n",
        "        mem_info = nvmlDeviceGetMemoryInfo(handle)\n",
        "        utilization = nvmlDeviceGetUtilizationRates(handle)\n",
        "\n",
        "        print(f\"GPU {i} - {nvmlDeviceGetName(handle)}\")\n",
        "        print(f\"Driver Version: {nvmlSystemGetDriverVersion()}\")\n",
        "        print(f\"Total VRAM: {round(mem_info.total / 1e9, 2)} GB\")\n",
        "        print(f\"Used VRAM: {round(mem_info.used / 1e9, 2)} GB\")\n",
        "        print(f\"Free VRAM: {round(mem_info.free / 1e9, 2)} GB\")\n",
        "        print(f\"GPU Usage: {utilization.gpu}%\")\n",
        "        print()\n",
        "\n",
        "    nvmlShutdown()  # Clean up NVML\n",
        "\n",
        "# Run and print system stats\n",
        "\n",
        "print(\"\\nðŸ”¹ CPU Stats:\", )\n",
        "print(\"\\nðŸ”¹ RAM Stats:\", )\n",
        "print(\"\\nðŸ”¹ GPU Stats:\", )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykEN9tloMw_h"
      },
      "source": [
        "## CPU & GPU specs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SymnXtWfM024",
        "outputId": "221fbcd9-0f31-427b-ab6b-3da846187ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU Usage: 18.2%\n",
            "CPU Frequency: 2200.2020000000007 MHz\n",
            "Physical Cores: 6\n",
            "Logical Cores: 12\n"
          ]
        }
      ],
      "source": [
        "get_cpu_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXzbNF9UM1-a",
        "outputId": "a671090a-9c36-4b33-d9dc-0ffa6345c3bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total RAM: 50.53 GB\n",
            "Available RAM: 47.43 GB\n",
            "Used RAM: 2.45 GB\n",
            "RAM Usage: 6.1 %\n"
          ]
        }
      ],
      "source": [
        "get_ram_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBp4-KgEM3CQ",
        "outputId": "f24136c9-3662-44a2-c684-f89e6a0f1df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0 - NVIDIA L4\n",
            "Driver Version: 535.104.05\n",
            "Total VRAM: 24.15 GB\n",
            "Used VRAM: 0.36 GB\n",
            "Free VRAM: 23.8 GB\n",
            "GPU Usage: 0%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "get_gpu_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkAUFwP4HM5p"
      },
      "source": [
        "## Model Loading and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "be7d3d1519a34eaaa8f873467a3a4a55",
            "cc9113a9e9254a10a0df622b21cb054f",
            "0c35875e056e423284ee648068f9093a",
            "98c82ded905b4a30966aaea7e43627bf",
            "f2244fd149514c77bfae54b1a9be6d9e",
            "de3bdfc9527d495a86f81721ef83dd2e",
            "735ebb27c4c74a26ad0f091e1eddc7a7",
            "6d6a4dcf72a54fdcab89541a1b6b0284",
            "8601b4f9d8534980ae64aac491f967f2",
            "40c2aecf1f0c463d954d4411ff8203ee",
            "32f9ea3ca4004d949e773e1c650b6066"
          ]
        },
        "id": "TPZT02dfHLb5",
        "outputId": "27488886-2f4f-469d-d9d7-f6b8def69737"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be7d3d1519a34eaaa8f873467a3a4a55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pad, token doesnt exists, using EOS token\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Embedding(128256, 4096)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load Pretrained Model and Tokenizer\n",
        "# model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
        "# model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "LLAMA_1B = \"meta-llama/Llama-3.2-1B\"\n",
        "LLAMA_8B = \"meta-llama/Llama-3.1-8B\"\n",
        "\n",
        "model_name = LLAMA_8B if RUN_MODE == \"main\" else LLAMA_1B\n",
        "\n",
        "# This line downloads (if needed) and initializes a tokenizer using the identifier stored in model_name.\n",
        "# The tokenizer converts text into a numerical format (tokens) that the model can process,\n",
        "# and it also handles the reverse process (converting tokens back to human-readable text).\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# This line loads a pre-trained causal language model (such as GPT-style models) using the same model identifier.\n",
        "# It retrieves the model architecture and its pre-trained weights so you can use it for tasks like text generation.\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,       # Loads model in FP16\n",
        "    device_map=\"auto\"                # Automatically distributes model across devices if needed\n",
        ")\n",
        "\n",
        "# !! NEW\n",
        "# Freeze all model parameters (ensuring no gradients are computed for the base model)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Ensure a pad token exists (set to eos token if not present).\n",
        "# 1. Check for the padding token id. If none, use the eos_token as the padding token\n",
        "if tokenizer.pad_token_id is None:\n",
        "    print(\"pad, token doesnt exists, using EOS token\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Adjusts the model's token embedding matrix to match the size of the tokenizer's vocabulary.\n",
        "# This is important because adding or changing tokens (like defining a pad token)\n",
        "# may change the size of the vocabulary, and the model's embedding layer needs to reflect that change.\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8h8bWh_HRTv"
      },
      "source": [
        "## LoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PKP9YMKBdIpe"
      },
      "outputs": [],
      "source": [
        "# To get all the intermediate layer config of the model\n",
        "# for name, module in model.named_modules():\n",
        "#     print(name, \":\", module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZffBS3pRHUrK",
        "outputId": "12b0f170-fd18-460b-8a8b-207ea32e3f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA applied. Trainable parameters:\n",
            "trainable params: 16,252,928 || all params: 8,046,514,176 || trainable%: 0.2020\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Enable gradient checkpointing to save memory.\n",
        "\n",
        "# This technique reduces memory usage during training by not storing all intermediate activations\n",
        "# during the forward pass. Instead, it saves only a subset of them and recomputes the missing ones\n",
        "# during the backward pass.\n",
        "model.config.gradient_checkpointing = True\n",
        "\n",
        "# Configure LoRA: update only a small set of additional parameters.\n",
        "# tried r=4 and lora+alpha = 32. Maybe that destabilized training so modifying to 8 and 16 respectively\n",
        "#initally was 0.1, changing to 0.05\n",
        "\n",
        "# studies say best to apply Lora to all layers\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,  # Fine-tuning for causal language modeling.\n",
        "    inference_mode=False,          # Training mode.\n",
        "    r=8,                           # Rank of low-rank decomposition.\n",
        "    lora_alpha=16,                 # Scaling factor.\n",
        "    lora_dropout=0.05,               # Dropout rate for LoRA layers.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# This function call takes the pre-trained model and applies the LoRA configuration you defined.\n",
        "# It modifies the model so that, instead of updating all parameters during fine-tuning,\n",
        "# only a small subset (the LoRA adapters) is trained.\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"LoRA applied. Trainable parameters:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Move the model to the chosen device and set to training mode.\n",
        "model.to(device)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fulb2eoMHXBD"
      },
      "source": [
        "## Data loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ooqU-XeiHY3Q"
      },
      "outputs": [],
      "source": [
        "# Load and Format Training Data\n",
        "import json\n",
        "\n",
        "formatted_strings = []\n",
        "\n",
        "with open(TRAIN_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Parse the JSON data from the line\n",
        "        data = json.loads(line.strip())\n",
        "        # Extract values\n",
        "        product_title = data['Product Title']\n",
        "        product_category = data['Product Categories']\n",
        "        review_rating = data['Rating']\n",
        "        review_title = data['Review Title']\n",
        "        review = data['Review']\n",
        "\n",
        "        # Format the string as per the required format\n",
        "        formatted_string = f'System prompt : Given the Product Title, Product Category, Review Rating and Review Title, you are required to generate the Review | Product Title: {product_title} | Product Category: {product_category} | Review Rating: {review_rating} | Review Title: {review_title} | Review: {review}'\n",
        "        # formatted_string = f'System prompt : You are an Amazon reviews generator that generates reviews based on available information | Product Title: {product_title} | Product Category: {product_category} | Review Rating: {review_rating} | Review Title: {review_title} | Review: {review}'\n",
        "\n",
        "        # Add the formatted string to the list\n",
        "        formatted_strings.append(formatted_string)\n",
        "        if RUN_MODE == \"test\":\n",
        "          if i == 1000:\n",
        "              break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPe6k7Dmdtwp",
        "outputId": "3df6ae75-78cd-4e32-fafc-a1467b16f599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size:  100000\n",
            "System prompt : Given the Product Title, Product Category, Review Rating and Review Title, you are required to generate the Review | Product Title: VUIIMEEK Square Case for iPhone 12 Pro Max 6.7\",Cute White Flowers Clear Print Design Slim Flexible Soft TPU High Impact Shockproof Case Reinforced Bumper Cool Protective Crystal Cover (Green Leaves) | Product Category: Cell Phones & Accessories | Review Rating: 4 | Review Title: No white background! Itâ€™s clear! | Review: I bought this bc I thought it had the nice white background. Turns out itâ€™s clear & since my phone is blue it doesnâ€™t look anything like this.  If I had known that I would have purchased something else. It works ok.\n",
            "length of largets string is:  648.93317\n"
          ]
        }
      ],
      "source": [
        "# Now `formatted_strings` contains the list of strings in the desired format\n",
        "print(\"Size: \",len(formatted_strings))\n",
        "print(formatted_strings[0])\n",
        "train_texts = formatted_strings\n",
        "strs = [len(formatted_str) for formatted_str in formatted_strings]\n",
        "print(\"length of largets string is: \",sum(strs) / len(strs))\n",
        "# avg around 328"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCsd5i7NH17t"
      },
      "source": [
        "## Data tokenization and dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV5ygoW-H6IH",
        "outputId": "4e026be6-2da4-4fcb-a4ff-2c7e0c5eda69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: torch.Size([100000, 256])\n"
          ]
        }
      ],
      "source": [
        "# !! NEW - max_length=512\n",
        "\n",
        "DATA_PARAMS = {\n",
        "  \"max_length\": 256,\n",
        "  \"batch_size\": 2,\n",
        "}\n",
        "\n",
        "# Tokenize training texts with padding and truncation.\n",
        "encodings = tokenizer(train_texts, return_tensors='pt', padding=True, truncation=True, max_length=DATA_PARAMS['max_length'])\n",
        "input_ids = encodings['input_ids']\n",
        "attention_mask = encodings['attention_mask']\n",
        "\n",
        "# For causal language modeling, use input_ids as labels.\n",
        "# Replace pad token positions with -100 so that they are ignored by the loss.\n",
        "\n",
        "#creates a copy of your input IDs, so you can modify them without affecting the original tensor.\n",
        "labels = input_ids.clone()\n",
        "\n",
        "#replaces all padding token positions with -100. This is a common convention (especially with PyTorchâ€™s CrossEntropyLoss)\n",
        "# to indicate that these positions should be ignored during loss computatio\n",
        "labels[input_ids == tokenizer.pad_token_id] = -100\n",
        "\n",
        "print(\"Training data shape:\", input_ids.shape)\n",
        "\n",
        "\n",
        "# !! NEW - num_workers=4, pin_memory=True\n",
        "# Create a TensorDataset and DataLoader with a small batch size.\n",
        "train_dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=DATA_PARAMS['batch_size'], shuffle=True, drop_last=True, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yda7cPXHH9kl"
      },
      "source": [
        "## Optimizer & Privacy engine setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nNoZnmXSeXhv"
      },
      "outputs": [],
      "source": [
        "# !! NEW\n",
        "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4pqKECaH-7T",
        "outputId": "fb3e6855-fd0a-42a4-ed82-617a96ce43e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "privacy_engine = PrivacyEngine()\n",
        "model, optimizer, train_loader = privacy_engine.make_private(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    noise_multiplier=0.3,      # Lower noise multiplier to reduce added noise\n",
        "    max_grad_norm=5,           # Increase clipping norm to allow larger gradients\n",
        "    batch_first=True,\n",
        "    loss_reduction=\"mean\",\n",
        "    poisson_sampling=False      # UPDATE - ERRORING OUT, SO NOT USING. Use Poisson sampling for potentially more stable training\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydDru4H6pMXR"
      },
      "source": [
        "## Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qLr62h6jpLpO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import datetime\n",
        "import pytz # PST time zone\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "class TrainingTracker:\n",
        "    def __init__(self, base_dir=\"./tracking_results\"):\n",
        "        \"\"\"\n",
        "        Initialize the training tracker.\n",
        "\n",
        "        Args:\n",
        "            base_dir: Directory to save tracking results\n",
        "        \"\"\"\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.base_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        # Generate a unique run ID based on timestamp\n",
        "        # Define PST timezone\n",
        "        pst = pytz.timezone(\"America/Los_Angeles\")\n",
        "        # Get current time in PST\n",
        "        pst_time = datetime.datetime.now(pytz.utc).astimezone(pst)\n",
        "        # Format the time\n",
        "        timestamp = pst_time.strftime(\"%d-%m_%H-%M-%S\")\n",
        "        # timestamp = datetime.datetime.now().strftime(\"%d-%m_%H-%M-%S\")\n",
        "        self.run_id = f\"run_{timestamp}\"\n",
        "\n",
        "        # Create run directory\n",
        "        self.run_dir = self.base_dir / self.run_id\n",
        "        self.run_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Initialize tracking data structures\n",
        "        self.params = {}\n",
        "        self.epoch_metrics = []\n",
        "        self.generated_samples = []\n",
        "        self.privacy_metrics = {}\n",
        "\n",
        "    def record_parameters(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Record training parameters for the current run.\n",
        "\n",
        "        Args:\n",
        "            **kwargs: Key-value pairs of parameters to record\n",
        "        \"\"\"\n",
        "        self.params.update(kwargs)\n",
        "\n",
        "        # Save parameters to file\n",
        "        with open(self.run_dir / \"parameters.json\", \"w\") as f:\n",
        "            json.dump(self.params, f, indent=4)\n",
        "\n",
        "    def record_epoch_metrics(self, epoch, loss, batch_times=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Record metrics for a training epoch.\n",
        "\n",
        "        Args:\n",
        "            epoch: Current epoch number\n",
        "            loss: Loss value for the epoch\n",
        "            batch_times: Optional list of batch processing times\n",
        "            **kwargs: Additional metrics to record\n",
        "        \"\"\"\n",
        "        metrics = {\n",
        "            \"epoch\": epoch,\n",
        "            \"loss\": loss,\n",
        "            **kwargs\n",
        "        }\n",
        "\n",
        "        if batch_times:\n",
        "            metrics[\"avg_batch_time\"] = sum(batch_times) / len(batch_times)\n",
        "            metrics[\"min_batch_time\"] = min(batch_times)\n",
        "            metrics[\"max_batch_time\"] = max(batch_times)\n",
        "\n",
        "        self.epoch_metrics.append(metrics)\n",
        "\n",
        "        # Save updated metrics to file\n",
        "        with open(self.run_dir / \"epoch_metrics.json\", \"w\") as f:\n",
        "            json.dump(self.epoch_metrics, f, indent=4)\n",
        "\n",
        "        # Also save as CSV for easier analysis\n",
        "        pd.DataFrame(self.epoch_metrics).to_csv(\n",
        "            self.run_dir / \"epoch_metrics.csv\", index=False)\n",
        "\n",
        "    def record_privacy_budget(self, epsilon, delta=1e-5, **kwargs):\n",
        "        \"\"\"\n",
        "        Record privacy budget metrics.\n",
        "\n",
        "        Args:\n",
        "            epsilon: Achieved epsilon value\n",
        "            delta: Delta value used\n",
        "            **kwargs: Additional privacy metrics\n",
        "        \"\"\"\n",
        "        self.privacy_metrics = {\n",
        "            \"epsilon\": epsilon,\n",
        "            \"delta\": delta,\n",
        "            **kwargs\n",
        "        }\n",
        "\n",
        "        # Save privacy metrics to file\n",
        "        with open(self.run_dir / \"privacy_metrics.json\", \"w\") as f:\n",
        "            json.dump(self.privacy_metrics, f, indent=4)\n",
        "\n",
        "    def record_sample(self, prompt, generated_text):\n",
        "        \"\"\"\n",
        "        Record a sample of generated text.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input prompt\n",
        "            generated_text: Generated text output\n",
        "        \"\"\"\n",
        "        sample = {\n",
        "            \"prompt\": prompt,\n",
        "            \"generated_text\": generated_text,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self.generated_samples.append(sample)\n",
        "\n",
        "        # Save samples to file\n",
        "        with open(self.run_dir / \"generated_samples.json\", \"w\") as f:\n",
        "            json.dump(self.generated_samples, f, indent=4)\n",
        "\n",
        "    def save_model_info(self, model_path, model_type, tokenizer_info=None):\n",
        "        \"\"\"\n",
        "        Record information about the saved model.\n",
        "\n",
        "        Args:\n",
        "            model_path: Path where model was saved\n",
        "            model_type: Type of model (e.g., \"with_dp\", \"without_dp\")\n",
        "            tokenizer_info: Additional tokenizer information\n",
        "        \"\"\"\n",
        "        model_info = {\n",
        "            \"model_path\": str(model_path),\n",
        "            \"model_type\": model_type,\n",
        "            \"tokenizer_info\": tokenizer_info or {}\n",
        "        }\n",
        "\n",
        "        # Save model info to file\n",
        "        with open(self.run_dir / \"model_info.json\", \"w\") as f:\n",
        "            json.dump(model_info, f, indent=4)\n",
        "\n",
        "    def generate_summary(self):\n",
        "        \"\"\"\n",
        "        Generate a summary of the training run.\n",
        "\n",
        "        Returns:\n",
        "            str: Summary text\n",
        "        \"\"\"\n",
        "        summary_lines = [\n",
        "            f\"Training Run: {self.run_id}\",\n",
        "            \"=\" * 50,\n",
        "            \"\\nParameters:\",\n",
        "        ]\n",
        "\n",
        "        for key, value in self.params.items():\n",
        "            summary_lines.append(f\"  {key}: {value}\")\n",
        "\n",
        "        if self.epoch_metrics:\n",
        "            summary_lines.extend([\n",
        "                \"\\nTraining Results:\",\n",
        "                f\"  Epochs completed: {len(self.epoch_metrics)}\",\n",
        "                f\"  Final loss: {self.epoch_metrics[-1]['loss']:.6f}\",\n",
        "                f\"  Initial loss: {self.epoch_metrics[0]['loss']:.6f}\",\n",
        "                f\"  Loss reduction: {self.epoch_metrics[0]['loss'] - self.epoch_metrics[-1]['loss']:.6f}\"\n",
        "            ])\n",
        "\n",
        "        if self.privacy_metrics:\n",
        "            summary_lines.extend([\n",
        "                \"\\nPrivacy Budget:\",\n",
        "                f\"  Epsilon: {self.privacy_metrics['epsilon']:.4f}\",\n",
        "                f\"  Delta: {self.privacy_metrics['delta']}\"\n",
        "            ])\n",
        "\n",
        "        summary_text = \"\\n\".join(summary_lines)\n",
        "\n",
        "        # Save summary to file\n",
        "        with open(self.run_dir / \"summary.txt\", \"w\") as f:\n",
        "            f.write(summary_text)\n",
        "\n",
        "        return summary_text\n",
        "\n",
        "    # def compare_with_previous_runs(self, metric=\"loss\"):\n",
        "    #     \"\"\"\n",
        "    #     Compare this run with previous runs based on a specific metric.\n",
        "\n",
        "    #     Args:\n",
        "    #         metric: Metric to compare (default: \"loss\")\n",
        "\n",
        "    #     Returns:\n",
        "    #         DataFrame: Comparison data\n",
        "    #     \"\"\"\n",
        "    #     # Collect data from all previous runs\n",
        "    #     all_runs = []\n",
        "\n",
        "    #     for run_dir in self.base_dir.iterdir():\n",
        "    #         if not run_dir.is_dir() or run_dir == self.run_dir:\n",
        "    #             continue\n",
        "\n",
        "    #         params_file = run_dir / \"parameters.json\"\n",
        "    #         metrics_file = run_dir / \"epoch_metrics.json\"\n",
        "\n",
        "    #         if params_file.exists() and metrics_file.exists():\n",
        "    #             with open(params_file, \"r\") as f:\n",
        "    #                 params = json.load(f)\n",
        "\n",
        "    #             with open(metrics_file, \"r\") as f:\n",
        "    #                 metrics = json.load(f)\n",
        "\n",
        "    #             if metrics:\n",
        "    #                 final_metric = metrics[-1].get(metric)\n",
        "\n",
        "    #                 run_data = {\n",
        "    #                     \"run_id\": run_dir.name,\n",
        "    #                     f\"final_{metric}\": final_metric,\n",
        "    #                     **params\n",
        "    #                 }\n",
        "\n",
        "    #                 all_runs.append(run_data)\n",
        "\n",
        "    #     # Add current run\n",
        "    #     if self.epoch_metrics:\n",
        "    #         current_run_data = {\n",
        "    #             \"run_id\": self.run_id,\n",
        "    #             f\"final_{metric}\": self.epoch_metrics[-1].get(metric),\n",
        "    #             **self.params\n",
        "    #         }\n",
        "    #         all_runs.append(current_run_data)\n",
        "\n",
        "    #     # Convert to DataFrame and sort\n",
        "    #     if all_runs:\n",
        "    #         df = pd.DataFrame(all_runs)\n",
        "    #         df = df.sort_values(by=f\"final_{metric}\")\n",
        "\n",
        "    #         # Save comparison to file\n",
        "    #         df.to_csv(self.run_dir / f\"comparison_{metric}.csv\", index=False)\n",
        "\n",
        "    #         return df\n",
        "\n",
        "    #     return pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rNSHqUqOCjWS"
      },
      "outputs": [],
      "source": [
        "# Record initial parameters after setting them up\n",
        "def record_initial_params():\n",
        "    # Record model configuration\n",
        "    tracker.record_parameters(\n",
        "        model_name=model_name,\n",
        "        device=str(device),\n",
        "        epochs=epochs,\n",
        "        batch_size=train_loader.batch_size,\n",
        "        learning_rate=optimizer.param_groups[0]['lr'],\n",
        "        gradient_accumulation_steps=accumulation_steps,\n",
        "\n",
        "        # LoRA parameters\n",
        "        lora_r=lora_config.r,\n",
        "        lora_alpha=lora_config.lora_alpha,\n",
        "        lora_dropout=lora_config.lora_dropout,\n",
        "        lora_target_modules=list(lora_config.target_modules),\n",
        "\n",
        "        # Privacy parameters (if using Opacus)\n",
        "        using_differential_privacy=hasattr(model, \"remove_hooks\"),\n",
        "        noise_multiplier=0.6 if hasattr(model, \"remove_hooks\") else None,\n",
        "        max_grad_norm=1.5 if hasattr(model, \"remove_hooks\") else None,\n",
        "\n",
        "        # Dataset info\n",
        "        dataset_size=len(formatted_strings),\n",
        "        avg_sample_length=sum(len(s) for s in formatted_strings) / len(formatted_strings),\n",
        "        tokenizer_max_length=DATA_PARAMS['max_length'],  # From tokenization step\n",
        "        data_batch_size=DATA_PARAMS['batch_size'],\n",
        "\n",
        "        # Tokenizer info\n",
        "        tokenizer_vocab_size=len(tokenizer),\n",
        "        tokenizer_model_max_length=tokenizer.model_max_length,\n",
        "\n",
        "\n",
        "        # System info\n",
        "        cuda_available=torch.cuda.is_available(),\n",
        "        gpu_name=torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjdbmm3wIB0M"
      },
      "source": [
        "## Training setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4iRZnGXfGgij"
      },
      "outputs": [],
      "source": [
        "epochs = 4 if RUN_MODE == \"main\" else 1\n",
        "\n",
        "# !! NEW\n",
        "scaler = GradScaler('cuda')  # Create a gradient scaler to manage FP16 stability\n",
        "accumulation_steps = 1  # Set gradient accumulation steps; use >1 to simulate larger batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Fxzk9pcCCacI"
      },
      "outputs": [],
      "source": [
        "# Initialize the tracker before loading the model\n",
        "tracker = TrainingTracker()\n",
        "# Call this after all parameters are set but before training starts\n",
        "record_initial_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aZm-yQCJOdve",
        "outputId": "39208d1b-47ee-4ea8-9919-b0973460adf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GradSampleModule(PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              "))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training loop\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7oX2UspPfxx"
      },
      "source": [
        "## Sanity check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SjuGRlmwPg2r",
        "outputId": "a775ece6-bb70-4a7f-820e-e87a8b8154f8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'meta-llama/Llama-3.1-8B'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMVloS-nPiBu",
        "outputId": "f440c68d-fb09-4c9c-914a-5e85ec9caf7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guGBiDbgPi8_",
        "outputId": "6cf1a612-bde6-4e8d-d6b9-0fc079b9a12c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sop-hygFaiS",
        "outputId": "7c3373fd-a3bf-49f8-ff80-c8e70a0c5f28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model_name': 'meta-llama/Llama-3.1-8B',\n",
              " 'device': 'cuda',\n",
              " 'epochs': 4,\n",
              " 'batch_size': 2,\n",
              " 'learning_rate': 1e-05,\n",
              " 'gradient_accumulation_steps': 1,\n",
              " 'lora_r': 8,\n",
              " 'lora_alpha': 16,\n",
              " 'lora_dropout': 0.05,\n",
              " 'lora_target_modules': ['gate_proj',\n",
              "  'o_proj',\n",
              "  'v_proj',\n",
              "  'up_proj',\n",
              "  'k_proj',\n",
              "  'q_proj'],\n",
              " 'using_differential_privacy': True,\n",
              " 'noise_multiplier': 0.6,\n",
              " 'max_grad_norm': 1.5,\n",
              " 'dataset_size': 100000,\n",
              " 'avg_sample_length': 648.93317,\n",
              " 'tokenizer_max_length': 256,\n",
              " 'data_batch_size': 2,\n",
              " 'tokenizer_vocab_size': 128256,\n",
              " 'tokenizer_model_max_length': 131072,\n",
              " 'cuda_available': True,\n",
              " 'gpu_name': 'NVIDIA L4'}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tracker.params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQyW5yCiPd68"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "eCilSHLsfGGa"
      },
      "outputs": [],
      "source": [
        "# # FINETUNING, NOT WORKING NOW\n",
        "# for epoch in range(epochs):  # Loop over each epoch\n",
        "#     total_loss = 0.0  # Initialize total loss accumulator for the epoch\n",
        "#     optimizer.zero_grad()  # Zero gradients at the start of the epoch\n",
        "#     for i, batch in enumerate(train_loader):  # Loop over mini-batches from the DataLoader\n",
        "#         # Move each tensor in the batch to the device (GPU) asynchronously if pin_memory is True\n",
        "#         input_ids_batch, attention_mask_batch, labels_batch = [\n",
        "#             x.to(device, non_blocking=True) for x in batch\n",
        "#         ]\n",
        "\n",
        "#         # Determine the sequence length for the current batch and create position IDs accordingly\n",
        "#         seq_len = input_ids_batch.size(1)  # Get the sequence length from the input tensor\n",
        "#         # Create a tensor [0, 1, ..., seq_len-1] and repeat it for each item in the batch\n",
        "#         position_ids = torch.arange(seq_len, device=device).unsqueeze(0).repeat(input_ids_batch.size(0), 1)\n",
        "\n",
        "#         # Use mixed precision context for the forward pass to save memory and speed up computation\n",
        "#         with autocast():\n",
        "#             outputs = model(\n",
        "#                 input_ids=input_ids_batch,        # Input token IDs for the model\n",
        "#                 attention_mask=attention_mask_batch,  # Attention mask to differentiate padded tokens\n",
        "#                 position_ids=position_ids,          # Positional IDs for the tokens\n",
        "#                 labels=labels_batch                 # Labels for computing the loss (typically same as input_ids for causal LM)\n",
        "#             )\n",
        "#             # Compute the loss; if using gradient accumulation, scale down the loss accordingly\n",
        "#             loss = outputs.loss / accumulation_steps\n",
        "\n",
        "#         # Scale the loss and perform the backward pass using the GradScaler for FP16 stability\n",
        "#         scaler.scale(loss).backward()\n",
        "\n",
        "#         # Every 'accumulation_steps' iterations, update the model weights\n",
        "#         if (i + 1) % accumulation_steps == 0:\n",
        "#             scaler.step(optimizer)  # Update parameters using scaled gradients\n",
        "#             scaler.update()         # Update the scale for the next iteration\n",
        "#             optimizer.zero_grad()   # Reset gradients after updating\n",
        "\n",
        "#         # Accumulate the loss (multiply back to undo the earlier division, so total_loss is in original scale)\n",
        "#         total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "#         # Optionally, print progress every 50 batches\n",
        "#         if i % 50 == 0:\n",
        "#             print(f\"Batch {i} processed.\")\n",
        "\n",
        "#     # Compute the average loss over the epoch\n",
        "#     avg_loss = total_loss / len(train_loader)\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} - Average loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Guo1CVB3fLLr",
        "outputId": "d446b880-a999-492c-bd39-7ccbe9d3f427"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4 - Batch 0/50000 - Loss: 2.9705\n",
            "Epoch 1/4 - Batch 50/50000 - Loss: 2.9740\n",
            "Epoch 1/4 - Batch 100/50000 - Loss: 2.2367\n",
            "Epoch 1/4 - Batch 150/50000 - Loss: 2.7121\n",
            "Epoch 1/4 - Batch 200/50000 - Loss: 2.6993\n",
            "Epoch 1/4 - Batch 250/50000 - Loss: 2.7691\n",
            "Epoch 1/4 - Batch 300/50000 - Loss: 2.8640\n",
            "Epoch 1/4 - Batch 350/50000 - Loss: 2.9068\n",
            "Epoch 1/4 - Batch 400/50000 - Loss: 2.8245\n",
            "Epoch 1/4 - Batch 450/50000 - Loss: 2.8058\n",
            "Epoch 1/4 - Batch 500/50000 - Loss: 2.6304\n",
            "Epoch 1/4 - Batch 550/50000 - Loss: 2.6939\n",
            "Epoch 1/4 - Batch 600/50000 - Loss: 2.9861\n",
            "Epoch 1/4 - Batch 650/50000 - Loss: 2.8600\n",
            "Epoch 1/4 - Batch 700/50000 - Loss: 2.9104\n",
            "Epoch 1/4 - Batch 750/50000 - Loss: 2.4468\n",
            "Epoch 1/4 - Batch 800/50000 - Loss: 2.8780\n",
            "Epoch 1/4 - Batch 850/50000 - Loss: 2.3775\n",
            "Epoch 1/4 - Batch 900/50000 - Loss: 3.0302\n",
            "Epoch 1/4 - Batch 950/50000 - Loss: 2.8713\n",
            "Epoch 1/4 - Batch 1000/50000 - Loss: 2.9465\n",
            "Epoch 1/4 - Batch 1050/50000 - Loss: 2.7029\n",
            "Epoch 1/4 - Batch 1100/50000 - Loss: 3.2280\n",
            "Epoch 1/4 - Batch 1150/50000 - Loss: 3.2994\n",
            "Epoch 1/4 - Batch 1200/50000 - Loss: 2.3671\n",
            "Epoch 1/4 - Batch 1250/50000 - Loss: 2.5971\n",
            "Epoch 1/4 - Batch 1300/50000 - Loss: 2.8724\n",
            "Epoch 1/4 - Batch 1350/50000 - Loss: 3.0145\n",
            "Epoch 1/4 - Batch 1400/50000 - Loss: 3.0149\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    batch_times = []\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        batch_start = datetime.datetime.now()\n",
        "\n",
        "\n",
        "        # Move each element of the batch to the device.\n",
        "        # input_ids_batch, attention_mask_batch, labels_batch = [x.to(device) for x in batch]\n",
        "        input_ids_batch, attention_mask_batch, labels_batch = [x.to(device, non_blocking=True) for x in batch]\n",
        "\n",
        "        # Create a position_ids tensor: shape [batch_size, seq_len]\n",
        "        seq_len = input_ids_batch.size(1)\n",
        "        position_ids = torch.arange(seq_len, device=device).unsqueeze(0).repeat(input_ids_batch.size(0), 1)\n",
        "\n",
        "        # Forward pass: compute the loss.\n",
        "        outputs = model(\n",
        "            input_ids=input_ids_batch,\n",
        "            attention_mask=attention_mask_batch,\n",
        "            position_ids=position_ids,\n",
        "            labels=labels_batch\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        # total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        total_loss += loss.item()\n",
        "        batch_end = datetime.datetime.now()\n",
        "        batch_times.append((batch_end - batch_start).total_seconds())\n",
        "\n",
        "        # Print progress\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Batch {i}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Compute epoch metrics\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    end_time = datetime.datetime.now()\n",
        "    epoch_duration = (end_time - start_time).total_seconds()\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Average loss: {avg_loss:.4f} - Duration: {epoch_duration:.2f}s\")\n",
        "\n",
        "    # Record epoch metrics\n",
        "    tracker.record_epoch_metrics(\n",
        "        epoch=epoch+1,\n",
        "        loss=avg_loss,\n",
        "        batch_times=batch_times,\n",
        "        epoch_duration=epoch_duration,\n",
        "        timestamp=datetime.datetime.now().isoformat()\n",
        "    )\n",
        "\n",
        "    # Record privacy budget if using differential privacy\n",
        "    if hasattr(model, \"remove_hooks\"):\n",
        "        epsilon = privacy_engine.accountant.get_epsilon(delta=1e-5)\n",
        "        print(f\"Achieved privacy budget: Îµ = {epsilon:.2f}\")\n",
        "        tracker.record_privacy_budget(epsilon=epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sjccWWMIFZi"
      },
      "source": [
        "## Model saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNz9DicrIGdx"
      },
      "outputs": [],
      "source": [
        "# Remove DP hooks if present\n",
        "if hasattr(model, \"remove_hooks\"):\n",
        "    model.remove_hooks()\n",
        "    model = model._module  # Unwrap the model\n",
        "\n",
        "# # Remove DP hooks to restore the underlying model.\n",
        "# model.remove_hooks()\n",
        "# model = model._module  # Unwrap the model.\n",
        "\n",
        "# Define model type based on whether differential privacy was used\n",
        "model_type = \"with_dp\" if hasattr(privacy_engine, \"accountant\") else \"without_dp\"\n",
        "\n",
        "# Specify the directory where you want to save your fine-tuned model\n",
        "save_directory = \"./finetuned_model_dp\"\n",
        "\n",
        "# Save the model weights and configuration\n",
        "model.save_pretrained(save_directory)\n",
        "\n",
        "# Save the tokenizer (this ensures that any custom tokens are preserved)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "# Record model info\n",
        "tracker.save_model_info(\n",
        "    model_path=save_directory,\n",
        "    model_type=model_type,\n",
        "    tokenizer_info={\n",
        "        \"vocab_size\": len(tokenizer),\n",
        "        \"model_max_length\": tokenizer.model_max_length,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Model and tokenizer saved to {save_directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3pAtkLcIIDl"
      },
      "source": [
        "## Interactive testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUKEFCVeIJ7P"
      },
      "outputs": [],
      "source": [
        "# Evaluate using a sample prompt.\n",
        "while True:\n",
        "    sample_prompt = input(\"Input: \")\n",
        "    if sample_prompt.lower() == \"bye\":\n",
        "        break\n",
        "    enc = tokenizer(sample_prompt, return_tensors='pt', padding=True, truncation=True)\n",
        "    enc = {k: v.to(device, non_blocking=True) for k, v in enc.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**enc, max_length=128, do_sample=True, top_k=50)\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zzRbASQrDIA"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# import torch\n",
        "\n",
        "# # Specify the directory where your model and tokenizer are saved\n",
        "# save_directory = \".\"\n",
        "\n",
        "# # Load the model and tokenizer from the saved directory\n",
        "# model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "\n",
        "# # Set up the device: use CUDA if available, otherwise fallback to CPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {device}\")\n",
        "\n",
        "# # Move the model to the selected device and set it to evaluation mode\n",
        "# model.to(device)\n",
        "# model.eval()\n",
        "\n",
        "# # Define a sample prompt\n",
        "# sample_prompt = (\"System prompt: Given the Rating and Title, you are required to generate the review, \"\n",
        "#                  \"Rating: 5, Title: Would definitely buy again, Review:\")\n",
        "\n",
        "# # Tokenize the input text\n",
        "# inputs = tokenizer(sample_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "# inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# # Generate text using the model\n",
        "# generated_ids = model.generate(**inputs, max_length=128, do_sample=True, top_k=50)\n",
        "# generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# print(\"Generated text:\", generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KwPM3hdJr1ez",
        "g8WJhCIZMuRB",
        "ykEN9tloMw_h",
        "M8h8bWh_HRTv",
        "Yda7cPXHH9kl",
        "9sjccWWMIFZi"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c35875e056e423284ee648068f9093a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d6a4dcf72a54fdcab89541a1b6b0284",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8601b4f9d8534980ae64aac491f967f2",
            "value": 4
          }
        },
        "32f9ea3ca4004d949e773e1c650b6066": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40c2aecf1f0c463d954d4411ff8203ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d6a4dcf72a54fdcab89541a1b6b0284": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "735ebb27c4c74a26ad0f091e1eddc7a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8601b4f9d8534980ae64aac491f967f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98c82ded905b4a30966aaea7e43627bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40c2aecf1f0c463d954d4411ff8203ee",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_32f9ea3ca4004d949e773e1c650b6066",
            "value": "â€‡4/4â€‡[00:09&lt;00:00,â€‡â€‡2.13s/it]"
          }
        },
        "be7d3d1519a34eaaa8f873467a3a4a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc9113a9e9254a10a0df622b21cb054f",
              "IPY_MODEL_0c35875e056e423284ee648068f9093a",
              "IPY_MODEL_98c82ded905b4a30966aaea7e43627bf"
            ],
            "layout": "IPY_MODEL_f2244fd149514c77bfae54b1a9be6d9e"
          }
        },
        "cc9113a9e9254a10a0df622b21cb054f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de3bdfc9527d495a86f81721ef83dd2e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_735ebb27c4c74a26ad0f091e1eddc7a7",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "de3bdfc9527d495a86f81721ef83dd2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2244fd149514c77bfae54b1a9be6d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
