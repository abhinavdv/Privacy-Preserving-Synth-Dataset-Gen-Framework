{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements and dependencies\n"
      ],
      "metadata": {
        "id": "KwPM3hdJr1ez"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZof-WuyHdF2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install opacus\n",
        "!pip install -U bitsandbytes transformers accelerate\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8Ga73J2NITU",
        "outputId": "5a9adb97-e92f-4efe-fed2-816518ededc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (12.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pynvml) (12.570.86)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCRKTux3HJuH",
        "outputId": "523ffbac-0d69-434a-dfd4-5be090ae96fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Using device: cuda\n",
            "GPU Device: NVIDIA L4\n",
            "Available GPU memory: 22.17 GB\n"
          ]
        }
      ],
      "source": [
        "# Check NumPy Version and Import Dependencies\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from opacus import PrivacyEngine\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Set up device - prioritize GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Print GPU info if available\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSOP969tMhsE"
      },
      "outputs": [],
      "source": [
        "## Clear GPU cache and storage\n",
        "torch.cuda.empty_cache()  # Frees unused memory\n",
        "torch.cuda.ipc_collect()  # Collects shared memory used in multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "06uZqEV3rGAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve token securely\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"Logged in successfully!\")\n",
        "else:\n",
        "    print(\"Hugging Face token not found. Please set it using `userdata.set`.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5eyCXarG33",
        "outputId": "34ebe03d-2fd7-4f70-8650-2772e2419718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU and GPU util functions"
      ],
      "metadata": {
        "id": "g8WJhCIZMuRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetUtilizationRates, nvmlSystemGetDriverVersion, nvmlDeviceGetName, nvmlShutdown\n",
        "    nvmlInit()\n",
        "    NVML_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NVML_AVAILABLE = False\n",
        "\n",
        "def get_cpu_stats():\n",
        "    \"\"\" Get CPU usage stats \"\"\"\n",
        "    cpu_usage = psutil.cpu_percent(interval=1)  # Get CPU usage %\n",
        "    cpu_freq = psutil.cpu_freq().current if psutil.cpu_freq() else \"Unknown\"  # CPU Frequency\n",
        "    num_cores = psutil.cpu_count(logical=False)  # Physical Cores\n",
        "    num_threads = psutil.cpu_count(logical=True)  # Logical Cores\n",
        "    print(f\"CPU Usage: {cpu_usage}%\")\n",
        "    print(f\"CPU Frequency: {cpu_freq} MHz\")\n",
        "    print(f\"Physical Cores: {num_cores}\")\n",
        "    print(f\"Logical Cores: {num_threads}\")\n",
        "\n",
        "def get_ram_stats():\n",
        "    \"\"\" Get system RAM stats \"\"\"\n",
        "    ram = psutil.virtual_memory()\n",
        "    print(\"Total RAM:\", round(ram.total / 1e9, 2), \"GB\")\n",
        "    print(\"Available RAM:\", round(ram.available / 1e9, 2), \"GB\")\n",
        "    print(\"Used RAM:\", round(ram.used / 1e9, 2), \"GB\")\n",
        "    print(\"RAM Usage:\", ram.percent, \"%\")\n",
        "\n",
        "def get_gpu_stats():\n",
        "    \"\"\" Get GPU stats if available \"\"\"\n",
        "    if not NVML_AVAILABLE:\n",
        "        return {\"Error\": \"pynvml not installed. Run: pip install nvidia-ml-py3\"}\n",
        "\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "\n",
        "    for i in range(num_gpus):\n",
        "        handle = nvmlDeviceGetHandleByIndex(i)\n",
        "        mem_info = nvmlDeviceGetMemoryInfo(handle)\n",
        "        utilization = nvmlDeviceGetUtilizationRates(handle)\n",
        "\n",
        "        print(f\"GPU {i} - {nvmlDeviceGetName(handle)}\")\n",
        "        print(f\"Driver Version: {nvmlSystemGetDriverVersion()}\")\n",
        "        print(f\"Total VRAM: {round(mem_info.total / 1e9, 2)} GB\")\n",
        "        print(f\"Used VRAM: {round(mem_info.used / 1e9, 2)} GB\")\n",
        "        print(f\"Free VRAM: {round(mem_info.free / 1e9, 2)} GB\")\n",
        "        print(f\"GPU Usage: {utilization.gpu}%\")\n",
        "        print()\n",
        "\n",
        "    nvmlShutdown()  # Clean up NVML\n",
        "\n",
        "# Run and print system stats\n",
        "\n",
        "print(\"\\nðŸ”¹ CPU Stats:\", )\n",
        "print(\"\\nðŸ”¹ RAM Stats:\", )\n",
        "print(\"\\nðŸ”¹ GPU Stats:\", )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebizwy4ZMnlk",
        "outputId": "951e7c40-d6d5-4c1f-b4bb-f71b96c814a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¹ CPU Stats:\n",
            "\n",
            "ðŸ”¹ RAM Stats:\n",
            "\n",
            "ðŸ”¹ GPU Stats:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU & GPU specs"
      ],
      "metadata": {
        "id": "ykEN9tloMw_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_cpu_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SymnXtWfM024",
        "outputId": "8f01306f-4bec-4580-901c-98da28c47984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Usage: 0.5%\n",
            "CPU Frequency: 2200.2180000000003 MHz\n",
            "Physical Cores: 4\n",
            "Logical Cores: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ram_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXzbNF9UM1-a",
        "outputId": "7c5787ee-b908-4bd9-93df-950575dd161e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total RAM: 33.67 GB\n",
            "Available RAM: 31.78 GB\n",
            "Used RAM: 1.4 GB\n",
            "RAM Usage: 5.6 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_gpu_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBp4-KgEM3CQ",
        "outputId": "5a71d59b-c5f1-4d7f-8670-3cbd0fd5faa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0 - NVIDIA L4\n",
            "Driver Version: 535.104.05\n",
            "Total VRAM: 24.15 GB\n",
            "Used VRAM: 0.36 GB\n",
            "Free VRAM: 23.8 GB\n",
            "GPU Usage: 0%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkAUFwP4HM5p"
      },
      "source": [
        "## Model Loading and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "1d9a9f8de9c04e0aa802326895a250ad",
            "098c8046938a411a96d0b7aa37b567d1",
            "bf2a1fa64ee04df98dfdbbe56365860b",
            "69a06391aaeb49bf82b3d06722427238",
            "699193bea7df4ed5836bc24eedd8ee13",
            "71aadf43c7c44753aabb9ab8f40d8ab0",
            "98e323bb9f224a5086592d63909501fb",
            "f5fe54755a9547fa8834a6263dffe60d",
            "8d4033c6d90c42938525fbadcee331fa",
            "20347cd5327c4236a05a3038294bb6af",
            "36c2294c1ba245cebccb324285232cfe"
          ]
        },
        "id": "TPZT02dfHLb5",
        "outputId": "8fb0bd04-676e-465f-fa92-25deadb9017c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d9a9f8de9c04e0aa802326895a250ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(128256, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Load Pretrained Model and Tokenizer\n",
        "# model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
        "# model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "model_name = \"meta-llama/Llama-3.1-8B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Load model with GPU memory optimizations\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # Use half precision\n",
        "    device_map=\"auto\"  # Automatically handle model parallelism if needed\n",
        ")\n",
        "\n",
        "# Ensure pad token exists\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(name, \":\", module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBGuZvxAxeV-",
        "outputId": "67980340-be02-4ebb-ed82-1cf5fb8c914f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " : LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
            ")\n",
            "model : LlamaModel(\n",
            "  (embed_tokens): Embedding(128256, 4096)\n",
            "  (layers): ModuleList(\n",
            "    (0-31): 32 x LlamaDecoderLayer(\n",
            "      (self_attn): LlamaAttention(\n",
            "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "      )\n",
            "      (mlp): LlamaMLP(\n",
            "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "        (act_fn): SiLU()\n",
            "      )\n",
            "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    )\n",
            "  )\n",
            "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (rotary_emb): LlamaRotaryEmbedding()\n",
            ")\n",
            "model.embed_tokens : Embedding(128256, 4096)\n",
            "model.layers : ModuleList(\n",
            "  (0-31): 32 x LlamaDecoderLayer(\n",
            "    (self_attn): LlamaAttention(\n",
            "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    )\n",
            "    (mlp): LlamaMLP(\n",
            "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "      (act_fn): SiLU()\n",
            "    )\n",
            "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  )\n",
            ")\n",
            "model.layers.0 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.0.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.0.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.0.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.0.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.0.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.0.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.0.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.0.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.0.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.0.mlp.act_fn : SiLU()\n",
            "model.layers.0.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.0.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.1 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.1.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.1.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.1.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.1.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.1.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.1.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.1.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.1.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.1.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.1.mlp.act_fn : SiLU()\n",
            "model.layers.1.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.1.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.2 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.2.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.2.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.2.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.2.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.2.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.2.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.2.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.2.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.2.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.2.mlp.act_fn : SiLU()\n",
            "model.layers.2.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.2.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.3 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.3.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.3.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.3.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.3.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.3.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.3.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.3.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.3.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.3.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.3.mlp.act_fn : SiLU()\n",
            "model.layers.3.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.3.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.4 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.4.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.4.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.4.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.4.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.4.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.4.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.4.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.4.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.4.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.4.mlp.act_fn : SiLU()\n",
            "model.layers.4.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.4.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.5 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.5.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.5.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.5.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.5.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.5.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.5.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.5.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.5.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.5.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.5.mlp.act_fn : SiLU()\n",
            "model.layers.5.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.5.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.6 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.6.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.6.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.6.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.6.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.6.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.6.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.6.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.6.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.6.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.6.mlp.act_fn : SiLU()\n",
            "model.layers.6.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.6.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.7 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.7.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.7.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.7.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.7.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.7.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.7.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.7.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.7.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.7.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.7.mlp.act_fn : SiLU()\n",
            "model.layers.7.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.7.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.8 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.8.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.8.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.8.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.8.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.8.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.8.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.8.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.8.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.8.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.8.mlp.act_fn : SiLU()\n",
            "model.layers.8.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.8.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.9 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.9.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.9.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.9.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.9.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.9.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.9.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.9.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.9.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.9.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.9.mlp.act_fn : SiLU()\n",
            "model.layers.9.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.9.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.10 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.10.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.10.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.10.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.10.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.10.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.10.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.10.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.10.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.10.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.10.mlp.act_fn : SiLU()\n",
            "model.layers.10.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.10.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.11 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.11.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.11.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.11.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.11.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.11.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.11.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.11.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.11.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.11.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.11.mlp.act_fn : SiLU()\n",
            "model.layers.11.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.11.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.12 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.12.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.12.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.12.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.12.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.12.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.12.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.12.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.12.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.12.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.12.mlp.act_fn : SiLU()\n",
            "model.layers.12.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.12.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.13 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.13.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.13.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.13.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.13.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.13.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.13.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.13.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.13.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.13.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.13.mlp.act_fn : SiLU()\n",
            "model.layers.13.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.13.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.14 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.14.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.14.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.14.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.14.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.14.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.14.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.14.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.14.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.14.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.14.mlp.act_fn : SiLU()\n",
            "model.layers.14.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.14.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.15 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.15.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.15.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.15.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.15.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.15.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.15.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.15.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.15.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.15.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.15.mlp.act_fn : SiLU()\n",
            "model.layers.15.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.15.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.16 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.16.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.16.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.16.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.16.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.16.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.16.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.16.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.16.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.16.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.16.mlp.act_fn : SiLU()\n",
            "model.layers.16.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.16.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.17 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.17.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.17.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.17.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.17.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.17.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.17.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.17.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.17.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.17.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.17.mlp.act_fn : SiLU()\n",
            "model.layers.17.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.17.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.18 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.18.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.18.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.18.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.18.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.18.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.18.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.18.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.18.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.18.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.18.mlp.act_fn : SiLU()\n",
            "model.layers.18.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.18.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.19 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.19.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.19.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.19.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.19.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.19.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.19.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.19.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.19.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.19.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.19.mlp.act_fn : SiLU()\n",
            "model.layers.19.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.19.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.20 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.20.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.20.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.20.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.20.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.20.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.20.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.20.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.20.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.20.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.20.mlp.act_fn : SiLU()\n",
            "model.layers.20.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.20.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.21 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.21.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.21.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.21.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.21.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.21.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.21.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.21.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.21.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.21.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.21.mlp.act_fn : SiLU()\n",
            "model.layers.21.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.21.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.22 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.22.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.22.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.22.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.22.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.22.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.22.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.22.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.22.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.22.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.22.mlp.act_fn : SiLU()\n",
            "model.layers.22.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.22.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.23 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.23.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.23.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.23.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.23.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.23.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.23.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.23.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.23.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.23.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.23.mlp.act_fn : SiLU()\n",
            "model.layers.23.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.23.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.24 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.24.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.24.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.24.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.24.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.24.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.24.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.24.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.24.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.24.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.24.mlp.act_fn : SiLU()\n",
            "model.layers.24.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.24.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.25 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.25.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.25.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.25.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.25.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.25.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.25.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.25.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.25.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.25.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.25.mlp.act_fn : SiLU()\n",
            "model.layers.25.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.25.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.26 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.26.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.26.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.26.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.26.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.26.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.26.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.26.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.26.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.26.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.26.mlp.act_fn : SiLU()\n",
            "model.layers.26.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.26.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.27 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.27.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.27.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.27.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.27.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.27.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.27.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.27.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.27.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.27.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.27.mlp.act_fn : SiLU()\n",
            "model.layers.27.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.27.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.28 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.28.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.28.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.28.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.28.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.28.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.28.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.28.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.28.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.28.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.28.mlp.act_fn : SiLU()\n",
            "model.layers.28.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.28.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.29 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.29.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.29.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.29.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.29.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.29.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.29.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.29.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.29.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.29.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.29.mlp.act_fn : SiLU()\n",
            "model.layers.29.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.29.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.30 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.30.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.30.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.30.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.30.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.30.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.30.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.30.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.30.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.30.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.30.mlp.act_fn : SiLU()\n",
            "model.layers.30.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.30.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.31 : LlamaDecoderLayer(\n",
            "  (self_attn): LlamaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  )\n",
            "  (mlp): LlamaMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            ")\n",
            "model.layers.31.self_attn : LlamaAttention(\n",
            "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            ")\n",
            "model.layers.31.self_attn.q_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.31.self_attn.k_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.31.self_attn.v_proj : Linear(in_features=4096, out_features=1024, bias=False)\n",
            "model.layers.31.self_attn.o_proj : Linear(in_features=4096, out_features=4096, bias=False)\n",
            "model.layers.31.mlp : LlamaMLP(\n",
            "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "  (act_fn): SiLU()\n",
            ")\n",
            "model.layers.31.mlp.gate_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.31.mlp.up_proj : Linear(in_features=4096, out_features=14336, bias=False)\n",
            "model.layers.31.mlp.down_proj : Linear(in_features=14336, out_features=4096, bias=False)\n",
            "model.layers.31.mlp.act_fn : SiLU()\n",
            "model.layers.31.input_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.layers.31.post_attention_layernorm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.norm : LlamaRMSNorm((4096,), eps=1e-05)\n",
            "model.rotary_emb : LlamaRotaryEmbedding()\n",
            "lm_head : Linear(in_features=4096, out_features=128256, bias=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8h8bWh_HRTv"
      },
      "source": [
        "## LoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZffBS3pRHUrK",
        "outputId": "735a738e-6673-4763-e717-cf1628eb04c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA applied. Trainable parameters:\n",
            "trainable params: 1,703,936 || all params: 8,031,965,184 || trainable%: 0.0212\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Apply LoRA for Parameter-Efficient Fine-Tuning\n",
        "model.config.gradient_checkpointing = True\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=4,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    # target_modules=[\"query_key_value\"]  # Specify target modules for GPT-Neo\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"LoRA applied. Trainable parameters:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Ensure model is on GPU\n",
        "model = model.to(device)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fulb2eoMHXBD"
      },
      "source": [
        "## Data loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooqU-XeiHY3Q"
      },
      "outputs": [],
      "source": [
        "# Load and Format Training Data\n",
        "import json\n",
        "\n",
        "formatted_strings = []\n",
        "\n",
        "with open(\"finetuning/train.jsonl\", \"r\") as f:\n",
        "    j = 0\n",
        "    for line in f:\n",
        "        data = json.loads(line.strip())\n",
        "        j+=1\n",
        "        # if j == 1000:\n",
        "        #     break\n",
        "        rating = data['Rating']\n",
        "        title = data['Title']\n",
        "        review = data['Review']\n",
        "\n",
        "        formatted_string = f'\"System prompt : Given the Rating and Title, you are required to generate the review\" | \"Rating\": {rating} | \"Title\": {title} | \"Review\": {review}'\n",
        "        formatted_strings.append(formatted_string)\n",
        "\n",
        "train_texts = formatted_strings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yQMXNbzUtgft"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCsd5i7NH17t"
      },
      "source": [
        "## Data tokenization and dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV5ygoW-H6IH",
        "outputId": "26c5d9f6-5135-4100-dccf-bc97dff30090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: torch.Size([100000, 128])\n"
          ]
        }
      ],
      "source": [
        "# Tokenize and Prepare Dataset\n",
        "encodings = tokenizer(train_texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "input_ids = encodings['input_ids']\n",
        "attention_mask = encodings['attention_mask']\n",
        "\n",
        "labels = input_ids.clone()\n",
        "labels[input_ids == tokenizer.pad_token_id] = -100\n",
        "\n",
        "print(\"Training data shape:\", input_ids.shape)\n",
        "\n",
        "train_dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yda7cPXHH9kl"
      },
      "source": [
        "## Privacy engine setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4pqKECaH-7T"
      },
      "outputs": [],
      "source": [
        "# Set Up Optimizer and PrivacyEngine\n",
        "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
        "privacy_engine = PrivacyEngine()\n",
        "model, optimizer, train_loader = privacy_engine.make_private(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        "    batch_first=True,\n",
        "    loss_reduction=\"mean\",\n",
        "    poisson_sampling=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjdbmm3wIB0M"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Enc0M0IAkX",
        "outputId": "582e423e-7420-492a-f1a3-50bdbef585ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "4000\n",
            "4500\n",
            "5000\n",
            "5500\n",
            "6000\n",
            "6500\n",
            "7000\n",
            "7500\n",
            "8000\n",
            "8500\n",
            "9000\n",
            "9500\n",
            "10000\n",
            "10500\n",
            "11000\n",
            "11500\n",
            "12000\n",
            "12500\n",
            "13000\n",
            "13500\n",
            "14000\n",
            "14500\n",
            "15000\n",
            "15500\n",
            "16000\n",
            "16500\n",
            "17000\n",
            "17500\n",
            "18000\n",
            "18500\n",
            "19000\n",
            "19500\n",
            "20000\n",
            "20500\n",
            "21000\n",
            "21500\n",
            "22000\n",
            "22500\n",
            "23000\n",
            "23500\n",
            "24000\n",
            "24500\n",
            "Epoch 1/3 - Average loss: 1.6101\n",
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "4000\n",
            "4500\n",
            "5000\n",
            "5500\n",
            "6000\n",
            "6500\n",
            "7000\n",
            "7500\n",
            "8000\n",
            "8500\n",
            "9000\n",
            "9500\n",
            "10000\n",
            "10500\n",
            "11000\n",
            "11500\n",
            "12000\n",
            "12500\n",
            "13000\n",
            "13500\n",
            "14000\n",
            "14500\n",
            "15000\n",
            "15500\n",
            "16000\n",
            "16500\n",
            "17000\n",
            "17500\n",
            "18000\n",
            "18500\n",
            "19000\n",
            "19500\n",
            "20000\n",
            "20500\n",
            "21000\n",
            "21500\n",
            "22000\n",
            "22500\n",
            "23000\n",
            "23500\n",
            "24000\n",
            "24500\n",
            "Epoch 2/3 - Average loss: 1.6199\n",
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "4000\n",
            "4500\n",
            "5000\n",
            "5500\n",
            "6000\n",
            "6500\n",
            "7000\n",
            "7500\n",
            "8000\n",
            "8500\n",
            "9000\n",
            "9500\n",
            "10000\n",
            "10500\n",
            "11000\n",
            "11500\n",
            "12000\n",
            "12500\n",
            "13000\n",
            "13500\n",
            "14000\n",
            "14500\n",
            "15000\n",
            "15500\n",
            "16000\n",
            "16500\n",
            "17000\n",
            "17500\n",
            "18000\n",
            "18500\n",
            "19000\n",
            "19500\n",
            "20000\n",
            "20500\n",
            "21000\n",
            "21500\n",
            "22000\n",
            "22500\n",
            "23000\n",
            "23500\n",
            "24000\n",
            "24500\n",
            "Epoch 3/3 - Average loss: 1.8221\n"
          ]
        }
      ],
      "source": [
        "# Training Loop with DP-SGD\n",
        "model.train()\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    i = 0\n",
        "    for batch in train_loader:\n",
        "        if i%500 == 0:\n",
        "            print(i)\n",
        "        i+=1\n",
        "        input_ids_batch, attention_mask_batch, labels_batch = [x.to(device) for x in batch]\n",
        "\n",
        "        seq_len = input_ids_batch.size(1)\n",
        "        position_ids = torch.arange(seq_len, device=device).unsqueeze(0).repeat(input_ids_batch.size(0), 1)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids_batch,\n",
        "            attention_mask=attention_mask_batch,\n",
        "            position_ids=position_ids,\n",
        "            labels=labels_batch\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Average loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sjccWWMIFZi"
      },
      "source": [
        "## Model saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNz9DicrIGdx",
        "outputId": "772d2545-da17-4f8d-8672-5a99412228ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer saved to ./finetuned_model_dp\n"
          ]
        }
      ],
      "source": [
        "# Save Fine-tuned Model\n",
        "# model.remove_hooks()\n",
        "# model = model._module\n",
        "\n",
        "save_directory = \"./finetuned_model_dp\"\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "print(f\"Model and tokenizer saved to {save_directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3pAtkLcIIDl"
      },
      "source": [
        "## Interactive testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "XUKEFCVeIJ7P",
        "outputId": "b8e6cbd0-0265-4641-e090-ad2bb3a46801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: \"System prompt\": \"Given the Rating and Title, you are required to generate the review\", \"Rating\": 4, \"Title\": \"No white background! It\\u2019s clear!\", \"Review\":\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: \"System prompt\": \"Given the Rating and Title, you are required to generate the review\", \"Rating\": 4, \"Title\": \"No white background! It\\u2019s clear!\", \"Review\": I need a camera.  I wish a case, I have the buy the case.  I only the watch and the put of watch.. the the order phone,, I's the the stand phone case of the the case.  I buy the case. I only it.  I.  I was the phone.   I.  I had it.  I buy the the the  I one. I watch,. I the the the the phone I. I back it. I the the the the the phone. I I like my the  I't my  I. I was one that. I, as the the the the the the, I   I I the the the the the the phone on the the phone,. I had the the the the the the phone. if it  ita that it's a the the the the the the the the the, you it... on phone of the the the., of the the phone. I. it.< so. I.. I the the the the the phone.. I. to the the phone I I not,,. I the phone. I., the will  the protector one one... I..  I's.. I it't that. the the the the back. I.. in one. it and it. if on.. I's a the the  one I the I the it. the the the the  it..\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-fc7d692b2b3d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msample_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_prompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bye\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Interactive Testing Loop\n",
        "model.eval()\n",
        "\n",
        "while True:\n",
        "    sample_prompt = input(\"Input: \")\n",
        "    if sample_prompt.lower() == \"bye\":\n",
        "        break\n",
        "    enc = tokenizer(sample_prompt, return_tensors='pt', padding=True, truncation=True)\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "    # play with max_length=350, in the paper it is defined different\n",
        "    # EOS token\n",
        "    generated_ids = model.generate(**enc, max_length=350, do_sample=True, top_k=50)\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(\"Generated text:\", generated_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KwPM3hdJr1ez",
        "g8WJhCIZMuRB",
        "ykEN9tloMw_h",
        "M8h8bWh_HRTv",
        "Fulb2eoMHXBD",
        "ZCsd5i7NH17t",
        "Yda7cPXHH9kl"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d9a9f8de9c04e0aa802326895a250ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_098c8046938a411a96d0b7aa37b567d1",
              "IPY_MODEL_bf2a1fa64ee04df98dfdbbe56365860b",
              "IPY_MODEL_69a06391aaeb49bf82b3d06722427238"
            ],
            "layout": "IPY_MODEL_699193bea7df4ed5836bc24eedd8ee13"
          }
        },
        "098c8046938a411a96d0b7aa37b567d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71aadf43c7c44753aabb9ab8f40d8ab0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_98e323bb9f224a5086592d63909501fb",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "bf2a1fa64ee04df98dfdbbe56365860b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5fe54755a9547fa8834a6263dffe60d",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d4033c6d90c42938525fbadcee331fa",
            "value": 4
          }
        },
        "69a06391aaeb49bf82b3d06722427238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20347cd5327c4236a05a3038294bb6af",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_36c2294c1ba245cebccb324285232cfe",
            "value": "â€‡4/4â€‡[00:13&lt;00:00,â€‡â€‡2.96s/it]"
          }
        },
        "699193bea7df4ed5836bc24eedd8ee13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71aadf43c7c44753aabb9ab8f40d8ab0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98e323bb9f224a5086592d63909501fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5fe54755a9547fa8834a6263dffe60d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4033c6d90c42938525fbadcee331fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20347cd5327c4236a05a3038294bb6af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36c2294c1ba245cebccb324285232cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}