{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# RUN_MODE = \"test\"\n",
        "RUN_MODE = \"main\""
      ],
      "metadata": {
        "id": "XXHWXGdJNhKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwPM3hdJr1ez"
      },
      "source": [
        "## Requirements and dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZof-WuyHdF2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install opacus\n",
        "# !pip install -U bitsandbytes transformers accelerate\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8Ga73J2NITU",
        "outputId": "b374e7b0-90dc-47e2-a826-d7817277b9d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (12.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pynvml) (12.570.86)\n"
          ]
        }
      ],
      "source": [
        "!pip install pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCRKTux3HJuH",
        "outputId": "b9e5ecd6-dabe-4abb-9cfd-60bfe178a416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Using device: cuda\n",
            "GPU Device: NVIDIA L4\n",
            "Available GPU memory: 22.17 GB\n"
          ]
        }
      ],
      "source": [
        "from random import sample\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)  # Should print \"1.23.5\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.amp import autocast, GradScaler  # Import automatic mixed precision tools\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from opacus import PrivacyEngine\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Set up device - prioritize GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Print GPU info if available\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSOP969tMhsE"
      },
      "outputs": [],
      "source": [
        "## Clear GPU cache and storage\n",
        "torch.cuda.empty_cache()  # Frees unused memory\n",
        "torch.cuda.ipc_collect()  # Collects shared memory used in multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06uZqEV3rGAe"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5eyCXarG33",
        "outputId": "b7d21304-147d-442d-fcfe-10986b3877f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in successfully!\n"
          ]
        }
      ],
      "source": [
        "# Retrieve token securely\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"Logged in successfully!\")\n",
        "else:\n",
        "    print(\"Hugging Face token not found. Please set it using `userdata.set`.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8WJhCIZMuRB"
      },
      "source": [
        "## CPU and GPU util functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebizwy4ZMnlk",
        "outputId": "6aaabc90-2011-444a-e4d3-65a7cd0fe090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¹ CPU Stats:\n",
            "\n",
            "ðŸ”¹ RAM Stats:\n",
            "\n",
            "ðŸ”¹ GPU Stats:\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetUtilizationRates, nvmlSystemGetDriverVersion, nvmlDeviceGetName, nvmlShutdown\n",
        "    nvmlInit()\n",
        "    NVML_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NVML_AVAILABLE = False\n",
        "\n",
        "def get_cpu_stats():\n",
        "    \"\"\" Get CPU usage stats \"\"\"\n",
        "    cpu_usage = psutil.cpu_percent(interval=1)  # Get CPU usage %\n",
        "    cpu_freq = psutil.cpu_freq().current if psutil.cpu_freq() else \"Unknown\"  # CPU Frequency\n",
        "    num_cores = psutil.cpu_count(logical=False)  # Physical Cores\n",
        "    num_threads = psutil.cpu_count(logical=True)  # Logical Cores\n",
        "    print(f\"CPU Usage: {cpu_usage}%\")\n",
        "    print(f\"CPU Frequency: {cpu_freq} MHz\")\n",
        "    print(f\"Physical Cores: {num_cores}\")\n",
        "    print(f\"Logical Cores: {num_threads}\")\n",
        "\n",
        "def get_ram_stats():\n",
        "    \"\"\" Get system RAM stats \"\"\"\n",
        "    ram = psutil.virtual_memory()\n",
        "    print(\"Total RAM:\", round(ram.total / 1e9, 2), \"GB\")\n",
        "    print(\"Available RAM:\", round(ram.available / 1e9, 2), \"GB\")\n",
        "    print(\"Used RAM:\", round(ram.used / 1e9, 2), \"GB\")\n",
        "    print(\"RAM Usage:\", ram.percent, \"%\")\n",
        "\n",
        "def get_gpu_stats():\n",
        "    \"\"\" Get GPU stats if available \"\"\"\n",
        "    if not NVML_AVAILABLE:\n",
        "        return {\"Error\": \"pynvml not installed. Run: pip install nvidia-ml-py3\"}\n",
        "\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "\n",
        "    for i in range(num_gpus):\n",
        "        handle = nvmlDeviceGetHandleByIndex(i)\n",
        "        mem_info = nvmlDeviceGetMemoryInfo(handle)\n",
        "        utilization = nvmlDeviceGetUtilizationRates(handle)\n",
        "\n",
        "        print(f\"GPU {i} - {nvmlDeviceGetName(handle)}\")\n",
        "        print(f\"Driver Version: {nvmlSystemGetDriverVersion()}\")\n",
        "        print(f\"Total VRAM: {round(mem_info.total / 1e9, 2)} GB\")\n",
        "        print(f\"Used VRAM: {round(mem_info.used / 1e9, 2)} GB\")\n",
        "        print(f\"Free VRAM: {round(mem_info.free / 1e9, 2)} GB\")\n",
        "        print(f\"GPU Usage: {utilization.gpu}%\")\n",
        "        print()\n",
        "\n",
        "    nvmlShutdown()  # Clean up NVML\n",
        "\n",
        "# Run and print system stats\n",
        "\n",
        "print(\"\\nðŸ”¹ CPU Stats:\", )\n",
        "print(\"\\nðŸ”¹ RAM Stats:\", )\n",
        "print(\"\\nðŸ”¹ GPU Stats:\", )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykEN9tloMw_h"
      },
      "source": [
        "## CPU & GPU specs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SymnXtWfM024",
        "outputId": "738ad921-e329-4a99-fc25-d4cf0e387eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Usage: 8.0%\n",
            "CPU Frequency: 2200.2020000000007 MHz\n",
            "Physical Cores: 6\n",
            "Logical Cores: 12\n"
          ]
        }
      ],
      "source": [
        "get_cpu_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXzbNF9UM1-a",
        "outputId": "55d10fb5-26b2-4932-e4d8-c2a1ce896a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total RAM: 50.53 GB\n",
            "Available RAM: 48.34 GB\n",
            "Used RAM: 1.55 GB\n",
            "RAM Usage: 4.3 %\n"
          ]
        }
      ],
      "source": [
        "get_ram_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBp4-KgEM3CQ",
        "outputId": "4fee72db-0c1b-462a-f50c-4d11fd1eafe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0 - NVIDIA L4\n",
            "Driver Version: 535.104.05\n",
            "Total VRAM: 24.15 GB\n",
            "Used VRAM: 0.36 GB\n",
            "Free VRAM: 23.8 GB\n",
            "GPU Usage: 0%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "get_gpu_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkAUFwP4HM5p"
      },
      "source": [
        "## Model Loading and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "9c3f9b9aa6424fec9c1a51cbfe1a44d5",
            "a0113e3179d948c395cc90d485c7415d",
            "851ef30e404a49978fd5067c72dda3e0",
            "bbc4590ce08747eba9d9afa0a205304f",
            "5e72ef4743a3457da85217110d83ebfc",
            "565bfbb1bd004a1fb35450536f34838f",
            "29d91c8076f444d09a3e5941cad7caec",
            "7918ee2aeece48beb5bb99835e94055e",
            "ea280ee5e4fa4d768bf60645ff5dd0f1",
            "3b15598576834a039a085f6b8a9491c2",
            "e94df0ce5ac746b3be53b9275d8a5558"
          ]
        },
        "id": "TPZT02dfHLb5",
        "outputId": "4655262a-15de-491a-8c26-321b712cfb4d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c3f9b9aa6424fec9c1a51cbfe1a44d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pad, token doesnt exists, using EOS token\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(128256, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Load Pretrained Model and Tokenizer\n",
        "# model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
        "# model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "LLAMA_1B = \"meta-llama/Llama-3.2-1B\"\n",
        "LLAMA_8B = \"meta-llama/Llama-3.1-8B\"\n",
        "\n",
        "model_name = LLAMA_8B if RUN_MODE == \"main\" else LLAMA_1B\n",
        "\n",
        "# This line downloads (if needed) and initializes a tokenizer using the identifier stored in model_name.\n",
        "# The tokenizer converts text into a numerical format (tokens) that the model can process,\n",
        "# and it also handles the reverse process (converting tokens back to human-readable text).\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# This line loads a pre-trained causal language model (such as GPT-style models) using the same model identifier.\n",
        "# It retrieves the model architecture and its pre-trained weights so you can use it for tasks like text generation.\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,       # Loads model in FP16\n",
        "    device_map=\"auto\"                # Automatically distributes model across devices if needed\n",
        ")\n",
        "\n",
        "# !! NEW\n",
        "# Freeze all model parameters (ensuring no gradients are computed for the base model)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Ensure a pad token exists (set to eos token if not present).\n",
        "# 1. Check for the padding token id. If none, use the eos_token as the padding token\n",
        "if tokenizer.pad_token_id is None:\n",
        "    print(\"pad, token doesnt exists, using EOS token\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Adjusts the model's token embedding matrix to match the size of the tokenizer's vocabulary.\n",
        "# This is important because adding or changing tokens (like defining a pad token)\n",
        "# may change the size of the vocabulary, and the model's embedding layer needs to reflect that change.\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8h8bWh_HRTv"
      },
      "source": [
        "## LoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKP9YMKBdIpe"
      },
      "outputs": [],
      "source": [
        "# To get all the intermediate layer config of the model\n",
        "# for name, module in model.named_modules():\n",
        "#     print(name, \":\", module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZffBS3pRHUrK",
        "outputId": "98fcd1cd-ca05-45c8-f4cf-9758e821ab79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA applied. Trainable parameters:\n",
            "trainable params: 16,252,928 || all params: 8,046,514,176 || trainable%: 0.2020\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Enable gradient checkpointing to save memory.\n",
        "\n",
        "# This technique reduces memory usage during training by not storing all intermediate activations\n",
        "# during the forward pass. Instead, it saves only a subset of them and recomputes the missing ones\n",
        "# during the backward pass.\n",
        "model.config.gradient_checkpointing = True\n",
        "\n",
        "# Configure LoRA: update only a small set of additional parameters.\n",
        "# tried r=4 and lora+alpha = 32. Maybe that destabilized training so modifying to 8 and 16 respectively\n",
        "#initally was 0.1, changing to 0.05\n",
        "\n",
        "# studies say best to apply Lora to all layers\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,  # Fine-tuning for causal language modeling.\n",
        "    inference_mode=False,          # Training mode.\n",
        "    r=8,                           # Rank of low-rank decomposition.\n",
        "    lora_alpha=16,                 # Scaling factor.\n",
        "    lora_dropout=0.05,               # Dropout rate for LoRA layers.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# This function call takes the pre-trained model and applies the LoRA configuration you defined.\n",
        "# It modifies the model so that, instead of updating all parameters during fine-tuning,\n",
        "# only a small subset (the LoRA adapters) is trained.\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"LoRA applied. Trainable parameters:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Move the model to the chosen device and set to training mode.\n",
        "model.to(device)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fulb2eoMHXBD"
      },
      "source": [
        "## Data loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooqU-XeiHY3Q"
      },
      "outputs": [],
      "source": [
        "# Load and Format Training Data\n",
        "import json\n",
        "\n",
        "formatted_strings = []\n",
        "\n",
        "with open(\"finetuning/train.jsonl\", \"r\") as f:\n",
        "    j = 0\n",
        "    for line in f:\n",
        "        # Parse the JSON data from the line\n",
        "        data = json.loads(line.strip())\n",
        "        # Extract values\n",
        "        rating = data['Rating']\n",
        "        title = data['Title']\n",
        "        review = data['Review']\n",
        "\n",
        "        # Format the string as per the required format\n",
        "        formatted_string = f'\"System prompt : Given the Rating and Title, you are required to generate the review\" | \"Rating\": {rating} | \"Title\": {title} | \"Review\": {review}'\n",
        "\n",
        "        # Add the formatted string to the list\n",
        "        formatted_strings.append(formatted_string)\n",
        "\n",
        "        if RUN_MODE == \"test\":\n",
        "          j += 1\n",
        "          if j == 1000:\n",
        "              break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPe6k7Dmdtwp",
        "outputId": "6e67ce7d-73fe-433d-839d-e4fd3a728436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  100000\n",
            "\"System prompt : Given the Rating and Title, you are required to generate the review\" | \"Rating\": 4 | \"Title\": No white background! Itâ€™s clear! | \"Review\": I bought this bc I thought it had the nice white background. Turns out itâ€™s clear & since my phone is blue it doesnâ€™t look anything like this.  If I had known that I would have purchased something else. It works ok.\n",
            "length of largets string is:  383.47625\n"
          ]
        }
      ],
      "source": [
        "# Now `formatted_strings` contains the list of strings in the desired format\n",
        "print(\"Size: \",len(formatted_strings))\n",
        "print(formatted_strings[0])\n",
        "train_texts = formatted_strings\n",
        "strs = [len(formatted_str) for formatted_str in formatted_strings]\n",
        "print(\"length of largets string is: \",sum(strs) / len(strs))\n",
        "# avg around 328"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCsd5i7NH17t"
      },
      "source": [
        "## Data tokenization and dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV5ygoW-H6IH",
        "outputId": "f72768ea-cd04-40ad-a892-ef0ad34b419a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: torch.Size([100000, 128])\n"
          ]
        }
      ],
      "source": [
        "# !! NEW - max_length=512\n",
        "\n",
        "DATA_PARAMS = {\n",
        "  \"max_length\": 128,\n",
        "  \"batch_size\": 4,\n",
        "}\n",
        "\n",
        "# Tokenize training texts with padding and truncation.\n",
        "encodings = tokenizer(train_texts, return_tensors='pt', padding=True, truncation=True, max_length=DATA_PARAMS['max_length'])\n",
        "input_ids = encodings['input_ids']\n",
        "attention_mask = encodings['attention_mask']\n",
        "\n",
        "# For causal language modeling, use input_ids as labels.\n",
        "# Replace pad token positions with -100 so that they are ignored by the loss.\n",
        "\n",
        "#creates a copy of your input IDs, so you can modify them without affecting the original tensor.\n",
        "labels = input_ids.clone()\n",
        "\n",
        "#replaces all padding token positions with -100. This is a common convention (especially with PyTorchâ€™s CrossEntropyLoss)\n",
        "# to indicate that these positions should be ignored during loss computatio\n",
        "labels[input_ids == tokenizer.pad_token_id] = -100\n",
        "\n",
        "print(\"Training data shape:\", input_ids.shape)\n",
        "\n",
        "\n",
        "# !! NEW - num_workers=4, pin_memory=True\n",
        "# Create a TensorDataset and DataLoader with a small batch size.\n",
        "train_dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=DATA_PARAMS['batch_size'], shuffle=True, drop_last=True, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yda7cPXHH9kl"
      },
      "source": [
        "## Optimizer & Privacy engine setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNoZnmXSeXhv"
      },
      "outputs": [],
      "source": [
        "# !! NEW\n",
        "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4pqKECaH-7T",
        "outputId": "2003cf8b-5b84-46ae-ac16-c1fe288de750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "privacy_engine = PrivacyEngine()\n",
        "model, optimizer, train_loader = privacy_engine.make_private(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    noise_multiplier=0.3,      # Lower noise multiplier to reduce added noise\n",
        "    max_grad_norm=5,           # Increase clipping norm to allow larger gradients\n",
        "    batch_first=True,\n",
        "    loss_reduction=\"mean\",\n",
        "    poisson_sampling=False      # UPDATE - ERRORING OUT, SO NOT USING. Use Poisson sampling for potentially more stable training\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydDru4H6pMXR"
      },
      "source": [
        "## Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLr62h6jpLpO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import datetime\n",
        "import pytz # PST time zone\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "class TrainingTracker:\n",
        "    def __init__(self, base_dir=\"./tracking_results\"):\n",
        "        \"\"\"\n",
        "        Initialize the training tracker.\n",
        "\n",
        "        Args:\n",
        "            base_dir: Directory to save tracking results\n",
        "        \"\"\"\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.base_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        # Generate a unique run ID based on timestamp\n",
        "        # Define PST timezone\n",
        "        pst = pytz.timezone(\"America/Los_Angeles\")\n",
        "        # Get current time in PST\n",
        "        pst_time = datetime.datetime.now(pytz.utc).astimezone(pst)\n",
        "        # Format the time\n",
        "        timestamp = pst_time.strftime(\"%d-%m_%H-%M-%S\")\n",
        "        # timestamp = datetime.datetime.now().strftime(\"%d-%m_%H-%M-%S\")\n",
        "        self.run_id = f\"run_{timestamp}\"\n",
        "\n",
        "        # Create run directory\n",
        "        self.run_dir = self.base_dir / self.run_id\n",
        "        self.run_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Initialize tracking data structures\n",
        "        self.params = {}\n",
        "        self.epoch_metrics = []\n",
        "        self.generated_samples = []\n",
        "        self.privacy_metrics = {}\n",
        "\n",
        "    def record_parameters(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Record training parameters for the current run.\n",
        "\n",
        "        Args:\n",
        "            **kwargs: Key-value pairs of parameters to record\n",
        "        \"\"\"\n",
        "        self.params.update(kwargs)\n",
        "\n",
        "        # Save parameters to file\n",
        "        with open(self.run_dir / \"parameters.json\", \"w\") as f:\n",
        "            json.dump(self.params, f, indent=4)\n",
        "\n",
        "    def record_epoch_metrics(self, epoch, loss, batch_times=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Record metrics for a training epoch.\n",
        "\n",
        "        Args:\n",
        "            epoch: Current epoch number\n",
        "            loss: Loss value for the epoch\n",
        "            batch_times: Optional list of batch processing times\n",
        "            **kwargs: Additional metrics to record\n",
        "        \"\"\"\n",
        "        metrics = {\n",
        "            \"epoch\": epoch,\n",
        "            \"loss\": loss,\n",
        "            **kwargs\n",
        "        }\n",
        "\n",
        "        if batch_times:\n",
        "            metrics[\"avg_batch_time\"] = sum(batch_times) / len(batch_times)\n",
        "            metrics[\"min_batch_time\"] = min(batch_times)\n",
        "            metrics[\"max_batch_time\"] = max(batch_times)\n",
        "\n",
        "        self.epoch_metrics.append(metrics)\n",
        "\n",
        "        # Save updated metrics to file\n",
        "        with open(self.run_dir / \"epoch_metrics.json\", \"w\") as f:\n",
        "            json.dump(self.epoch_metrics, f, indent=4)\n",
        "\n",
        "        # Also save as CSV for easier analysis\n",
        "        pd.DataFrame(self.epoch_metrics).to_csv(\n",
        "            self.run_dir / \"epoch_metrics.csv\", index=False)\n",
        "\n",
        "    def record_privacy_budget(self, epsilon, delta=1e-5, **kwargs):\n",
        "        \"\"\"\n",
        "        Record privacy budget metrics.\n",
        "\n",
        "        Args:\n",
        "            epsilon: Achieved epsilon value\n",
        "            delta: Delta value used\n",
        "            **kwargs: Additional privacy metrics\n",
        "        \"\"\"\n",
        "        self.privacy_metrics = {\n",
        "            \"epsilon\": epsilon,\n",
        "            \"delta\": delta,\n",
        "            **kwargs\n",
        "        }\n",
        "\n",
        "        # Save privacy metrics to file\n",
        "        with open(self.run_dir / \"privacy_metrics.json\", \"w\") as f:\n",
        "            json.dump(self.privacy_metrics, f, indent=4)\n",
        "\n",
        "    def record_sample(self, prompt, generated_text):\n",
        "        \"\"\"\n",
        "        Record a sample of generated text.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input prompt\n",
        "            generated_text: Generated text output\n",
        "        \"\"\"\n",
        "        sample = {\n",
        "            \"prompt\": prompt,\n",
        "            \"generated_text\": generated_text,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self.generated_samples.append(sample)\n",
        "\n",
        "        # Save samples to file\n",
        "        with open(self.run_dir / \"generated_samples.json\", \"w\") as f:\n",
        "            json.dump(self.generated_samples, f, indent=4)\n",
        "\n",
        "    def save_model_info(self, model_path, model_type, tokenizer_info=None):\n",
        "        \"\"\"\n",
        "        Record information about the saved model.\n",
        "\n",
        "        Args:\n",
        "            model_path: Path where model was saved\n",
        "            model_type: Type of model (e.g., \"with_dp\", \"without_dp\")\n",
        "            tokenizer_info: Additional tokenizer information\n",
        "        \"\"\"\n",
        "        model_info = {\n",
        "            \"model_path\": str(model_path),\n",
        "            \"model_type\": model_type,\n",
        "            \"tokenizer_info\": tokenizer_info or {}\n",
        "        }\n",
        "\n",
        "        # Save model info to file\n",
        "        with open(self.run_dir / \"model_info.json\", \"w\") as f:\n",
        "            json.dump(model_info, f, indent=4)\n",
        "\n",
        "    def generate_summary(self):\n",
        "        \"\"\"\n",
        "        Generate a summary of the training run.\n",
        "\n",
        "        Returns:\n",
        "            str: Summary text\n",
        "        \"\"\"\n",
        "        summary_lines = [\n",
        "            f\"Training Run: {self.run_id}\",\n",
        "            \"=\" * 50,\n",
        "            \"\\nParameters:\",\n",
        "        ]\n",
        "\n",
        "        for key, value in self.params.items():\n",
        "            summary_lines.append(f\"  {key}: {value}\")\n",
        "\n",
        "        if self.epoch_metrics:\n",
        "            summary_lines.extend([\n",
        "                \"\\nTraining Results:\",\n",
        "                f\"  Epochs completed: {len(self.epoch_metrics)}\",\n",
        "                f\"  Final loss: {self.epoch_metrics[-1]['loss']:.6f}\",\n",
        "                f\"  Initial loss: {self.epoch_metrics[0]['loss']:.6f}\",\n",
        "                f\"  Loss reduction: {self.epoch_metrics[0]['loss'] - self.epoch_metrics[-1]['loss']:.6f}\"\n",
        "            ])\n",
        "\n",
        "        if self.privacy_metrics:\n",
        "            summary_lines.extend([\n",
        "                \"\\nPrivacy Budget:\",\n",
        "                f\"  Epsilon: {self.privacy_metrics['epsilon']:.4f}\",\n",
        "                f\"  Delta: {self.privacy_metrics['delta']}\"\n",
        "            ])\n",
        "\n",
        "        summary_text = \"\\n\".join(summary_lines)\n",
        "\n",
        "        # Save summary to file\n",
        "        with open(self.run_dir / \"summary.txt\", \"w\") as f:\n",
        "            f.write(summary_text)\n",
        "\n",
        "        return summary_text\n",
        "\n",
        "    # def compare_with_previous_runs(self, metric=\"loss\"):\n",
        "    #     \"\"\"\n",
        "    #     Compare this run with previous runs based on a specific metric.\n",
        "\n",
        "    #     Args:\n",
        "    #         metric: Metric to compare (default: \"loss\")\n",
        "\n",
        "    #     Returns:\n",
        "    #         DataFrame: Comparison data\n",
        "    #     \"\"\"\n",
        "    #     # Collect data from all previous runs\n",
        "    #     all_runs = []\n",
        "\n",
        "    #     for run_dir in self.base_dir.iterdir():\n",
        "    #         if not run_dir.is_dir() or run_dir == self.run_dir:\n",
        "    #             continue\n",
        "\n",
        "    #         params_file = run_dir / \"parameters.json\"\n",
        "    #         metrics_file = run_dir / \"epoch_metrics.json\"\n",
        "\n",
        "    #         if params_file.exists() and metrics_file.exists():\n",
        "    #             with open(params_file, \"r\") as f:\n",
        "    #                 params = json.load(f)\n",
        "\n",
        "    #             with open(metrics_file, \"r\") as f:\n",
        "    #                 metrics = json.load(f)\n",
        "\n",
        "    #             if metrics:\n",
        "    #                 final_metric = metrics[-1].get(metric)\n",
        "\n",
        "    #                 run_data = {\n",
        "    #                     \"run_id\": run_dir.name,\n",
        "    #                     f\"final_{metric}\": final_metric,\n",
        "    #                     **params\n",
        "    #                 }\n",
        "\n",
        "    #                 all_runs.append(run_data)\n",
        "\n",
        "    #     # Add current run\n",
        "    #     if self.epoch_metrics:\n",
        "    #         current_run_data = {\n",
        "    #             \"run_id\": self.run_id,\n",
        "    #             f\"final_{metric}\": self.epoch_metrics[-1].get(metric),\n",
        "    #             **self.params\n",
        "    #         }\n",
        "    #         all_runs.append(current_run_data)\n",
        "\n",
        "    #     # Convert to DataFrame and sort\n",
        "    #     if all_runs:\n",
        "    #         df = pd.DataFrame(all_runs)\n",
        "    #         df = df.sort_values(by=f\"final_{metric}\")\n",
        "\n",
        "    #         # Save comparison to file\n",
        "    #         df.to_csv(self.run_dir / f\"comparison_{metric}.csv\", index=False)\n",
        "\n",
        "    #         return df\n",
        "\n",
        "    #     return pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNSHqUqOCjWS"
      },
      "outputs": [],
      "source": [
        "# Record initial parameters after setting them up\n",
        "def record_initial_params():\n",
        "    # Record model configuration\n",
        "    tracker.record_parameters(\n",
        "        model_name=model_name,\n",
        "        device=str(device),\n",
        "        epochs=epochs,\n",
        "        batch_size=train_loader.batch_size,\n",
        "        learning_rate=optimizer.param_groups[0]['lr'],\n",
        "        gradient_accumulation_steps=accumulation_steps,\n",
        "\n",
        "        # LoRA parameters\n",
        "        lora_r=lora_config.r,\n",
        "        lora_alpha=lora_config.lora_alpha,\n",
        "        lora_dropout=lora_config.lora_dropout,\n",
        "        lora_target_modules=list(lora_config.target_modules),\n",
        "\n",
        "        # Privacy parameters (if using Opacus)\n",
        "        using_differential_privacy=hasattr(model, \"remove_hooks\"),\n",
        "        noise_multiplier=0.6 if hasattr(model, \"remove_hooks\") else None,\n",
        "        max_grad_norm=1.5 if hasattr(model, \"remove_hooks\") else None,\n",
        "\n",
        "        # Dataset info\n",
        "        dataset_size=len(formatted_strings),\n",
        "        avg_sample_length=sum(len(s) for s in formatted_strings) / len(formatted_strings),\n",
        "        tokenizer_max_length=DATA_PARAMS['max_length'],  # From tokenization step\n",
        "        data_batch_size=DATA_PARAMS['batch_size'],\n",
        "\n",
        "        # Tokenizer info\n",
        "        tokenizer_vocab_size=len(tokenizer),\n",
        "        tokenizer_model_max_length=tokenizer.model_max_length,\n",
        "\n",
        "\n",
        "        # System info\n",
        "        cuda_available=torch.cuda.is_available(),\n",
        "        gpu_name=torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjdbmm3wIB0M"
      },
      "source": [
        "## Training setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iRZnGXfGgij"
      },
      "outputs": [],
      "source": [
        "epochs = 4 if RUN_MODE == \"main\" else 1\n",
        "\n",
        "# !! NEW\n",
        "scaler = GradScaler('cuda')  # Create a gradient scaler to manage FP16 stability\n",
        "accumulation_steps = 1  # Set gradient accumulation steps; use >1 to simulate larger batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxzk9pcCCacI"
      },
      "outputs": [],
      "source": [
        "# Initialize the tracker before loading the model\n",
        "tracker = TrainingTracker()\n",
        "# Call this after all parameters are set but before training starts\n",
        "record_initial_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aZm-yQCJOdve",
        "outputId": "2ca8b163-bedd-4f0d-9d61-98c1eeb530ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradSampleModule(PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              "))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Training loop\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity check\n"
      ],
      "metadata": {
        "id": "l7oX2UspPfxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SjuGRlmwPg2r",
        "outputId": "6a67f16e-b537-4be2-beca-727ea7021204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'meta-llama/Llama-3.1-8B'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMVloS-nPiBu",
        "outputId": "13789b07-7902-41ff-e9ec-d070c734f5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guGBiDbgPi8_",
        "outputId": "205be277-362d-444b-d4d6-e83e1438339f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "iQyW5yCiPd68"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCilSHLsfGGa"
      },
      "outputs": [],
      "source": [
        "# # FINETUNING, NOT WORKING NOW\n",
        "# for epoch in range(epochs):  # Loop over each epoch\n",
        "#     total_loss = 0.0  # Initialize total loss accumulator for the epoch\n",
        "#     optimizer.zero_grad()  # Zero gradients at the start of the epoch\n",
        "#     for i, batch in enumerate(train_loader):  # Loop over mini-batches from the DataLoader\n",
        "#         # Move each tensor in the batch to the device (GPU) asynchronously if pin_memory is True\n",
        "#         input_ids_batch, attention_mask_batch, labels_batch = [\n",
        "#             x.to(device, non_blocking=True) for x in batch\n",
        "#         ]\n",
        "\n",
        "#         # Determine the sequence length for the current batch and create position IDs accordingly\n",
        "#         seq_len = input_ids_batch.size(1)  # Get the sequence length from the input tensor\n",
        "#         # Create a tensor [0, 1, ..., seq_len-1] and repeat it for each item in the batch\n",
        "#         position_ids = torch.arange(seq_len, device=device).unsqueeze(0).repeat(input_ids_batch.size(0), 1)\n",
        "\n",
        "#         # Use mixed precision context for the forward pass to save memory and speed up computation\n",
        "#         with autocast():\n",
        "#             outputs = model(\n",
        "#                 input_ids=input_ids_batch,        # Input token IDs for the model\n",
        "#                 attention_mask=attention_mask_batch,  # Attention mask to differentiate padded tokens\n",
        "#                 position_ids=position_ids,          # Positional IDs for the tokens\n",
        "#                 labels=labels_batch                 # Labels for computing the loss (typically same as input_ids for causal LM)\n",
        "#             )\n",
        "#             # Compute the loss; if using gradient accumulation, scale down the loss accordingly\n",
        "#             loss = outputs.loss / accumulation_steps\n",
        "\n",
        "#         # Scale the loss and perform the backward pass using the GradScaler for FP16 stability\n",
        "#         scaler.scale(loss).backward()\n",
        "\n",
        "#         # Every 'accumulation_steps' iterations, update the model weights\n",
        "#         if (i + 1) % accumulation_steps == 0:\n",
        "#             scaler.step(optimizer)  # Update parameters using scaled gradients\n",
        "#             scaler.update()         # Update the scale for the next iteration\n",
        "#             optimizer.zero_grad()   # Reset gradients after updating\n",
        "\n",
        "#         # Accumulate the loss (multiply back to undo the earlier division, so total_loss is in original scale)\n",
        "#         total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "#         # Optionally, print progress every 50 batches\n",
        "#         if i % 50 == 0:\n",
        "#             print(f\"Batch {i} processed.\")\n",
        "\n",
        "#     # Compute the average loss over the epoch\n",
        "#     avg_loss = total_loss / len(train_loader)\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} - Average loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Guo1CVB3fLLr",
        "outputId": "19299495-c21a-4b31-f9e5-df5c1d3cb777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4 - Batch 0/25000 - Loss: 3.7414\n",
            "Epoch 1/4 - Batch 50/25000 - Loss: 3.5234\n",
            "Epoch 1/4 - Batch 100/25000 - Loss: 3.5384\n",
            "Epoch 1/4 - Batch 150/25000 - Loss: 3.6427\n",
            "Epoch 1/4 - Batch 200/25000 - Loss: 3.0998\n",
            "Epoch 1/4 - Batch 250/25000 - Loss: 3.3532\n",
            "Epoch 1/4 - Batch 300/25000 - Loss: 3.3488\n",
            "Epoch 1/4 - Batch 350/25000 - Loss: 3.1736\n",
            "Epoch 1/4 - Batch 400/25000 - Loss: 3.5294\n",
            "Epoch 1/4 - Batch 450/25000 - Loss: 3.2866\n",
            "Epoch 1/4 - Batch 500/25000 - Loss: 3.2757\n",
            "Epoch 1/4 - Batch 550/25000 - Loss: 3.2063\n",
            "Epoch 1/4 - Batch 600/25000 - Loss: 3.3168\n",
            "Epoch 1/4 - Batch 650/25000 - Loss: 3.2022\n",
            "Epoch 1/4 - Batch 700/25000 - Loss: 3.4977\n",
            "Epoch 1/4 - Batch 750/25000 - Loss: 3.5639\n",
            "Epoch 1/4 - Batch 800/25000 - Loss: 3.4709\n",
            "Epoch 1/4 - Batch 850/25000 - Loss: 3.3943\n",
            "Epoch 1/4 - Batch 900/25000 - Loss: 3.5366\n",
            "Epoch 1/4 - Batch 950/25000 - Loss: 3.1521\n",
            "Epoch 1/4 - Batch 1000/25000 - Loss: 3.3253\n",
            "Epoch 1/4 - Batch 1050/25000 - Loss: 2.9724\n",
            "Epoch 1/4 - Batch 1100/25000 - Loss: 3.2340\n",
            "Epoch 1/4 - Batch 1150/25000 - Loss: 3.1714\n",
            "Epoch 1/4 - Batch 1200/25000 - Loss: 3.3290\n",
            "Epoch 1/4 - Batch 1250/25000 - Loss: 3.5038\n",
            "Epoch 1/4 - Batch 1300/25000 - Loss: 3.3654\n",
            "Epoch 1/4 - Batch 1350/25000 - Loss: 3.7011\n",
            "Epoch 1/4 - Batch 1400/25000 - Loss: 3.6518\n",
            "Epoch 1/4 - Batch 1450/25000 - Loss: 3.7676\n",
            "Epoch 1/4 - Batch 1500/25000 - Loss: 3.2439\n",
            "Epoch 1/4 - Batch 1550/25000 - Loss: 3.2515\n",
            "Epoch 1/4 - Batch 1600/25000 - Loss: 3.3733\n",
            "Epoch 1/4 - Batch 1650/25000 - Loss: 3.2326\n",
            "Epoch 1/4 - Batch 1700/25000 - Loss: 3.3693\n",
            "Epoch 1/4 - Batch 1750/25000 - Loss: 3.6607\n",
            "Epoch 1/4 - Batch 1800/25000 - Loss: 3.2636\n",
            "Epoch 1/4 - Batch 1850/25000 - Loss: 3.4240\n",
            "Epoch 1/4 - Batch 1900/25000 - Loss: 3.6546\n",
            "Epoch 1/4 - Batch 1950/25000 - Loss: 3.5170\n",
            "Epoch 1/4 - Batch 2000/25000 - Loss: 3.1886\n",
            "Epoch 1/4 - Batch 2050/25000 - Loss: 3.4198\n",
            "Epoch 1/4 - Batch 2100/25000 - Loss: 3.5309\n",
            "Epoch 1/4 - Batch 2150/25000 - Loss: 3.3868\n",
            "Epoch 1/4 - Batch 2200/25000 - Loss: 3.3063\n",
            "Epoch 1/4 - Batch 2250/25000 - Loss: 3.1772\n",
            "Epoch 1/4 - Batch 2300/25000 - Loss: 3.0737\n",
            "Epoch 1/4 - Batch 2350/25000 - Loss: 3.6058\n",
            "Epoch 1/4 - Batch 2400/25000 - Loss: 3.6395\n",
            "Epoch 1/4 - Batch 2450/25000 - Loss: 3.1682\n",
            "Epoch 1/4 - Batch 2500/25000 - Loss: 3.2785\n",
            "Epoch 1/4 - Batch 2550/25000 - Loss: 3.3614\n",
            "Epoch 1/4 - Batch 2600/25000 - Loss: 3.2738\n",
            "Epoch 1/4 - Batch 2650/25000 - Loss: 3.3664\n",
            "Epoch 1/4 - Batch 2700/25000 - Loss: 3.2552\n",
            "Epoch 1/4 - Batch 2750/25000 - Loss: 3.2509\n",
            "Epoch 1/4 - Batch 2800/25000 - Loss: 3.3900\n",
            "Epoch 1/4 - Batch 2850/25000 - Loss: 3.7158\n",
            "Epoch 1/4 - Batch 2900/25000 - Loss: 3.0599\n",
            "Epoch 1/4 - Batch 2950/25000 - Loss: 3.7182\n",
            "Epoch 1/4 - Batch 3000/25000 - Loss: 3.0824\n",
            "Epoch 1/4 - Batch 3050/25000 - Loss: 3.3493\n",
            "Epoch 1/4 - Batch 3100/25000 - Loss: 2.8217\n",
            "Epoch 1/4 - Batch 3150/25000 - Loss: 2.8992\n",
            "Epoch 1/4 - Batch 3200/25000 - Loss: 3.5250\n",
            "Epoch 1/4 - Batch 3250/25000 - Loss: 3.1663\n",
            "Epoch 1/4 - Batch 3300/25000 - Loss: 3.0707\n",
            "Epoch 1/4 - Batch 3350/25000 - Loss: 3.2426\n",
            "Epoch 1/4 - Batch 3400/25000 - Loss: 2.9095\n",
            "Epoch 1/4 - Batch 3450/25000 - Loss: 3.1516\n",
            "Epoch 1/4 - Batch 3500/25000 - Loss: 3.0835\n",
            "Epoch 1/4 - Batch 3550/25000 - Loss: 2.9674\n",
            "Epoch 1/4 - Batch 3600/25000 - Loss: 3.1092\n",
            "Epoch 1/4 - Batch 3650/25000 - Loss: 2.8999\n",
            "Epoch 1/4 - Batch 3700/25000 - Loss: 3.1293\n",
            "Epoch 1/4 - Batch 3750/25000 - Loss: 2.9346\n",
            "Epoch 1/4 - Batch 3800/25000 - Loss: 2.8866\n",
            "Epoch 1/4 - Batch 3850/25000 - Loss: 3.0741\n",
            "Epoch 1/4 - Batch 3900/25000 - Loss: 3.0630\n",
            "Epoch 1/4 - Batch 3950/25000 - Loss: 3.1727\n",
            "Epoch 1/4 - Batch 4000/25000 - Loss: 3.2165\n",
            "Epoch 1/4 - Batch 4050/25000 - Loss: 3.5608\n",
            "Epoch 1/4 - Batch 4100/25000 - Loss: 3.4688\n",
            "Epoch 1/4 - Batch 4150/25000 - Loss: 3.4268\n",
            "Epoch 1/4 - Batch 4200/25000 - Loss: 2.9968\n",
            "Epoch 1/4 - Batch 4250/25000 - Loss: 3.1281\n",
            "Epoch 1/4 - Batch 4300/25000 - Loss: 3.2742\n",
            "Epoch 1/4 - Batch 4350/25000 - Loss: 3.1877\n",
            "Epoch 1/4 - Batch 4400/25000 - Loss: 3.0591\n",
            "Epoch 1/4 - Batch 4450/25000 - Loss: 3.3508\n",
            "Epoch 1/4 - Batch 4500/25000 - Loss: 3.5115\n",
            "Epoch 1/4 - Batch 4550/25000 - Loss: 3.2865\n",
            "Epoch 1/4 - Batch 4600/25000 - Loss: 2.9147\n",
            "Epoch 1/4 - Batch 4650/25000 - Loss: 2.8815\n",
            "Epoch 1/4 - Batch 4700/25000 - Loss: 3.1215\n",
            "Epoch 1/4 - Batch 4750/25000 - Loss: 3.0796\n",
            "Epoch 1/4 - Batch 4800/25000 - Loss: 2.9521\n",
            "Epoch 1/4 - Batch 4850/25000 - Loss: 3.5101\n",
            "Epoch 1/4 - Batch 4900/25000 - Loss: 3.0823\n",
            "Epoch 1/4 - Batch 4950/25000 - Loss: 3.1456\n",
            "Epoch 1/4 - Batch 5000/25000 - Loss: 3.0000\n",
            "Epoch 1/4 - Batch 5050/25000 - Loss: 3.6363\n",
            "Epoch 1/4 - Batch 5100/25000 - Loss: 2.8991\n",
            "Epoch 1/4 - Batch 5150/25000 - Loss: 3.0380\n",
            "Epoch 1/4 - Batch 5200/25000 - Loss: 3.0420\n",
            "Epoch 1/4 - Batch 5250/25000 - Loss: 2.9314\n",
            "Epoch 1/4 - Batch 5300/25000 - Loss: 3.1013\n",
            "Epoch 1/4 - Batch 5350/25000 - Loss: 3.1916\n",
            "Epoch 1/4 - Batch 5400/25000 - Loss: 3.2824\n",
            "Epoch 1/4 - Batch 5450/25000 - Loss: 3.4328\n",
            "Epoch 1/4 - Batch 5500/25000 - Loss: 3.0273\n",
            "Epoch 1/4 - Batch 5550/25000 - Loss: 3.0067\n",
            "Epoch 1/4 - Batch 5600/25000 - Loss: 3.2189\n",
            "Epoch 1/4 - Batch 5650/25000 - Loss: 2.9514\n",
            "Epoch 1/4 - Batch 5700/25000 - Loss: 2.8125\n",
            "Epoch 1/4 - Batch 5750/25000 - Loss: 3.5086\n",
            "Epoch 1/4 - Batch 5800/25000 - Loss: 3.0123\n",
            "Epoch 1/4 - Batch 5850/25000 - Loss: 3.0476\n",
            "Epoch 1/4 - Batch 5900/25000 - Loss: 3.0927\n",
            "Epoch 1/4 - Batch 5950/25000 - Loss: 3.1254\n",
            "Epoch 1/4 - Batch 6000/25000 - Loss: 2.8393\n",
            "Epoch 1/4 - Batch 6050/25000 - Loss: 2.9927\n",
            "Epoch 1/4 - Batch 6100/25000 - Loss: 2.9629\n",
            "Epoch 1/4 - Batch 6150/25000 - Loss: 3.1208\n",
            "Epoch 1/4 - Batch 6200/25000 - Loss: 2.6808\n",
            "Epoch 1/4 - Batch 6250/25000 - Loss: 3.1430\n",
            "Epoch 1/4 - Batch 6300/25000 - Loss: 2.8582\n",
            "Epoch 1/4 - Batch 6350/25000 - Loss: 2.7516\n",
            "Epoch 1/4 - Batch 6400/25000 - Loss: 2.7157\n",
            "Epoch 1/4 - Batch 6450/25000 - Loss: 2.9837\n",
            "Epoch 1/4 - Batch 6500/25000 - Loss: 3.0084\n",
            "Epoch 1/4 - Batch 6550/25000 - Loss: 2.8289\n",
            "Epoch 1/4 - Batch 6600/25000 - Loss: 3.0604\n",
            "Epoch 1/4 - Batch 6650/25000 - Loss: 3.0611\n",
            "Epoch 1/4 - Batch 6700/25000 - Loss: 3.1993\n",
            "Epoch 1/4 - Batch 6750/25000 - Loss: 2.6594\n",
            "Epoch 1/4 - Batch 6800/25000 - Loss: 2.6465\n",
            "Epoch 1/4 - Batch 6850/25000 - Loss: 2.8339\n",
            "Epoch 1/4 - Batch 6900/25000 - Loss: 2.8715\n",
            "Epoch 1/4 - Batch 6950/25000 - Loss: 3.1274\n",
            "Epoch 1/4 - Batch 7000/25000 - Loss: 2.8193\n",
            "Epoch 1/4 - Batch 7050/25000 - Loss: 2.9543\n",
            "Epoch 1/4 - Batch 7100/25000 - Loss: 3.3555\n",
            "Epoch 1/4 - Batch 7150/25000 - Loss: 2.5408\n",
            "Epoch 1/4 - Batch 7200/25000 - Loss: 2.6359\n",
            "Epoch 1/4 - Batch 7250/25000 - Loss: 2.7663\n",
            "Epoch 1/4 - Batch 7300/25000 - Loss: 2.4803\n",
            "Epoch 1/4 - Batch 7350/25000 - Loss: 2.4382\n",
            "Epoch 1/4 - Batch 7400/25000 - Loss: 2.7786\n",
            "Epoch 1/4 - Batch 7450/25000 - Loss: 3.2354\n",
            "Epoch 1/4 - Batch 7500/25000 - Loss: 3.1287\n",
            "Epoch 1/4 - Batch 7550/25000 - Loss: 2.5982\n",
            "Epoch 1/4 - Batch 7600/25000 - Loss: 2.6330\n",
            "Epoch 1/4 - Batch 7650/25000 - Loss: 2.9323\n",
            "Epoch 1/4 - Batch 7700/25000 - Loss: 2.6130\n",
            "Epoch 1/4 - Batch 7750/25000 - Loss: 2.7443\n",
            "Epoch 1/4 - Batch 7800/25000 - Loss: 2.6965\n",
            "Epoch 1/4 - Batch 7850/25000 - Loss: 2.6891\n",
            "Epoch 1/4 - Batch 7900/25000 - Loss: 2.7650\n",
            "Epoch 1/4 - Batch 7950/25000 - Loss: 2.4261\n",
            "Epoch 1/4 - Batch 8000/25000 - Loss: 2.4745\n",
            "Epoch 1/4 - Batch 8050/25000 - Loss: 2.9276\n",
            "Epoch 1/4 - Batch 8100/25000 - Loss: 2.6675\n",
            "Epoch 1/4 - Batch 8150/25000 - Loss: 2.8627\n",
            "Epoch 1/4 - Batch 8200/25000 - Loss: 2.4775\n",
            "Epoch 1/4 - Batch 8250/25000 - Loss: 2.8139\n",
            "Epoch 1/4 - Batch 8300/25000 - Loss: 2.5126\n",
            "Epoch 1/4 - Batch 8350/25000 - Loss: 2.3303\n",
            "Epoch 1/4 - Batch 8400/25000 - Loss: 2.6622\n",
            "Epoch 1/4 - Batch 8450/25000 - Loss: 2.5987\n",
            "Epoch 1/4 - Batch 8500/25000 - Loss: 2.5541\n",
            "Epoch 1/4 - Batch 8550/25000 - Loss: 2.5878\n",
            "Epoch 1/4 - Batch 8600/25000 - Loss: 2.2945\n",
            "Epoch 1/4 - Batch 8650/25000 - Loss: 2.8334\n",
            "Epoch 1/4 - Batch 8700/25000 - Loss: 2.3083\n",
            "Epoch 1/4 - Batch 8750/25000 - Loss: 2.6958\n",
            "Epoch 1/4 - Batch 8800/25000 - Loss: 2.6307\n",
            "Epoch 1/4 - Batch 8850/25000 - Loss: 2.2955\n",
            "Epoch 1/4 - Batch 8900/25000 - Loss: 2.5869\n",
            "Epoch 1/4 - Batch 8950/25000 - Loss: 2.4239\n",
            "Epoch 1/4 - Batch 9000/25000 - Loss: 2.4949\n",
            "Epoch 1/4 - Batch 9050/25000 - Loss: 2.7497\n",
            "Epoch 1/4 - Batch 9100/25000 - Loss: 2.3498\n",
            "Epoch 1/4 - Batch 9150/25000 - Loss: 2.6561\n",
            "Epoch 1/4 - Batch 9200/25000 - Loss: 2.2019\n",
            "Epoch 1/4 - Batch 9250/25000 - Loss: 2.3983\n",
            "Epoch 1/4 - Batch 9300/25000 - Loss: 2.5107\n",
            "Epoch 1/4 - Batch 9350/25000 - Loss: 2.4059\n",
            "Epoch 1/4 - Batch 9400/25000 - Loss: 2.4092\n",
            "Epoch 1/4 - Batch 9450/25000 - Loss: 2.6347\n",
            "Epoch 1/4 - Batch 9500/25000 - Loss: 2.5457\n",
            "Epoch 1/4 - Batch 9550/25000 - Loss: 2.3915\n",
            "Epoch 1/4 - Batch 9600/25000 - Loss: 2.2637\n",
            "Epoch 1/4 - Batch 9650/25000 - Loss: 2.5598\n",
            "Epoch 1/4 - Batch 9700/25000 - Loss: 2.1392\n",
            "Epoch 1/4 - Batch 9750/25000 - Loss: 2.2868\n",
            "Epoch 1/4 - Batch 9800/25000 - Loss: 2.0471\n",
            "Epoch 1/4 - Batch 9850/25000 - Loss: 2.3675\n",
            "Epoch 1/4 - Batch 9900/25000 - Loss: 2.5561\n",
            "Epoch 1/4 - Batch 9950/25000 - Loss: 2.2766\n",
            "Epoch 1/4 - Batch 10000/25000 - Loss: 2.3443\n",
            "Epoch 1/4 - Batch 10050/25000 - Loss: 2.3669\n",
            "Epoch 1/4 - Batch 10100/25000 - Loss: 2.4140\n",
            "Epoch 1/4 - Batch 10150/25000 - Loss: 2.3348\n",
            "Epoch 1/4 - Batch 10200/25000 - Loss: 1.9017\n",
            "Epoch 1/4 - Batch 10250/25000 - Loss: 2.5605\n",
            "Epoch 1/4 - Batch 10300/25000 - Loss: 2.2095\n",
            "Epoch 1/4 - Batch 10350/25000 - Loss: 1.7698\n",
            "Epoch 1/4 - Batch 10400/25000 - Loss: 2.5259\n",
            "Epoch 1/4 - Batch 10450/25000 - Loss: 1.8528\n",
            "Epoch 1/4 - Batch 10500/25000 - Loss: 2.4156\n",
            "Epoch 1/4 - Batch 10550/25000 - Loss: 2.3681\n",
            "Epoch 1/4 - Batch 10600/25000 - Loss: 2.2589\n",
            "Epoch 1/4 - Batch 10650/25000 - Loss: 1.7998\n",
            "Epoch 1/4 - Batch 10700/25000 - Loss: 1.6979\n",
            "Epoch 1/4 - Batch 10750/25000 - Loss: 2.1676\n",
            "Epoch 1/4 - Batch 10800/25000 - Loss: 1.7184\n",
            "Epoch 1/4 - Batch 10850/25000 - Loss: 2.0048\n",
            "Epoch 1/4 - Batch 10900/25000 - Loss: 2.3823\n",
            "Epoch 1/4 - Batch 10950/25000 - Loss: 2.4691\n",
            "Epoch 1/4 - Batch 11000/25000 - Loss: 2.0116\n",
            "Epoch 1/4 - Batch 11050/25000 - Loss: 2.0753\n",
            "Epoch 1/4 - Batch 11100/25000 - Loss: 2.0081\n",
            "Epoch 1/4 - Batch 11150/25000 - Loss: 1.7651\n",
            "Epoch 1/4 - Batch 11200/25000 - Loss: 1.8602\n",
            "Epoch 1/4 - Batch 11250/25000 - Loss: 2.0200\n",
            "Epoch 1/4 - Batch 11300/25000 - Loss: 2.1483\n",
            "Epoch 1/4 - Batch 11350/25000 - Loss: 2.2467\n",
            "Epoch 1/4 - Batch 11400/25000 - Loss: 2.0898\n",
            "Epoch 1/4 - Batch 11450/25000 - Loss: 1.8096\n",
            "Epoch 1/4 - Batch 11500/25000 - Loss: 1.5755\n",
            "Epoch 1/4 - Batch 11550/25000 - Loss: 2.2090\n",
            "Epoch 1/4 - Batch 11600/25000 - Loss: 1.8607\n",
            "Epoch 1/4 - Batch 11650/25000 - Loss: 2.1375\n",
            "Epoch 1/4 - Batch 11700/25000 - Loss: 2.0135\n",
            "Epoch 1/4 - Batch 11750/25000 - Loss: 2.1197\n",
            "Epoch 1/4 - Batch 11800/25000 - Loss: 2.3700\n",
            "Epoch 1/4 - Batch 11850/25000 - Loss: 1.9244\n",
            "Epoch 1/4 - Batch 11900/25000 - Loss: 1.7405\n",
            "Epoch 1/4 - Batch 11950/25000 - Loss: 1.7899\n",
            "Epoch 1/4 - Batch 12000/25000 - Loss: 1.5393\n",
            "Epoch 1/4 - Batch 12050/25000 - Loss: 1.9492\n",
            "Epoch 1/4 - Batch 12100/25000 - Loss: 2.1089\n",
            "Epoch 1/4 - Batch 12150/25000 - Loss: 2.0154\n",
            "Epoch 1/4 - Batch 12200/25000 - Loss: 1.9269\n",
            "Epoch 1/4 - Batch 12250/25000 - Loss: 1.9099\n",
            "Epoch 1/4 - Batch 12300/25000 - Loss: 2.2406\n",
            "Epoch 1/4 - Batch 12350/25000 - Loss: 1.9729\n",
            "Epoch 1/4 - Batch 12400/25000 - Loss: 1.6657\n",
            "Epoch 1/4 - Batch 12450/25000 - Loss: 1.7052\n",
            "Epoch 1/4 - Batch 12500/25000 - Loss: 1.9150\n",
            "Epoch 1/4 - Batch 12550/25000 - Loss: 1.7751\n",
            "Epoch 1/4 - Batch 12600/25000 - Loss: 1.7894\n",
            "Epoch 1/4 - Batch 12650/25000 - Loss: 2.0097\n",
            "Epoch 1/4 - Batch 12700/25000 - Loss: 1.7713\n",
            "Epoch 1/4 - Batch 12750/25000 - Loss: 2.1260\n",
            "Epoch 1/4 - Batch 12800/25000 - Loss: 2.3427\n",
            "Epoch 1/4 - Batch 12850/25000 - Loss: 1.5912\n",
            "Epoch 1/4 - Batch 12900/25000 - Loss: 2.0253\n",
            "Epoch 1/4 - Batch 12950/25000 - Loss: 1.9648\n",
            "Epoch 1/4 - Batch 13000/25000 - Loss: 2.1819\n",
            "Epoch 1/4 - Batch 13050/25000 - Loss: 1.9841\n",
            "Epoch 1/4 - Batch 13100/25000 - Loss: 1.8957\n",
            "Epoch 1/4 - Batch 13150/25000 - Loss: 2.1253\n",
            "Epoch 1/4 - Batch 13200/25000 - Loss: 2.0761\n",
            "Epoch 1/4 - Batch 13250/25000 - Loss: 2.0762\n",
            "Epoch 1/4 - Batch 13300/25000 - Loss: 1.7743\n",
            "Epoch 1/4 - Batch 13350/25000 - Loss: 2.0603\n",
            "Epoch 1/4 - Batch 13400/25000 - Loss: 1.8462\n",
            "Epoch 1/4 - Batch 13450/25000 - Loss: 2.0259\n",
            "Epoch 1/4 - Batch 13500/25000 - Loss: 1.7875\n",
            "Epoch 1/4 - Batch 13550/25000 - Loss: 1.9394\n",
            "Epoch 1/4 - Batch 13600/25000 - Loss: 1.0329\n",
            "Epoch 1/4 - Batch 13650/25000 - Loss: 1.9526\n",
            "Epoch 1/4 - Batch 13700/25000 - Loss: 2.1634\n",
            "Epoch 1/4 - Batch 13750/25000 - Loss: 1.4707\n",
            "Epoch 1/4 - Batch 13800/25000 - Loss: 2.3101\n",
            "Epoch 1/4 - Batch 13850/25000 - Loss: 1.9111\n",
            "Epoch 1/4 - Batch 13900/25000 - Loss: 1.9502\n",
            "Epoch 1/4 - Batch 13950/25000 - Loss: 1.9972\n",
            "Epoch 1/4 - Batch 14000/25000 - Loss: 2.1818\n",
            "Epoch 1/4 - Batch 14050/25000 - Loss: 2.0537\n",
            "Epoch 1/4 - Batch 14100/25000 - Loss: 2.1328\n",
            "Epoch 1/4 - Batch 14150/25000 - Loss: 1.7742\n",
            "Epoch 1/4 - Batch 14200/25000 - Loss: 1.8691\n",
            "Epoch 1/4 - Batch 14250/25000 - Loss: 1.8075\n",
            "Epoch 1/4 - Batch 14300/25000 - Loss: 1.9351\n",
            "Epoch 1/4 - Batch 14350/25000 - Loss: 1.5314\n",
            "Epoch 1/4 - Batch 14400/25000 - Loss: 1.7885\n",
            "Epoch 1/4 - Batch 14450/25000 - Loss: 1.9619\n",
            "Epoch 1/4 - Batch 14500/25000 - Loss: 2.0911\n",
            "Epoch 1/4 - Batch 14550/25000 - Loss: 2.0278\n",
            "Epoch 1/4 - Batch 14600/25000 - Loss: 1.5260\n",
            "Epoch 1/4 - Batch 14650/25000 - Loss: 2.2275\n",
            "Epoch 1/4 - Batch 14700/25000 - Loss: 1.3790\n",
            "Epoch 1/4 - Batch 14750/25000 - Loss: 1.6168\n",
            "Epoch 1/4 - Batch 14800/25000 - Loss: 1.3655\n",
            "Epoch 1/4 - Batch 14850/25000 - Loss: 1.9118\n",
            "Epoch 1/4 - Batch 14900/25000 - Loss: 1.6269\n",
            "Epoch 1/4 - Batch 14950/25000 - Loss: 1.4962\n",
            "Epoch 1/4 - Batch 15000/25000 - Loss: 1.6159\n",
            "Epoch 1/4 - Batch 15050/25000 - Loss: 1.6070\n",
            "Epoch 1/4 - Batch 15100/25000 - Loss: 1.3719\n",
            "Epoch 1/4 - Batch 15150/25000 - Loss: 1.8352\n",
            "Epoch 1/4 - Batch 15200/25000 - Loss: 1.7300\n",
            "Epoch 1/4 - Batch 15250/25000 - Loss: 1.1815\n",
            "Epoch 1/4 - Batch 15300/25000 - Loss: 1.6637\n",
            "Epoch 1/4 - Batch 15350/25000 - Loss: 1.4476\n",
            "Epoch 1/4 - Batch 15400/25000 - Loss: 1.8879\n",
            "Epoch 1/4 - Batch 15450/25000 - Loss: 2.0990\n",
            "Epoch 1/4 - Batch 15500/25000 - Loss: 1.9425\n",
            "Epoch 1/4 - Batch 15550/25000 - Loss: 1.8547\n",
            "Epoch 1/4 - Batch 15600/25000 - Loss: 1.7254\n",
            "Epoch 1/4 - Batch 15650/25000 - Loss: 2.0487\n",
            "Epoch 1/4 - Batch 15700/25000 - Loss: 1.8828\n",
            "Epoch 1/4 - Batch 15750/25000 - Loss: 1.9488\n",
            "Epoch 1/4 - Batch 15800/25000 - Loss: 1.6785\n",
            "Epoch 1/4 - Batch 15850/25000 - Loss: 1.8518\n",
            "Epoch 1/4 - Batch 15900/25000 - Loss: 1.5782\n",
            "Epoch 1/4 - Batch 15950/25000 - Loss: 2.1426\n",
            "Epoch 1/4 - Batch 16000/25000 - Loss: 1.9666\n",
            "Epoch 1/4 - Batch 16050/25000 - Loss: 2.2861\n",
            "Epoch 1/4 - Batch 16100/25000 - Loss: 1.9255\n",
            "Epoch 1/4 - Batch 16150/25000 - Loss: 1.8234\n",
            "Epoch 1/4 - Batch 16200/25000 - Loss: 1.8732\n",
            "Epoch 1/4 - Batch 16250/25000 - Loss: 1.8145\n",
            "Epoch 1/4 - Batch 16300/25000 - Loss: 2.1968\n",
            "Epoch 1/4 - Batch 16350/25000 - Loss: 1.3611\n",
            "Epoch 1/4 - Batch 16400/25000 - Loss: 1.9176\n",
            "Epoch 1/4 - Batch 16450/25000 - Loss: 2.0363\n",
            "Epoch 1/4 - Batch 16500/25000 - Loss: 1.0908\n",
            "Epoch 1/4 - Batch 16550/25000 - Loss: 1.8488\n",
            "Epoch 1/4 - Batch 16600/25000 - Loss: 1.9584\n",
            "Epoch 1/4 - Batch 16650/25000 - Loss: 1.9982\n",
            "Epoch 1/4 - Batch 16700/25000 - Loss: 2.2078\n",
            "Epoch 1/4 - Batch 16750/25000 - Loss: 1.8542\n",
            "Epoch 1/4 - Batch 16800/25000 - Loss: 1.6957\n",
            "Epoch 1/4 - Batch 16850/25000 - Loss: 1.8372\n",
            "Epoch 1/4 - Batch 16900/25000 - Loss: 1.7350\n",
            "Epoch 1/4 - Batch 16950/25000 - Loss: 2.0499\n",
            "Epoch 1/4 - Batch 17000/25000 - Loss: 1.4983\n",
            "Epoch 1/4 - Batch 17050/25000 - Loss: 1.7931\n",
            "Epoch 1/4 - Batch 17100/25000 - Loss: 1.8707\n",
            "Epoch 1/4 - Batch 17150/25000 - Loss: 1.6146\n",
            "Epoch 1/4 - Batch 17200/25000 - Loss: 1.9364\n",
            "Epoch 1/4 - Batch 17250/25000 - Loss: 1.6041\n",
            "Epoch 1/4 - Batch 17300/25000 - Loss: 1.8224\n",
            "Epoch 1/4 - Batch 17350/25000 - Loss: 1.6856\n",
            "Epoch 1/4 - Batch 17400/25000 - Loss: 1.9659\n",
            "Epoch 1/4 - Batch 17450/25000 - Loss: 1.5087\n",
            "Epoch 1/4 - Batch 17500/25000 - Loss: 2.1189\n",
            "Epoch 1/4 - Batch 17550/25000 - Loss: 1.6303\n",
            "Epoch 1/4 - Batch 17600/25000 - Loss: 1.7700\n",
            "Epoch 1/4 - Batch 17650/25000 - Loss: 1.5575\n",
            "Epoch 1/4 - Batch 17700/25000 - Loss: 2.0144\n",
            "Epoch 1/4 - Batch 17750/25000 - Loss: 1.4905\n",
            "Epoch 1/4 - Batch 17800/25000 - Loss: 1.6584\n",
            "Epoch 1/4 - Batch 17850/25000 - Loss: 2.2905\n",
            "Epoch 1/4 - Batch 17900/25000 - Loss: 1.6758\n",
            "Epoch 1/4 - Batch 17950/25000 - Loss: 2.1960\n",
            "Epoch 1/4 - Batch 18000/25000 - Loss: 1.7925\n",
            "Epoch 1/4 - Batch 18050/25000 - Loss: 1.3692\n",
            "Epoch 1/4 - Batch 18100/25000 - Loss: 1.8870\n",
            "Epoch 1/4 - Batch 18150/25000 - Loss: 1.5431\n",
            "Epoch 1/4 - Batch 18200/25000 - Loss: 1.4101\n",
            "Epoch 1/4 - Batch 18250/25000 - Loss: 2.0404\n",
            "Epoch 1/4 - Batch 18300/25000 - Loss: 1.7446\n",
            "Epoch 1/4 - Batch 18350/25000 - Loss: 2.0818\n",
            "Epoch 1/4 - Batch 18400/25000 - Loss: 1.5680\n",
            "Epoch 1/4 - Batch 18450/25000 - Loss: 2.2898\n",
            "Epoch 1/4 - Batch 18500/25000 - Loss: 1.7219\n",
            "Epoch 1/4 - Batch 18550/25000 - Loss: 1.4436\n",
            "Epoch 1/4 - Batch 18600/25000 - Loss: 1.7515\n",
            "Epoch 1/4 - Batch 18650/25000 - Loss: 2.1935\n",
            "Epoch 1/4 - Batch 18700/25000 - Loss: 1.5234\n",
            "Epoch 1/4 - Batch 18750/25000 - Loss: 1.6286\n",
            "Epoch 1/4 - Batch 18800/25000 - Loss: 1.7764\n",
            "Epoch 1/4 - Batch 18850/25000 - Loss: 1.9641\n",
            "Epoch 1/4 - Batch 18900/25000 - Loss: 2.0788\n",
            "Epoch 1/4 - Batch 18950/25000 - Loss: 2.0656\n",
            "Epoch 1/4 - Batch 19000/25000 - Loss: 1.7829\n",
            "Epoch 1/4 - Batch 19050/25000 - Loss: 1.9732\n",
            "Epoch 1/4 - Batch 19100/25000 - Loss: 1.9014\n",
            "Epoch 1/4 - Batch 19150/25000 - Loss: 1.5784\n",
            "Epoch 1/4 - Batch 19200/25000 - Loss: 1.9674\n",
            "Epoch 1/4 - Batch 19250/25000 - Loss: 1.5138\n",
            "Epoch 1/4 - Batch 19300/25000 - Loss: 2.0720\n",
            "Epoch 1/4 - Batch 19350/25000 - Loss: 1.7246\n",
            "Epoch 1/4 - Batch 19400/25000 - Loss: 1.8680\n",
            "Epoch 1/4 - Batch 19450/25000 - Loss: 2.1016\n",
            "Epoch 1/4 - Batch 19500/25000 - Loss: 2.0075\n",
            "Epoch 1/4 - Batch 19550/25000 - Loss: 1.7597\n",
            "Epoch 1/4 - Batch 19600/25000 - Loss: 1.9532\n",
            "Epoch 1/4 - Batch 19650/25000 - Loss: 1.6358\n",
            "Epoch 1/4 - Batch 19700/25000 - Loss: 1.5815\n",
            "Epoch 1/4 - Batch 19750/25000 - Loss: 1.7264\n",
            "Epoch 1/4 - Batch 19800/25000 - Loss: 1.9812\n",
            "Epoch 1/4 - Batch 19850/25000 - Loss: 2.2722\n",
            "Epoch 1/4 - Batch 19900/25000 - Loss: 1.8305\n",
            "Epoch 1/4 - Batch 19950/25000 - Loss: 1.2119\n",
            "Epoch 1/4 - Batch 20000/25000 - Loss: 1.4936\n",
            "Epoch 1/4 - Batch 20050/25000 - Loss: 2.0562\n",
            "Epoch 1/4 - Batch 20100/25000 - Loss: 1.4800\n",
            "Epoch 1/4 - Batch 20150/25000 - Loss: 2.0214\n",
            "Epoch 1/4 - Batch 20200/25000 - Loss: 1.7554\n",
            "Epoch 1/4 - Batch 20250/25000 - Loss: 1.9227\n",
            "Epoch 1/4 - Batch 20300/25000 - Loss: 1.5166\n",
            "Epoch 1/4 - Batch 20350/25000 - Loss: 1.6741\n",
            "Epoch 1/4 - Batch 20400/25000 - Loss: 1.8185\n",
            "Epoch 1/4 - Batch 20450/25000 - Loss: 1.8865\n",
            "Epoch 1/4 - Batch 20500/25000 - Loss: 2.3148\n",
            "Epoch 1/4 - Batch 20550/25000 - Loss: 1.9031\n",
            "Epoch 1/4 - Batch 20600/25000 - Loss: 2.2459\n",
            "Epoch 1/4 - Batch 20650/25000 - Loss: 1.8207\n",
            "Epoch 1/4 - Batch 20700/25000 - Loss: 1.7159\n",
            "Epoch 1/4 - Batch 20750/25000 - Loss: 1.6767\n",
            "Epoch 1/4 - Batch 20800/25000 - Loss: 1.9619\n",
            "Epoch 1/4 - Batch 20850/25000 - Loss: 1.4370\n",
            "Epoch 1/4 - Batch 20900/25000 - Loss: 2.2013\n",
            "Epoch 1/4 - Batch 20950/25000 - Loss: 2.0144\n",
            "Epoch 1/4 - Batch 21000/25000 - Loss: 1.5110\n",
            "Epoch 1/4 - Batch 21050/25000 - Loss: 1.4370\n",
            "Epoch 1/4 - Batch 21100/25000 - Loss: 2.0391\n",
            "Epoch 1/4 - Batch 21150/25000 - Loss: 1.9317\n",
            "Epoch 1/4 - Batch 21200/25000 - Loss: 1.8835\n",
            "Epoch 1/4 - Batch 21250/25000 - Loss: 1.9669\n",
            "Epoch 1/4 - Batch 21300/25000 - Loss: 1.8437\n",
            "Epoch 1/4 - Batch 21350/25000 - Loss: 1.6523\n",
            "Epoch 1/4 - Batch 21400/25000 - Loss: 1.8248\n",
            "Epoch 1/4 - Batch 21450/25000 - Loss: 1.8120\n",
            "Epoch 1/4 - Batch 21500/25000 - Loss: 1.7093\n",
            "Epoch 1/4 - Batch 21550/25000 - Loss: 1.8707\n",
            "Epoch 1/4 - Batch 21600/25000 - Loss: 1.7575\n",
            "Epoch 1/4 - Batch 21650/25000 - Loss: 1.7838\n",
            "Epoch 1/4 - Batch 21700/25000 - Loss: 2.0849\n",
            "Epoch 1/4 - Batch 21750/25000 - Loss: 2.5831\n",
            "Epoch 1/4 - Batch 21800/25000 - Loss: 1.7667\n",
            "Epoch 1/4 - Batch 21850/25000 - Loss: 1.9676\n",
            "Epoch 1/4 - Batch 21900/25000 - Loss: 1.3120\n",
            "Epoch 1/4 - Batch 21950/25000 - Loss: 1.4586\n",
            "Epoch 1/4 - Batch 22000/25000 - Loss: 1.9123\n",
            "Epoch 1/4 - Batch 22050/25000 - Loss: 2.2809\n",
            "Epoch 1/4 - Batch 22100/25000 - Loss: 1.8863\n",
            "Epoch 1/4 - Batch 22150/25000 - Loss: 1.9381\n",
            "Epoch 1/4 - Batch 22200/25000 - Loss: 1.6869\n",
            "Epoch 1/4 - Batch 22250/25000 - Loss: 1.7718\n",
            "Epoch 1/4 - Batch 22300/25000 - Loss: 1.6190\n",
            "Epoch 1/4 - Batch 22350/25000 - Loss: 1.6665\n",
            "Epoch 1/4 - Batch 22400/25000 - Loss: 1.7814\n",
            "Epoch 1/4 - Batch 22450/25000 - Loss: 2.1661\n",
            "Epoch 1/4 - Batch 22500/25000 - Loss: 1.8235\n",
            "Epoch 1/4 - Batch 22550/25000 - Loss: 2.0122\n",
            "Epoch 1/4 - Batch 22600/25000 - Loss: 1.9841\n",
            "Epoch 1/4 - Batch 22650/25000 - Loss: 1.6496\n",
            "Epoch 1/4 - Batch 22700/25000 - Loss: 1.7964\n",
            "Epoch 1/4 - Batch 22750/25000 - Loss: 1.7049\n",
            "Epoch 1/4 - Batch 22800/25000 - Loss: 1.8200\n",
            "Epoch 1/4 - Batch 22850/25000 - Loss: 2.2149\n",
            "Epoch 1/4 - Batch 22900/25000 - Loss: 1.9255\n",
            "Epoch 1/4 - Batch 22950/25000 - Loss: 1.9479\n",
            "Epoch 1/4 - Batch 23000/25000 - Loss: 1.8506\n",
            "Epoch 1/4 - Batch 23050/25000 - Loss: 1.7179\n",
            "Epoch 1/4 - Batch 23100/25000 - Loss: 1.7256\n",
            "Epoch 1/4 - Batch 23150/25000 - Loss: 1.8553\n",
            "Epoch 1/4 - Batch 23200/25000 - Loss: 1.8099\n",
            "Epoch 1/4 - Batch 23250/25000 - Loss: 2.1135\n",
            "Epoch 1/4 - Batch 23300/25000 - Loss: 1.9619\n",
            "Epoch 1/4 - Batch 23350/25000 - Loss: 1.6592\n",
            "Epoch 1/4 - Batch 23400/25000 - Loss: 1.8341\n",
            "Epoch 1/4 - Batch 23450/25000 - Loss: 2.2100\n",
            "Epoch 1/4 - Batch 23500/25000 - Loss: 1.9846\n",
            "Epoch 1/4 - Batch 23550/25000 - Loss: 1.6718\n",
            "Epoch 1/4 - Batch 23600/25000 - Loss: 2.1042\n",
            "Epoch 1/4 - Batch 23650/25000 - Loss: 1.6114\n",
            "Epoch 1/4 - Batch 23700/25000 - Loss: 1.7562\n",
            "Epoch 1/4 - Batch 23750/25000 - Loss: 1.7966\n",
            "Epoch 1/4 - Batch 23800/25000 - Loss: 1.8642\n",
            "Epoch 1/4 - Batch 23850/25000 - Loss: 1.9556\n",
            "Epoch 1/4 - Batch 23900/25000 - Loss: 1.9440\n",
            "Epoch 1/4 - Batch 23950/25000 - Loss: 1.7310\n",
            "Epoch 1/4 - Batch 24000/25000 - Loss: 1.9746\n",
            "Epoch 1/4 - Batch 24050/25000 - Loss: 1.9661\n",
            "Epoch 1/4 - Batch 24100/25000 - Loss: 1.5847\n",
            "Epoch 1/4 - Batch 24150/25000 - Loss: 1.9382\n",
            "Epoch 1/4 - Batch 24200/25000 - Loss: 1.9950\n",
            "Epoch 1/4 - Batch 24250/25000 - Loss: 2.1211\n",
            "Epoch 1/4 - Batch 24300/25000 - Loss: 1.5714\n",
            "Epoch 1/4 - Batch 24350/25000 - Loss: 1.7369\n",
            "Epoch 1/4 - Batch 24400/25000 - Loss: 2.0738\n",
            "Epoch 1/4 - Batch 24450/25000 - Loss: 1.4543\n",
            "Epoch 1/4 - Batch 24500/25000 - Loss: 1.5575\n",
            "Epoch 1/4 - Batch 24550/25000 - Loss: 1.7515\n",
            "Epoch 1/4 - Batch 24600/25000 - Loss: 1.6595\n",
            "Epoch 1/4 - Batch 24650/25000 - Loss: 2.3487\n",
            "Epoch 1/4 - Batch 24700/25000 - Loss: 1.7005\n",
            "Epoch 1/4 - Batch 24750/25000 - Loss: 2.0687\n",
            "Epoch 1/4 - Batch 24800/25000 - Loss: 1.3833\n",
            "Epoch 1/4 - Batch 24850/25000 - Loss: 1.3884\n",
            "Epoch 1/4 - Batch 24900/25000 - Loss: 1.8921\n",
            "Epoch 1/4 - Batch 24950/25000 - Loss: 1.8012\n",
            "Epoch 1/4 - Average loss: 2.3221 - Duration: 15976.00s\n",
            "Achieved privacy budget: Îµ = 10.11\n",
            "Epoch 2/4 - Batch 0/25000 - Loss: 1.3835\n",
            "Epoch 2/4 - Batch 50/25000 - Loss: 2.0059\n",
            "Epoch 2/4 - Batch 100/25000 - Loss: 1.5091\n",
            "Epoch 2/4 - Batch 150/25000 - Loss: 2.0148\n",
            "Epoch 2/4 - Batch 200/25000 - Loss: 1.5495\n",
            "Epoch 2/4 - Batch 250/25000 - Loss: 1.8284\n",
            "Epoch 2/4 - Batch 300/25000 - Loss: 1.8608\n",
            "Epoch 2/4 - Batch 350/25000 - Loss: 1.9326\n",
            "Epoch 2/4 - Batch 400/25000 - Loss: 2.2139\n",
            "Epoch 2/4 - Batch 450/25000 - Loss: 1.7839\n",
            "Epoch 2/4 - Batch 500/25000 - Loss: 1.3874\n",
            "Epoch 2/4 - Batch 550/25000 - Loss: 2.1453\n",
            "Epoch 2/4 - Batch 600/25000 - Loss: 1.7576\n",
            "Epoch 2/4 - Batch 650/25000 - Loss: 1.4927\n",
            "Epoch 2/4 - Batch 700/25000 - Loss: 2.0282\n",
            "Epoch 2/4 - Batch 750/25000 - Loss: 1.7616\n",
            "Epoch 2/4 - Batch 800/25000 - Loss: 1.7849\n",
            "Epoch 2/4 - Batch 850/25000 - Loss: 1.6060\n",
            "Epoch 2/4 - Batch 900/25000 - Loss: 1.8289\n",
            "Epoch 2/4 - Batch 950/25000 - Loss: 2.4638\n",
            "Epoch 2/4 - Batch 1000/25000 - Loss: 1.4627\n",
            "Epoch 2/4 - Batch 1050/25000 - Loss: 2.5972\n",
            "Epoch 2/4 - Batch 1100/25000 - Loss: 1.6173\n",
            "Epoch 2/4 - Batch 1150/25000 - Loss: 1.3718\n",
            "Epoch 2/4 - Batch 1200/25000 - Loss: 1.7644\n",
            "Epoch 2/4 - Batch 1250/25000 - Loss: 1.9234\n",
            "Epoch 2/4 - Batch 1300/25000 - Loss: 2.1212\n",
            "Epoch 2/4 - Batch 1350/25000 - Loss: 1.3817\n",
            "Epoch 2/4 - Batch 1400/25000 - Loss: 1.2352\n",
            "Epoch 2/4 - Batch 1450/25000 - Loss: 1.5164\n",
            "Epoch 2/4 - Batch 1500/25000 - Loss: 2.1047\n",
            "Epoch 2/4 - Batch 1550/25000 - Loss: 1.8755\n",
            "Epoch 2/4 - Batch 1600/25000 - Loss: 1.8535\n",
            "Epoch 2/4 - Batch 1650/25000 - Loss: 1.9689\n",
            "Epoch 2/4 - Batch 1700/25000 - Loss: 1.6079\n",
            "Epoch 2/4 - Batch 1750/25000 - Loss: 1.4871\n",
            "Epoch 2/4 - Batch 1800/25000 - Loss: 2.3557\n",
            "Epoch 2/4 - Batch 1850/25000 - Loss: 2.1156\n",
            "Epoch 2/4 - Batch 1900/25000 - Loss: 1.6393\n",
            "Epoch 2/4 - Batch 1950/25000 - Loss: 1.9436\n",
            "Epoch 2/4 - Batch 2000/25000 - Loss: 1.9996\n",
            "Epoch 2/4 - Batch 2050/25000 - Loss: 2.0046\n",
            "Epoch 2/4 - Batch 2100/25000 - Loss: 1.7391\n",
            "Epoch 2/4 - Batch 2150/25000 - Loss: 1.5603\n",
            "Epoch 2/4 - Batch 2200/25000 - Loss: 1.9171\n",
            "Epoch 2/4 - Batch 2250/25000 - Loss: 2.1577\n",
            "Epoch 2/4 - Batch 2300/25000 - Loss: 1.7494\n",
            "Epoch 2/4 - Batch 2350/25000 - Loss: 1.6318\n",
            "Epoch 2/4 - Batch 2400/25000 - Loss: 2.0634\n",
            "Epoch 2/4 - Batch 2450/25000 - Loss: 1.2919\n",
            "Epoch 2/4 - Batch 2500/25000 - Loss: 1.6519\n",
            "Epoch 2/4 - Batch 2550/25000 - Loss: 1.3682\n",
            "Epoch 2/4 - Batch 2600/25000 - Loss: 1.7358\n",
            "Epoch 2/4 - Batch 2650/25000 - Loss: 1.9606\n",
            "Epoch 2/4 - Batch 2700/25000 - Loss: 1.1605\n",
            "Epoch 2/4 - Batch 2750/25000 - Loss: 1.4220\n",
            "Epoch 2/4 - Batch 2800/25000 - Loss: 1.6007\n",
            "Epoch 2/4 - Batch 2850/25000 - Loss: 1.6913\n",
            "Epoch 2/4 - Batch 2900/25000 - Loss: 1.8177\n",
            "Epoch 2/4 - Batch 2950/25000 - Loss: 2.0946\n",
            "Epoch 2/4 - Batch 3000/25000 - Loss: 1.9867\n",
            "Epoch 2/4 - Batch 3050/25000 - Loss: 1.5421\n",
            "Epoch 2/4 - Batch 3100/25000 - Loss: 1.9271\n",
            "Epoch 2/4 - Batch 3150/25000 - Loss: 1.6443\n",
            "Epoch 2/4 - Batch 3200/25000 - Loss: 1.8388\n",
            "Epoch 2/4 - Batch 3250/25000 - Loss: 2.0544\n",
            "Epoch 2/4 - Batch 3300/25000 - Loss: 1.2925\n",
            "Epoch 2/4 - Batch 3350/25000 - Loss: 1.5803\n",
            "Epoch 2/4 - Batch 3400/25000 - Loss: 2.2166\n",
            "Epoch 2/4 - Batch 3450/25000 - Loss: 1.8474\n",
            "Epoch 2/4 - Batch 3500/25000 - Loss: 2.2909\n",
            "Epoch 2/4 - Batch 3550/25000 - Loss: 1.7307\n",
            "Epoch 2/4 - Batch 3600/25000 - Loss: 1.7417\n",
            "Epoch 2/4 - Batch 3650/25000 - Loss: 1.4171\n",
            "Epoch 2/4 - Batch 3700/25000 - Loss: 1.6852\n",
            "Epoch 2/4 - Batch 3750/25000 - Loss: 1.5007\n",
            "Epoch 2/4 - Batch 3800/25000 - Loss: 1.9147\n",
            "Epoch 2/4 - Batch 3850/25000 - Loss: 1.7119\n",
            "Epoch 2/4 - Batch 3900/25000 - Loss: 1.5419\n",
            "Epoch 2/4 - Batch 3950/25000 - Loss: 1.7793\n",
            "Epoch 2/4 - Batch 4000/25000 - Loss: 1.4732\n",
            "Epoch 2/4 - Batch 4050/25000 - Loss: 1.8783\n",
            "Epoch 2/4 - Batch 4100/25000 - Loss: 1.6984\n",
            "Epoch 2/4 - Batch 4150/25000 - Loss: 1.7983\n",
            "Epoch 2/4 - Batch 4200/25000 - Loss: 2.1696\n",
            "Epoch 2/4 - Batch 4250/25000 - Loss: 1.9802\n",
            "Epoch 2/4 - Batch 4300/25000 - Loss: 1.4824\n",
            "Epoch 2/4 - Batch 4350/25000 - Loss: 1.8830\n",
            "Epoch 2/4 - Batch 4400/25000 - Loss: 2.5810\n",
            "Epoch 2/4 - Batch 4450/25000 - Loss: 2.1560\n",
            "Epoch 2/4 - Batch 4500/25000 - Loss: 1.7829\n",
            "Epoch 2/4 - Batch 4550/25000 - Loss: 1.6174\n",
            "Epoch 2/4 - Batch 4600/25000 - Loss: 1.9600\n",
            "Epoch 2/4 - Batch 4650/25000 - Loss: 1.6756\n",
            "Epoch 2/4 - Batch 4700/25000 - Loss: 1.3451\n",
            "Epoch 2/4 - Batch 4750/25000 - Loss: 1.4224\n",
            "Epoch 2/4 - Batch 4800/25000 - Loss: 1.7293\n",
            "Epoch 2/4 - Batch 4850/25000 - Loss: 1.4500\n",
            "Epoch 2/4 - Batch 4900/25000 - Loss: 1.6836\n",
            "Epoch 2/4 - Batch 4950/25000 - Loss: 1.6527\n",
            "Epoch 2/4 - Batch 5000/25000 - Loss: 1.8817\n",
            "Epoch 2/4 - Batch 5050/25000 - Loss: 1.5520\n",
            "Epoch 2/4 - Batch 5100/25000 - Loss: 1.6907\n",
            "Epoch 2/4 - Batch 5150/25000 - Loss: 1.4843\n",
            "Epoch 2/4 - Batch 5200/25000 - Loss: 1.1501\n",
            "Epoch 2/4 - Batch 5250/25000 - Loss: 1.3286\n",
            "Epoch 2/4 - Batch 5300/25000 - Loss: 1.5929\n",
            "Epoch 2/4 - Batch 5350/25000 - Loss: 1.9446\n",
            "Epoch 2/4 - Batch 5400/25000 - Loss: 1.8517\n",
            "Epoch 2/4 - Batch 5450/25000 - Loss: 1.9077\n",
            "Epoch 2/4 - Batch 5500/25000 - Loss: 1.8320\n",
            "Epoch 2/4 - Batch 5550/25000 - Loss: 1.9024\n",
            "Epoch 2/4 - Batch 5600/25000 - Loss: 1.6206\n",
            "Epoch 2/4 - Batch 5650/25000 - Loss: 1.7449\n",
            "Epoch 2/4 - Batch 5700/25000 - Loss: 1.3681\n",
            "Epoch 2/4 - Batch 5750/25000 - Loss: 1.8748\n",
            "Epoch 2/4 - Batch 5800/25000 - Loss: 1.9202\n",
            "Epoch 2/4 - Batch 5850/25000 - Loss: 2.0134\n",
            "Epoch 2/4 - Batch 5900/25000 - Loss: 1.3371\n",
            "Epoch 2/4 - Batch 5950/25000 - Loss: 1.6142\n",
            "Epoch 2/4 - Batch 6000/25000 - Loss: 1.6163\n",
            "Epoch 2/4 - Batch 6050/25000 - Loss: 2.1252\n",
            "Epoch 2/4 - Batch 6100/25000 - Loss: 2.3746\n",
            "Epoch 2/4 - Batch 6150/25000 - Loss: 1.9937\n",
            "Epoch 2/4 - Batch 6200/25000 - Loss: 1.7685\n",
            "Epoch 2/4 - Batch 6250/25000 - Loss: 2.2083\n",
            "Epoch 2/4 - Batch 6300/25000 - Loss: 1.6785\n",
            "Epoch 2/4 - Batch 6350/25000 - Loss: 1.3003\n",
            "Epoch 2/4 - Batch 6400/25000 - Loss: 2.2405\n",
            "Epoch 2/4 - Batch 6450/25000 - Loss: 1.9618\n",
            "Epoch 2/4 - Batch 6500/25000 - Loss: 1.8402\n",
            "Epoch 2/4 - Batch 6550/25000 - Loss: 1.5446\n",
            "Epoch 2/4 - Batch 6600/25000 - Loss: 2.0749\n",
            "Epoch 2/4 - Batch 6650/25000 - Loss: 1.8758\n",
            "Epoch 2/4 - Batch 6700/25000 - Loss: 2.2428\n",
            "Epoch 2/4 - Batch 6750/25000 - Loss: 2.2772\n",
            "Epoch 2/4 - Batch 6800/25000 - Loss: 2.1444\n",
            "Epoch 2/4 - Batch 6850/25000 - Loss: 1.5207\n",
            "Epoch 2/4 - Batch 6900/25000 - Loss: 1.9824\n",
            "Epoch 2/4 - Batch 6950/25000 - Loss: 1.8352\n",
            "Epoch 2/4 - Batch 7000/25000 - Loss: 1.6582\n",
            "Epoch 2/4 - Batch 7050/25000 - Loss: 1.4161\n",
            "Epoch 2/4 - Batch 7100/25000 - Loss: 1.9666\n",
            "Epoch 2/4 - Batch 7150/25000 - Loss: 1.9442\n",
            "Epoch 2/4 - Batch 7200/25000 - Loss: 2.1429\n",
            "Epoch 2/4 - Batch 7250/25000 - Loss: 1.3923\n",
            "Epoch 2/4 - Batch 7300/25000 - Loss: 1.7390\n",
            "Epoch 2/4 - Batch 7350/25000 - Loss: 1.8955\n",
            "Epoch 2/4 - Batch 7400/25000 - Loss: 2.1701\n",
            "Epoch 2/4 - Batch 7450/25000 - Loss: 1.6193\n",
            "Epoch 2/4 - Batch 7500/25000 - Loss: 1.9199\n",
            "Epoch 2/4 - Batch 7550/25000 - Loss: 2.0408\n",
            "Epoch 2/4 - Batch 7600/25000 - Loss: 2.1803\n",
            "Epoch 2/4 - Batch 7650/25000 - Loss: 2.3482\n",
            "Epoch 2/4 - Batch 7700/25000 - Loss: 2.1518\n",
            "Epoch 2/4 - Batch 7750/25000 - Loss: 1.9773\n",
            "Epoch 2/4 - Batch 7800/25000 - Loss: 2.2584\n",
            "Epoch 2/4 - Batch 7850/25000 - Loss: 1.5935\n",
            "Epoch 2/4 - Batch 7900/25000 - Loss: 1.6845\n",
            "Epoch 2/4 - Batch 7950/25000 - Loss: 1.7006\n",
            "Epoch 2/4 - Batch 8000/25000 - Loss: 1.8978\n",
            "Epoch 2/4 - Batch 8050/25000 - Loss: 2.4783\n",
            "Epoch 2/4 - Batch 8100/25000 - Loss: 1.6399\n",
            "Epoch 2/4 - Batch 8150/25000 - Loss: 1.6173\n",
            "Epoch 2/4 - Batch 8200/25000 - Loss: 1.9477\n",
            "Epoch 2/4 - Batch 8250/25000 - Loss: 1.7834\n",
            "Epoch 2/4 - Batch 8300/25000 - Loss: 1.4740\n",
            "Epoch 2/4 - Batch 8350/25000 - Loss: 2.0271\n",
            "Epoch 2/4 - Batch 8400/25000 - Loss: 1.8735\n",
            "Epoch 2/4 - Batch 8450/25000 - Loss: 1.6767\n",
            "Epoch 2/4 - Batch 8500/25000 - Loss: 1.5177\n",
            "Epoch 2/4 - Batch 8550/25000 - Loss: 1.4318\n",
            "Epoch 2/4 - Batch 8600/25000 - Loss: 1.8052\n",
            "Epoch 2/4 - Batch 8650/25000 - Loss: 2.4145\n",
            "Epoch 2/4 - Batch 8700/25000 - Loss: 2.3077\n",
            "Epoch 2/4 - Batch 8750/25000 - Loss: 2.0878\n",
            "Epoch 2/4 - Batch 8800/25000 - Loss: 1.4397\n",
            "Epoch 2/4 - Batch 8850/25000 - Loss: 1.5413\n",
            "Epoch 2/4 - Batch 8900/25000 - Loss: 1.5675\n",
            "Epoch 2/4 - Batch 8950/25000 - Loss: 1.3707\n",
            "Epoch 2/4 - Batch 9000/25000 - Loss: 2.1277\n",
            "Epoch 2/4 - Batch 9050/25000 - Loss: 1.8467\n",
            "Epoch 2/4 - Batch 9100/25000 - Loss: 1.1107\n",
            "Epoch 2/4 - Batch 9150/25000 - Loss: 1.9633\n",
            "Epoch 2/4 - Batch 9200/25000 - Loss: 2.2113\n",
            "Epoch 2/4 - Batch 9250/25000 - Loss: 1.9370\n",
            "Epoch 2/4 - Batch 9300/25000 - Loss: 2.1990\n",
            "Epoch 2/4 - Batch 9350/25000 - Loss: 2.2048\n",
            "Epoch 2/4 - Batch 9400/25000 - Loss: 1.6185\n",
            "Epoch 2/4 - Batch 9450/25000 - Loss: 1.7217\n",
            "Epoch 2/4 - Batch 9500/25000 - Loss: 1.7454\n",
            "Epoch 2/4 - Batch 9550/25000 - Loss: 1.5053\n",
            "Epoch 2/4 - Batch 9600/25000 - Loss: 1.8320\n",
            "Epoch 2/4 - Batch 9650/25000 - Loss: 1.4505\n",
            "Epoch 2/4 - Batch 9700/25000 - Loss: 1.8744\n",
            "Epoch 2/4 - Batch 9750/25000 - Loss: 2.0404\n",
            "Epoch 2/4 - Batch 9800/25000 - Loss: 2.0273\n",
            "Epoch 2/4 - Batch 9850/25000 - Loss: 1.5225\n",
            "Epoch 2/4 - Batch 9900/25000 - Loss: 1.8465\n",
            "Epoch 2/4 - Batch 9950/25000 - Loss: 1.8316\n",
            "Epoch 2/4 - Batch 10000/25000 - Loss: 1.4261\n",
            "Epoch 2/4 - Batch 10050/25000 - Loss: 2.0399\n",
            "Epoch 2/4 - Batch 10100/25000 - Loss: 2.3705\n",
            "Epoch 2/4 - Batch 10150/25000 - Loss: 1.5994\n",
            "Epoch 2/4 - Batch 10200/25000 - Loss: 1.7750\n",
            "Epoch 2/4 - Batch 10250/25000 - Loss: 1.1851\n",
            "Epoch 2/4 - Batch 10300/25000 - Loss: 2.0208\n",
            "Epoch 2/4 - Batch 10350/25000 - Loss: 1.5811\n",
            "Epoch 2/4 - Batch 10400/25000 - Loss: 1.6617\n",
            "Epoch 2/4 - Batch 10450/25000 - Loss: 1.8443\n",
            "Epoch 2/4 - Batch 10500/25000 - Loss: 2.0911\n",
            "Epoch 2/4 - Batch 10550/25000 - Loss: 1.3938\n",
            "Epoch 2/4 - Batch 10600/25000 - Loss: 0.8503\n",
            "Epoch 2/4 - Batch 10650/25000 - Loss: 1.8729\n",
            "Epoch 2/4 - Batch 10700/25000 - Loss: 1.4018\n",
            "Epoch 2/4 - Batch 10750/25000 - Loss: 2.0852\n",
            "Epoch 2/4 - Batch 10800/25000 - Loss: 1.8854\n",
            "Epoch 2/4 - Batch 10850/25000 - Loss: 1.7015\n",
            "Epoch 2/4 - Batch 10900/25000 - Loss: 1.4298\n",
            "Epoch 2/4 - Batch 10950/25000 - Loss: 1.7673\n",
            "Epoch 2/4 - Batch 11000/25000 - Loss: 1.8240\n",
            "Epoch 2/4 - Batch 11050/25000 - Loss: 1.1953\n",
            "Epoch 2/4 - Batch 11100/25000 - Loss: 1.4541\n",
            "Epoch 2/4 - Batch 11150/25000 - Loss: 1.7827\n",
            "Epoch 2/4 - Batch 11200/25000 - Loss: 1.8783\n",
            "Epoch 2/4 - Batch 11250/25000 - Loss: 1.6859\n",
            "Epoch 2/4 - Batch 11300/25000 - Loss: 1.7741\n",
            "Epoch 2/4 - Batch 11350/25000 - Loss: 2.1080\n",
            "Epoch 2/4 - Batch 11400/25000 - Loss: 1.5590\n",
            "Epoch 2/4 - Batch 11450/25000 - Loss: 1.6391\n",
            "Epoch 2/4 - Batch 11500/25000 - Loss: 1.7326\n",
            "Epoch 2/4 - Batch 11550/25000 - Loss: 1.3406\n",
            "Epoch 2/4 - Batch 11600/25000 - Loss: 2.0523\n",
            "Epoch 2/4 - Batch 11650/25000 - Loss: 1.7124\n",
            "Epoch 2/4 - Batch 11700/25000 - Loss: 2.0449\n",
            "Epoch 2/4 - Batch 11750/25000 - Loss: 2.0350\n",
            "Epoch 2/4 - Batch 11800/25000 - Loss: 1.9588\n",
            "Epoch 2/4 - Batch 11850/25000 - Loss: 1.6561\n",
            "Epoch 2/4 - Batch 11900/25000 - Loss: 1.5423\n",
            "Epoch 2/4 - Batch 11950/25000 - Loss: 1.9033\n",
            "Epoch 2/4 - Batch 12000/25000 - Loss: 1.9979\n",
            "Epoch 2/4 - Batch 12050/25000 - Loss: 2.1658\n",
            "Epoch 2/4 - Batch 12100/25000 - Loss: 1.8914\n",
            "Epoch 2/4 - Batch 12150/25000 - Loss: 1.9560\n",
            "Epoch 2/4 - Batch 12200/25000 - Loss: 1.6854\n",
            "Epoch 2/4 - Batch 12250/25000 - Loss: 1.5791\n",
            "Epoch 2/4 - Batch 12300/25000 - Loss: 1.8155\n",
            "Epoch 2/4 - Batch 12350/25000 - Loss: 1.6073\n",
            "Epoch 2/4 - Batch 12400/25000 - Loss: 1.5119\n",
            "Epoch 2/4 - Batch 12450/25000 - Loss: 1.9524\n",
            "Epoch 2/4 - Batch 12500/25000 - Loss: 1.3135\n",
            "Epoch 2/4 - Batch 12550/25000 - Loss: 1.8946\n",
            "Epoch 2/4 - Batch 12600/25000 - Loss: 1.6006\n",
            "Epoch 2/4 - Batch 12650/25000 - Loss: 1.6129\n",
            "Epoch 2/4 - Batch 12700/25000 - Loss: 2.0072\n",
            "Epoch 2/4 - Batch 12750/25000 - Loss: 2.0432\n",
            "Epoch 2/4 - Batch 12800/25000 - Loss: 1.7130\n",
            "Epoch 2/4 - Batch 12850/25000 - Loss: 1.7147\n",
            "Epoch 2/4 - Batch 12900/25000 - Loss: 1.6892\n",
            "Epoch 2/4 - Batch 12950/25000 - Loss: 1.3609\n",
            "Epoch 2/4 - Batch 13000/25000 - Loss: 1.7143\n",
            "Epoch 2/4 - Batch 13050/25000 - Loss: 1.6642\n",
            "Epoch 2/4 - Batch 13100/25000 - Loss: 1.9424\n",
            "Epoch 2/4 - Batch 13150/25000 - Loss: 2.2528\n",
            "Epoch 2/4 - Batch 13200/25000 - Loss: 1.8008\n",
            "Epoch 2/4 - Batch 13250/25000 - Loss: 1.5961\n",
            "Epoch 2/4 - Batch 13300/25000 - Loss: 1.6176\n",
            "Epoch 2/4 - Batch 13350/25000 - Loss: 1.6233\n",
            "Epoch 2/4 - Batch 13400/25000 - Loss: 1.6045\n",
            "Epoch 2/4 - Batch 13450/25000 - Loss: 1.1763\n",
            "Epoch 2/4 - Batch 13500/25000 - Loss: 1.9436\n",
            "Epoch 2/4 - Batch 13550/25000 - Loss: 2.0569\n",
            "Epoch 2/4 - Batch 13600/25000 - Loss: 1.7448\n",
            "Epoch 2/4 - Batch 13650/25000 - Loss: 1.3994\n",
            "Epoch 2/4 - Batch 13700/25000 - Loss: 2.2832\n",
            "Epoch 2/4 - Batch 13750/25000 - Loss: 1.3506\n",
            "Epoch 2/4 - Batch 13800/25000 - Loss: 1.8408\n",
            "Epoch 2/4 - Batch 13850/25000 - Loss: 1.6785\n",
            "Epoch 2/4 - Batch 13900/25000 - Loss: 2.1315\n",
            "Epoch 2/4 - Batch 13950/25000 - Loss: 1.5360\n",
            "Epoch 2/4 - Batch 14000/25000 - Loss: 1.8420\n",
            "Epoch 2/4 - Batch 14050/25000 - Loss: 2.0832\n",
            "Epoch 2/4 - Batch 14100/25000 - Loss: 1.7189\n",
            "Epoch 2/4 - Batch 14150/25000 - Loss: 1.6014\n",
            "Epoch 2/4 - Batch 14200/25000 - Loss: 1.5597\n",
            "Epoch 2/4 - Batch 14250/25000 - Loss: 1.4937\n",
            "Epoch 2/4 - Batch 14300/25000 - Loss: 1.9692\n",
            "Epoch 2/4 - Batch 14350/25000 - Loss: 1.8925\n",
            "Epoch 2/4 - Batch 14400/25000 - Loss: 2.4850\n",
            "Epoch 2/4 - Batch 14450/25000 - Loss: 1.4941\n",
            "Epoch 2/4 - Batch 14500/25000 - Loss: 1.6299\n",
            "Epoch 2/4 - Batch 14550/25000 - Loss: 2.4671\n",
            "Epoch 2/4 - Batch 14600/25000 - Loss: 2.0973\n",
            "Epoch 2/4 - Batch 14650/25000 - Loss: 2.0434\n",
            "Epoch 2/4 - Batch 14700/25000 - Loss: 1.7442\n",
            "Epoch 2/4 - Batch 14750/25000 - Loss: 1.8784\n",
            "Epoch 2/4 - Batch 14800/25000 - Loss: 1.6835\n",
            "Epoch 2/4 - Batch 14850/25000 - Loss: 1.9562\n",
            "Epoch 2/4 - Batch 14900/25000 - Loss: 1.4561\n",
            "Epoch 2/4 - Batch 14950/25000 - Loss: 1.6954\n",
            "Epoch 2/4 - Batch 15000/25000 - Loss: 2.0641\n",
            "Epoch 2/4 - Batch 15050/25000 - Loss: 1.6815\n",
            "Epoch 2/4 - Batch 15100/25000 - Loss: 1.5215\n",
            "Epoch 2/4 - Batch 15150/25000 - Loss: 1.4637\n",
            "Epoch 2/4 - Batch 15200/25000 - Loss: 1.1384\n",
            "Epoch 2/4 - Batch 15250/25000 - Loss: 1.5372\n",
            "Epoch 2/4 - Batch 15300/25000 - Loss: 2.0472\n",
            "Epoch 2/4 - Batch 15350/25000 - Loss: 1.6003\n",
            "Epoch 2/4 - Batch 15400/25000 - Loss: 1.9441\n",
            "Epoch 2/4 - Batch 15450/25000 - Loss: 1.9393\n",
            "Epoch 2/4 - Batch 15500/25000 - Loss: 1.7884\n",
            "Epoch 2/4 - Batch 15550/25000 - Loss: 1.5996\n",
            "Epoch 2/4 - Batch 15600/25000 - Loss: 1.6302\n",
            "Epoch 2/4 - Batch 15650/25000 - Loss: 1.4902\n",
            "Epoch 2/4 - Batch 15700/25000 - Loss: 1.5496\n",
            "Epoch 2/4 - Batch 15750/25000 - Loss: 1.5950\n",
            "Epoch 2/4 - Batch 15800/25000 - Loss: 1.8984\n",
            "Epoch 2/4 - Batch 15850/25000 - Loss: 1.9399\n",
            "Epoch 2/4 - Batch 15900/25000 - Loss: 1.5871\n",
            "Epoch 2/4 - Batch 15950/25000 - Loss: 1.6775\n",
            "Epoch 2/4 - Batch 16000/25000 - Loss: 1.9780\n",
            "Epoch 2/4 - Batch 16050/25000 - Loss: 1.5371\n",
            "Epoch 2/4 - Batch 16100/25000 - Loss: 2.1762\n",
            "Epoch 2/4 - Batch 16150/25000 - Loss: 2.0052\n",
            "Epoch 2/4 - Batch 16200/25000 - Loss: 1.9017\n",
            "Epoch 2/4 - Batch 16250/25000 - Loss: 1.5303\n",
            "Epoch 2/4 - Batch 16300/25000 - Loss: 1.5936\n",
            "Epoch 2/4 - Batch 16350/25000 - Loss: 1.8854\n",
            "Epoch 2/4 - Batch 16400/25000 - Loss: 1.9507\n",
            "Epoch 2/4 - Batch 16450/25000 - Loss: 1.4283\n",
            "Epoch 2/4 - Batch 16500/25000 - Loss: 2.2986\n",
            "Epoch 2/4 - Batch 16550/25000 - Loss: 1.9457\n",
            "Epoch 2/4 - Batch 16600/25000 - Loss: 1.2939\n",
            "Epoch 2/4 - Batch 16650/25000 - Loss: 1.9088\n",
            "Epoch 2/4 - Batch 16700/25000 - Loss: 0.8916\n",
            "Epoch 2/4 - Batch 16750/25000 - Loss: 1.0591\n",
            "Epoch 2/4 - Batch 16800/25000 - Loss: 1.7679\n",
            "Epoch 2/4 - Batch 16850/25000 - Loss: 2.1731\n",
            "Epoch 2/4 - Batch 16900/25000 - Loss: 1.7717\n",
            "Epoch 2/4 - Batch 16950/25000 - Loss: 1.9349\n",
            "Epoch 2/4 - Batch 17000/25000 - Loss: 1.8159\n",
            "Epoch 2/4 - Batch 17050/25000 - Loss: 2.0030\n",
            "Epoch 2/4 - Batch 17100/25000 - Loss: 1.0829\n",
            "Epoch 2/4 - Batch 17150/25000 - Loss: 1.8482\n",
            "Epoch 2/4 - Batch 17200/25000 - Loss: 1.7439\n",
            "Epoch 2/4 - Batch 17250/25000 - Loss: 1.4676\n",
            "Epoch 2/4 - Batch 17300/25000 - Loss: 1.9887\n",
            "Epoch 2/4 - Batch 17350/25000 - Loss: 1.9912\n",
            "Epoch 2/4 - Batch 17400/25000 - Loss: 1.6646\n",
            "Epoch 2/4 - Batch 17450/25000 - Loss: 1.7881\n",
            "Epoch 2/4 - Batch 17500/25000 - Loss: 1.5516\n",
            "Epoch 2/4 - Batch 17550/25000 - Loss: 1.9510\n",
            "Epoch 2/4 - Batch 17600/25000 - Loss: 1.5237\n",
            "Epoch 2/4 - Batch 17650/25000 - Loss: 1.6780\n",
            "Epoch 2/4 - Batch 17700/25000 - Loss: 1.4621\n",
            "Epoch 2/4 - Batch 17750/25000 - Loss: 1.7647\n",
            "Epoch 2/4 - Batch 17800/25000 - Loss: 1.6608\n",
            "Epoch 2/4 - Batch 17850/25000 - Loss: 2.0143\n",
            "Epoch 2/4 - Batch 17900/25000 - Loss: 1.4122\n",
            "Epoch 2/4 - Batch 17950/25000 - Loss: 1.9357\n",
            "Epoch 2/4 - Batch 18000/25000 - Loss: 1.9556\n",
            "Epoch 2/4 - Batch 18050/25000 - Loss: 1.6866\n",
            "Epoch 2/4 - Batch 18100/25000 - Loss: 1.4969\n",
            "Epoch 2/4 - Batch 18150/25000 - Loss: 1.5287\n",
            "Epoch 2/4 - Batch 18200/25000 - Loss: 1.4744\n",
            "Epoch 2/4 - Batch 18250/25000 - Loss: 2.0105\n",
            "Epoch 2/4 - Batch 18300/25000 - Loss: 1.9063\n",
            "Epoch 2/4 - Batch 18350/25000 - Loss: 1.1036\n",
            "Epoch 2/4 - Batch 18400/25000 - Loss: 1.7664\n",
            "Epoch 2/4 - Batch 18450/25000 - Loss: 1.7546\n",
            "Epoch 2/4 - Batch 18500/25000 - Loss: 1.9633\n",
            "Epoch 2/4 - Batch 18550/25000 - Loss: 1.6351\n",
            "Epoch 2/4 - Batch 18600/25000 - Loss: 1.9418\n",
            "Epoch 2/4 - Batch 18650/25000 - Loss: 1.7538\n",
            "Epoch 2/4 - Batch 18700/25000 - Loss: 1.4424\n",
            "Epoch 2/4 - Batch 18750/25000 - Loss: 1.5312\n",
            "Epoch 2/4 - Batch 18800/25000 - Loss: 1.6970\n",
            "Epoch 2/4 - Batch 18850/25000 - Loss: 1.5003\n",
            "Epoch 2/4 - Batch 18900/25000 - Loss: 2.0693\n",
            "Epoch 2/4 - Batch 18950/25000 - Loss: 1.1077\n",
            "Epoch 2/4 - Batch 19000/25000 - Loss: 1.7029\n",
            "Epoch 2/4 - Batch 19050/25000 - Loss: 1.5250\n",
            "Epoch 2/4 - Batch 19100/25000 - Loss: 1.9449\n",
            "Epoch 2/4 - Batch 19150/25000 - Loss: 1.9304\n",
            "Epoch 2/4 - Batch 19200/25000 - Loss: 1.7040\n",
            "Epoch 2/4 - Batch 19250/25000 - Loss: 2.0838\n",
            "Epoch 2/4 - Batch 19300/25000 - Loss: 2.0083\n",
            "Epoch 2/4 - Batch 19350/25000 - Loss: 1.7558\n",
            "Epoch 2/4 - Batch 19400/25000 - Loss: 1.6617\n",
            "Epoch 2/4 - Batch 19450/25000 - Loss: 1.8112\n",
            "Epoch 2/4 - Batch 19500/25000 - Loss: 1.6646\n",
            "Epoch 2/4 - Batch 19550/25000 - Loss: 2.0092\n",
            "Epoch 2/4 - Batch 19600/25000 - Loss: 1.9290\n",
            "Epoch 2/4 - Batch 19650/25000 - Loss: 1.7703\n",
            "Epoch 2/4 - Batch 19700/25000 - Loss: 1.8732\n",
            "Epoch 2/4 - Batch 19750/25000 - Loss: 1.8918\n",
            "Epoch 2/4 - Batch 19800/25000 - Loss: 1.6773\n",
            "Epoch 2/4 - Batch 19850/25000 - Loss: 2.0351\n",
            "Epoch 2/4 - Batch 19900/25000 - Loss: 1.4018\n",
            "Epoch 2/4 - Batch 19950/25000 - Loss: 1.8376\n",
            "Epoch 2/4 - Batch 20000/25000 - Loss: 1.7848\n",
            "Epoch 2/4 - Batch 20050/25000 - Loss: 1.7669\n",
            "Epoch 2/4 - Batch 20100/25000 - Loss: 1.7197\n",
            "Epoch 2/4 - Batch 20150/25000 - Loss: 1.6788\n",
            "Epoch 2/4 - Batch 20200/25000 - Loss: 1.5379\n",
            "Epoch 2/4 - Batch 20250/25000 - Loss: 1.9096\n",
            "Epoch 2/4 - Batch 20300/25000 - Loss: 1.9075\n",
            "Epoch 2/4 - Batch 20350/25000 - Loss: 1.5996\n",
            "Epoch 2/4 - Batch 20400/25000 - Loss: 1.7194\n",
            "Epoch 2/4 - Batch 20450/25000 - Loss: 2.6223\n",
            "Epoch 2/4 - Batch 20500/25000 - Loss: 1.4311\n",
            "Epoch 2/4 - Batch 20550/25000 - Loss: 1.9910\n",
            "Epoch 2/4 - Batch 20600/25000 - Loss: 1.4521\n",
            "Epoch 2/4 - Batch 20650/25000 - Loss: 1.9076\n",
            "Epoch 2/4 - Batch 20700/25000 - Loss: 1.9294\n",
            "Epoch 2/4 - Batch 20750/25000 - Loss: 1.6615\n",
            "Epoch 2/4 - Batch 20800/25000 - Loss: 1.7700\n",
            "Epoch 2/4 - Batch 20850/25000 - Loss: 1.8390\n",
            "Epoch 2/4 - Batch 20900/25000 - Loss: 1.5728\n",
            "Epoch 2/4 - Batch 20950/25000 - Loss: 1.8699\n",
            "Epoch 2/4 - Batch 21000/25000 - Loss: 1.9885\n",
            "Epoch 2/4 - Batch 21050/25000 - Loss: 1.8939\n",
            "Epoch 2/4 - Batch 21100/25000 - Loss: 1.6717\n",
            "Epoch 2/4 - Batch 21150/25000 - Loss: 2.0225\n",
            "Epoch 2/4 - Batch 21200/25000 - Loss: 1.5987\n",
            "Epoch 2/4 - Batch 21250/25000 - Loss: 1.5910\n",
            "Epoch 2/4 - Batch 21300/25000 - Loss: 2.1116\n",
            "Epoch 2/4 - Batch 21350/25000 - Loss: 1.7772\n",
            "Epoch 2/4 - Batch 21400/25000 - Loss: 1.8329\n",
            "Epoch 2/4 - Batch 21450/25000 - Loss: 1.6799\n",
            "Epoch 2/4 - Batch 21500/25000 - Loss: 1.6033\n",
            "Epoch 2/4 - Batch 21550/25000 - Loss: 1.7833\n",
            "Epoch 2/4 - Batch 21600/25000 - Loss: 1.8936\n",
            "Epoch 2/4 - Batch 21650/25000 - Loss: 1.9036\n",
            "Epoch 2/4 - Batch 21700/25000 - Loss: 1.4345\n",
            "Epoch 2/4 - Batch 21750/25000 - Loss: 1.8521\n",
            "Epoch 2/4 - Batch 21800/25000 - Loss: 1.7802\n",
            "Epoch 2/4 - Batch 21850/25000 - Loss: 1.9239\n",
            "Epoch 2/4 - Batch 21900/25000 - Loss: 1.6264\n",
            "Epoch 2/4 - Batch 21950/25000 - Loss: 1.4127\n",
            "Epoch 2/4 - Batch 22000/25000 - Loss: 1.7368\n",
            "Epoch 2/4 - Batch 22050/25000 - Loss: 1.4684\n",
            "Epoch 2/4 - Batch 22100/25000 - Loss: 1.4312\n",
            "Epoch 2/4 - Batch 22150/25000 - Loss: 1.3779\n",
            "Epoch 2/4 - Batch 22200/25000 - Loss: 1.9681\n",
            "Epoch 2/4 - Batch 22250/25000 - Loss: 1.9468\n",
            "Epoch 2/4 - Batch 22300/25000 - Loss: 1.5807\n",
            "Epoch 2/4 - Batch 22350/25000 - Loss: 1.7043\n",
            "Epoch 2/4 - Batch 22400/25000 - Loss: 1.7611\n",
            "Epoch 2/4 - Batch 22450/25000 - Loss: 1.6961\n",
            "Epoch 2/4 - Batch 22500/25000 - Loss: 1.9074\n",
            "Epoch 2/4 - Batch 22550/25000 - Loss: 1.8266\n",
            "Epoch 2/4 - Batch 22600/25000 - Loss: 1.6475\n",
            "Epoch 2/4 - Batch 22650/25000 - Loss: 1.6840\n",
            "Epoch 2/4 - Batch 22700/25000 - Loss: 1.7598\n",
            "Epoch 2/4 - Batch 22750/25000 - Loss: 1.6134\n",
            "Epoch 2/4 - Batch 22800/25000 - Loss: 1.7628\n",
            "Epoch 2/4 - Batch 22850/25000 - Loss: 1.6006\n",
            "Epoch 2/4 - Batch 22900/25000 - Loss: 2.2144\n",
            "Epoch 2/4 - Batch 22950/25000 - Loss: 2.0499\n",
            "Epoch 2/4 - Batch 23000/25000 - Loss: 1.3951\n",
            "Epoch 2/4 - Batch 23050/25000 - Loss: 1.1329\n",
            "Epoch 2/4 - Batch 23100/25000 - Loss: 1.6383\n",
            "Epoch 2/4 - Batch 23150/25000 - Loss: 1.7810\n",
            "Epoch 2/4 - Batch 23200/25000 - Loss: 2.2262\n",
            "Epoch 2/4 - Batch 23250/25000 - Loss: 1.6612\n",
            "Epoch 2/4 - Batch 23300/25000 - Loss: 1.4278\n",
            "Epoch 2/4 - Batch 23350/25000 - Loss: 1.7677\n",
            "Epoch 2/4 - Batch 23400/25000 - Loss: 1.8463\n",
            "Epoch 2/4 - Batch 23450/25000 - Loss: 1.4421\n",
            "Epoch 2/4 - Batch 23500/25000 - Loss: 1.5399\n",
            "Epoch 2/4 - Batch 23550/25000 - Loss: 1.7846\n",
            "Epoch 2/4 - Batch 23600/25000 - Loss: 1.8152\n",
            "Epoch 2/4 - Batch 23650/25000 - Loss: 1.1507\n",
            "Epoch 2/4 - Batch 23700/25000 - Loss: 1.6044\n",
            "Epoch 2/4 - Batch 23750/25000 - Loss: 1.4694\n",
            "Epoch 2/4 - Batch 23800/25000 - Loss: 2.1597\n",
            "Epoch 2/4 - Batch 23850/25000 - Loss: 1.7542\n",
            "Epoch 2/4 - Batch 23900/25000 - Loss: 1.8829\n",
            "Epoch 2/4 - Batch 23950/25000 - Loss: 1.4836\n",
            "Epoch 2/4 - Batch 24000/25000 - Loss: 1.9380\n",
            "Epoch 2/4 - Batch 24050/25000 - Loss: 1.8292\n",
            "Epoch 2/4 - Batch 24100/25000 - Loss: 1.5986\n",
            "Epoch 2/4 - Batch 24150/25000 - Loss: 1.7562\n",
            "Epoch 2/4 - Batch 24200/25000 - Loss: 1.9497\n",
            "Epoch 2/4 - Batch 24250/25000 - Loss: 1.8001\n",
            "Epoch 2/4 - Batch 24300/25000 - Loss: 1.6079\n",
            "Epoch 2/4 - Batch 24350/25000 - Loss: 1.7976\n",
            "Epoch 2/4 - Batch 24400/25000 - Loss: 1.5165\n",
            "Epoch 2/4 - Batch 24450/25000 - Loss: 1.9390\n",
            "Epoch 2/4 - Batch 24500/25000 - Loss: 1.8662\n",
            "Epoch 2/4 - Batch 24550/25000 - Loss: 1.8978\n",
            "Epoch 2/4 - Batch 24600/25000 - Loss: 1.6020\n",
            "Epoch 2/4 - Batch 24650/25000 - Loss: 1.6411\n",
            "Epoch 2/4 - Batch 24700/25000 - Loss: 1.2749\n",
            "Epoch 2/4 - Batch 24750/25000 - Loss: 2.4310\n",
            "Epoch 2/4 - Batch 24800/25000 - Loss: 1.4419\n",
            "Epoch 2/4 - Batch 24850/25000 - Loss: 1.7955\n",
            "Epoch 2/4 - Batch 24900/25000 - Loss: 1.5626\n",
            "Epoch 2/4 - Batch 24950/25000 - Loss: 2.1050\n",
            "Epoch 2/4 - Average loss: 1.7587 - Duration: 15970.85s\n",
            "Achieved privacy budget: Îµ = 11.71\n",
            "Epoch 3/4 - Batch 0/25000 - Loss: 1.9772\n",
            "Epoch 3/4 - Batch 50/25000 - Loss: 1.2407\n",
            "Epoch 3/4 - Batch 100/25000 - Loss: 1.4185\n",
            "Epoch 3/4 - Batch 150/25000 - Loss: 1.5324\n",
            "Epoch 3/4 - Batch 200/25000 - Loss: 1.4743\n",
            "Epoch 3/4 - Batch 250/25000 - Loss: 1.6195\n",
            "Epoch 3/4 - Batch 300/25000 - Loss: 2.3066\n",
            "Epoch 3/4 - Batch 350/25000 - Loss: 1.7503\n",
            "Epoch 3/4 - Batch 400/25000 - Loss: 1.4608\n",
            "Epoch 3/4 - Batch 450/25000 - Loss: 1.9928\n",
            "Epoch 3/4 - Batch 500/25000 - Loss: 1.7523\n",
            "Epoch 3/4 - Batch 550/25000 - Loss: 2.0327\n",
            "Epoch 3/4 - Batch 600/25000 - Loss: 1.7542\n",
            "Epoch 3/4 - Batch 650/25000 - Loss: 2.3401\n",
            "Epoch 3/4 - Batch 700/25000 - Loss: 1.1287\n",
            "Epoch 3/4 - Batch 750/25000 - Loss: 1.6452\n",
            "Epoch 3/4 - Batch 800/25000 - Loss: 1.6482\n",
            "Epoch 3/4 - Batch 850/25000 - Loss: 1.8701\n",
            "Epoch 3/4 - Batch 900/25000 - Loss: 1.9426\n",
            "Epoch 3/4 - Batch 950/25000 - Loss: 1.9834\n",
            "Epoch 3/4 - Batch 1000/25000 - Loss: 1.6681\n",
            "Epoch 3/4 - Batch 1050/25000 - Loss: 1.5799\n",
            "Epoch 3/4 - Batch 1100/25000 - Loss: 1.7061\n",
            "Epoch 3/4 - Batch 1150/25000 - Loss: 1.5074\n",
            "Epoch 3/4 - Batch 1200/25000 - Loss: 1.7653\n",
            "Epoch 3/4 - Batch 1250/25000 - Loss: 1.8854\n",
            "Epoch 3/4 - Batch 1300/25000 - Loss: 1.5688\n",
            "Epoch 3/4 - Batch 1350/25000 - Loss: 1.5635\n",
            "Epoch 3/4 - Batch 1400/25000 - Loss: 1.2376\n",
            "Epoch 3/4 - Batch 1450/25000 - Loss: 1.6560\n",
            "Epoch 3/4 - Batch 1500/25000 - Loss: 1.1892\n",
            "Epoch 3/4 - Batch 1550/25000 - Loss: 1.8154\n",
            "Epoch 3/4 - Batch 1600/25000 - Loss: 1.2074\n",
            "Epoch 3/4 - Batch 1650/25000 - Loss: 1.7740\n",
            "Epoch 3/4 - Batch 1700/25000 - Loss: 1.5605\n",
            "Epoch 3/4 - Batch 1750/25000 - Loss: 1.8708\n",
            "Epoch 3/4 - Batch 1800/25000 - Loss: 1.9001\n",
            "Epoch 3/4 - Batch 1850/25000 - Loss: 1.9060\n",
            "Epoch 3/4 - Batch 1900/25000 - Loss: 1.1906\n",
            "Epoch 3/4 - Batch 1950/25000 - Loss: 1.4110\n",
            "Epoch 3/4 - Batch 2000/25000 - Loss: 1.6374\n",
            "Epoch 3/4 - Batch 2050/25000 - Loss: 1.7721\n",
            "Epoch 3/4 - Batch 2100/25000 - Loss: 2.2005\n",
            "Epoch 3/4 - Batch 2150/25000 - Loss: 1.1466\n",
            "Epoch 3/4 - Batch 2200/25000 - Loss: 1.4193\n",
            "Epoch 3/4 - Batch 2250/25000 - Loss: 2.0414\n",
            "Epoch 3/4 - Batch 2300/25000 - Loss: 1.4381\n",
            "Epoch 3/4 - Batch 2350/25000 - Loss: 1.9208\n",
            "Epoch 3/4 - Batch 2400/25000 - Loss: 1.8400\n",
            "Epoch 3/4 - Batch 2450/25000 - Loss: 2.2366\n",
            "Epoch 3/4 - Batch 2500/25000 - Loss: 1.9185\n",
            "Epoch 3/4 - Batch 2550/25000 - Loss: 1.5604\n",
            "Epoch 3/4 - Batch 2600/25000 - Loss: 1.8422\n",
            "Epoch 3/4 - Batch 2650/25000 - Loss: 1.4615\n",
            "Epoch 3/4 - Batch 2700/25000 - Loss: 2.0268\n",
            "Epoch 3/4 - Batch 2750/25000 - Loss: 1.6511\n",
            "Epoch 3/4 - Batch 2800/25000 - Loss: 1.4696\n",
            "Epoch 3/4 - Batch 2850/25000 - Loss: 1.7900\n",
            "Epoch 3/4 - Batch 2900/25000 - Loss: 1.8291\n",
            "Epoch 3/4 - Batch 2950/25000 - Loss: 0.8319\n",
            "Epoch 3/4 - Batch 3000/25000 - Loss: 1.6857\n",
            "Epoch 3/4 - Batch 3050/25000 - Loss: 2.1126\n",
            "Epoch 3/4 - Batch 3100/25000 - Loss: 1.9637\n",
            "Epoch 3/4 - Batch 3150/25000 - Loss: 1.7125\n",
            "Epoch 3/4 - Batch 3200/25000 - Loss: 2.1732\n",
            "Epoch 3/4 - Batch 3250/25000 - Loss: 1.5400\n",
            "Epoch 3/4 - Batch 3300/25000 - Loss: 1.6915\n",
            "Epoch 3/4 - Batch 3350/25000 - Loss: 2.0307\n",
            "Epoch 3/4 - Batch 3400/25000 - Loss: 1.7611\n",
            "Epoch 3/4 - Batch 3450/25000 - Loss: 2.0798\n",
            "Epoch 3/4 - Batch 3500/25000 - Loss: 1.9456\n",
            "Epoch 3/4 - Batch 3550/25000 - Loss: 1.5954\n",
            "Epoch 3/4 - Batch 3600/25000 - Loss: 1.0074\n",
            "Epoch 3/4 - Batch 3650/25000 - Loss: 1.9289\n",
            "Epoch 3/4 - Batch 3700/25000 - Loss: 2.1914\n",
            "Epoch 3/4 - Batch 3750/25000 - Loss: 1.5384\n",
            "Epoch 3/4 - Batch 3800/25000 - Loss: 1.4626\n",
            "Epoch 3/4 - Batch 3850/25000 - Loss: 1.4452\n",
            "Epoch 3/4 - Batch 3900/25000 - Loss: 1.8946\n",
            "Epoch 3/4 - Batch 3950/25000 - Loss: 2.3935\n",
            "Epoch 3/4 - Batch 4000/25000 - Loss: 2.0999\n",
            "Epoch 3/4 - Batch 4050/25000 - Loss: 1.3932\n",
            "Epoch 3/4 - Batch 4100/25000 - Loss: 2.4666\n",
            "Epoch 3/4 - Batch 4150/25000 - Loss: 1.3937\n",
            "Epoch 3/4 - Batch 4200/25000 - Loss: 1.9350\n",
            "Epoch 3/4 - Batch 4250/25000 - Loss: 1.7392\n",
            "Epoch 3/4 - Batch 4300/25000 - Loss: 1.0670\n",
            "Epoch 3/4 - Batch 4350/25000 - Loss: 1.8151\n",
            "Epoch 3/4 - Batch 4400/25000 - Loss: 1.9072\n",
            "Epoch 3/4 - Batch 4450/25000 - Loss: 1.5783\n",
            "Epoch 3/4 - Batch 4500/25000 - Loss: 1.4226\n",
            "Epoch 3/4 - Batch 4550/25000 - Loss: 2.0150\n",
            "Epoch 3/4 - Batch 4600/25000 - Loss: 1.3840\n",
            "Epoch 3/4 - Batch 4650/25000 - Loss: 1.7398\n",
            "Epoch 3/4 - Batch 4700/25000 - Loss: 1.1594\n",
            "Epoch 3/4 - Batch 4750/25000 - Loss: 1.5709\n",
            "Epoch 3/4 - Batch 4800/25000 - Loss: 1.9457\n",
            "Epoch 3/4 - Batch 4850/25000 - Loss: 1.9754\n",
            "Epoch 3/4 - Batch 4900/25000 - Loss: 1.3088\n",
            "Epoch 3/4 - Batch 4950/25000 - Loss: 1.5121\n",
            "Epoch 3/4 - Batch 5000/25000 - Loss: 1.7115\n",
            "Epoch 3/4 - Batch 5050/25000 - Loss: 1.7576\n",
            "Epoch 3/4 - Batch 5100/25000 - Loss: 1.6140\n",
            "Epoch 3/4 - Batch 5150/25000 - Loss: 1.5162\n",
            "Epoch 3/4 - Batch 5200/25000 - Loss: 2.1875\n",
            "Epoch 3/4 - Batch 5250/25000 - Loss: 1.9649\n",
            "Epoch 3/4 - Batch 5300/25000 - Loss: 1.9465\n",
            "Epoch 3/4 - Batch 5350/25000 - Loss: 1.0004\n",
            "Epoch 3/4 - Batch 5400/25000 - Loss: 1.9088\n",
            "Epoch 3/4 - Batch 5450/25000 - Loss: 1.4726\n",
            "Epoch 3/4 - Batch 5500/25000 - Loss: 1.9570\n",
            "Epoch 3/4 - Batch 5550/25000 - Loss: 1.8262\n",
            "Epoch 3/4 - Batch 5600/25000 - Loss: 1.7003\n",
            "Epoch 3/4 - Batch 5650/25000 - Loss: 1.5972\n",
            "Epoch 3/4 - Batch 5700/25000 - Loss: 1.3260\n",
            "Epoch 3/4 - Batch 5750/25000 - Loss: 2.0110\n",
            "Epoch 3/4 - Batch 5800/25000 - Loss: 2.0728\n",
            "Epoch 3/4 - Batch 5850/25000 - Loss: 1.3972\n",
            "Epoch 3/4 - Batch 5900/25000 - Loss: 2.2636\n",
            "Epoch 3/4 - Batch 5950/25000 - Loss: 1.8528\n",
            "Epoch 3/4 - Batch 6000/25000 - Loss: 1.9600\n",
            "Epoch 3/4 - Batch 6050/25000 - Loss: 1.3797\n",
            "Epoch 3/4 - Batch 6100/25000 - Loss: 1.9404\n",
            "Epoch 3/4 - Batch 6150/25000 - Loss: 1.6150\n",
            "Epoch 3/4 - Batch 6200/25000 - Loss: 1.6393\n",
            "Epoch 3/4 - Batch 6250/25000 - Loss: 1.3018\n",
            "Epoch 3/4 - Batch 6300/25000 - Loss: 1.6167\n",
            "Epoch 3/4 - Batch 6350/25000 - Loss: 0.7131\n",
            "Epoch 3/4 - Batch 6400/25000 - Loss: 1.6685\n",
            "Epoch 3/4 - Batch 6450/25000 - Loss: 1.6254\n",
            "Epoch 3/4 - Batch 6500/25000 - Loss: 1.8591\n",
            "Epoch 3/4 - Batch 6550/25000 - Loss: 2.0243\n",
            "Epoch 3/4 - Batch 6600/25000 - Loss: 1.3160\n",
            "Epoch 3/4 - Batch 6650/25000 - Loss: 1.5964\n",
            "Epoch 3/4 - Batch 6700/25000 - Loss: 1.8013\n",
            "Epoch 3/4 - Batch 6750/25000 - Loss: 1.7510\n",
            "Epoch 3/4 - Batch 6800/25000 - Loss: 1.8615\n",
            "Epoch 3/4 - Batch 6850/25000 - Loss: 1.4683\n",
            "Epoch 3/4 - Batch 6900/25000 - Loss: 1.7963\n",
            "Epoch 3/4 - Batch 6950/25000 - Loss: 1.8408\n",
            "Epoch 3/4 - Batch 7000/25000 - Loss: 1.9946\n",
            "Epoch 3/4 - Batch 7050/25000 - Loss: 1.5811\n",
            "Epoch 3/4 - Batch 7100/25000 - Loss: 1.3001\n",
            "Epoch 3/4 - Batch 7150/25000 - Loss: 1.5544\n",
            "Epoch 3/4 - Batch 7200/25000 - Loss: 1.6677\n",
            "Epoch 3/4 - Batch 7250/25000 - Loss: 1.7284\n",
            "Epoch 3/4 - Batch 7300/25000 - Loss: 1.8669\n",
            "Epoch 3/4 - Batch 7350/25000 - Loss: 1.2939\n",
            "Epoch 3/4 - Batch 7400/25000 - Loss: 1.9446\n",
            "Epoch 3/4 - Batch 7450/25000 - Loss: 1.8853\n",
            "Epoch 3/4 - Batch 7500/25000 - Loss: 1.4982\n",
            "Epoch 3/4 - Batch 7550/25000 - Loss: 1.7534\n",
            "Epoch 3/4 - Batch 7600/25000 - Loss: 1.8799\n",
            "Epoch 3/4 - Batch 7650/25000 - Loss: 1.4624\n",
            "Epoch 3/4 - Batch 7700/25000 - Loss: 1.9810\n",
            "Epoch 3/4 - Batch 7750/25000 - Loss: 1.5925\n",
            "Epoch 3/4 - Batch 7800/25000 - Loss: 1.1546\n",
            "Epoch 3/4 - Batch 7850/25000 - Loss: 1.2909\n",
            "Epoch 3/4 - Batch 7900/25000 - Loss: 1.9062\n",
            "Epoch 3/4 - Batch 7950/25000 - Loss: 1.9840\n",
            "Epoch 3/4 - Batch 8000/25000 - Loss: 1.5420\n",
            "Epoch 3/4 - Batch 8050/25000 - Loss: 2.1447\n",
            "Epoch 3/4 - Batch 8100/25000 - Loss: 1.6060\n",
            "Epoch 3/4 - Batch 8150/25000 - Loss: 2.1314\n",
            "Epoch 3/4 - Batch 8200/25000 - Loss: 1.5687\n",
            "Epoch 3/4 - Batch 8250/25000 - Loss: 1.8356\n",
            "Epoch 3/4 - Batch 8300/25000 - Loss: 0.8716\n",
            "Epoch 3/4 - Batch 8350/25000 - Loss: 1.7562\n",
            "Epoch 3/4 - Batch 8400/25000 - Loss: 1.7230\n",
            "Epoch 3/4 - Batch 8450/25000 - Loss: 1.7001\n",
            "Epoch 3/4 - Batch 8500/25000 - Loss: 1.9381\n",
            "Epoch 3/4 - Batch 8550/25000 - Loss: 1.1713\n",
            "Epoch 3/4 - Batch 8600/25000 - Loss: 2.0248\n",
            "Epoch 3/4 - Batch 8650/25000 - Loss: 0.9456\n",
            "Epoch 3/4 - Batch 8700/25000 - Loss: 1.6431\n",
            "Epoch 3/4 - Batch 8750/25000 - Loss: 1.1636\n",
            "Epoch 3/4 - Batch 8800/25000 - Loss: 1.7498\n",
            "Epoch 3/4 - Batch 8850/25000 - Loss: 1.9926\n",
            "Epoch 3/4 - Batch 8900/25000 - Loss: 1.7391\n",
            "Epoch 3/4 - Batch 8950/25000 - Loss: 1.5944\n",
            "Epoch 3/4 - Batch 9000/25000 - Loss: 2.0139\n",
            "Epoch 3/4 - Batch 9050/25000 - Loss: 1.9689\n",
            "Epoch 3/4 - Batch 9100/25000 - Loss: 1.9342\n",
            "Epoch 3/4 - Batch 9150/25000 - Loss: 2.0669\n",
            "Epoch 3/4 - Batch 9200/25000 - Loss: 1.8403\n",
            "Epoch 3/4 - Batch 9250/25000 - Loss: 2.0753\n",
            "Epoch 3/4 - Batch 9300/25000 - Loss: 1.6844\n",
            "Epoch 3/4 - Batch 9350/25000 - Loss: 0.9072\n",
            "Epoch 3/4 - Batch 9400/25000 - Loss: 1.8083\n",
            "Epoch 3/4 - Batch 9450/25000 - Loss: 1.8071\n",
            "Epoch 3/4 - Batch 9500/25000 - Loss: 1.9351\n",
            "Epoch 3/4 - Batch 9550/25000 - Loss: 1.5392\n",
            "Epoch 3/4 - Batch 9600/25000 - Loss: 1.7052\n",
            "Epoch 3/4 - Batch 9650/25000 - Loss: 2.2480\n",
            "Epoch 3/4 - Batch 9700/25000 - Loss: 2.2938\n",
            "Epoch 3/4 - Batch 9750/25000 - Loss: 1.8745\n",
            "Epoch 3/4 - Batch 9800/25000 - Loss: 1.1477\n",
            "Epoch 3/4 - Batch 9850/25000 - Loss: 2.1538\n",
            "Epoch 3/4 - Batch 9900/25000 - Loss: 2.0088\n",
            "Epoch 3/4 - Batch 9950/25000 - Loss: 1.5665\n",
            "Epoch 3/4 - Batch 10000/25000 - Loss: 1.7474\n",
            "Epoch 3/4 - Batch 10050/25000 - Loss: 1.5078\n",
            "Epoch 3/4 - Batch 10100/25000 - Loss: 1.9003\n",
            "Epoch 3/4 - Batch 10150/25000 - Loss: 1.9510\n",
            "Epoch 3/4 - Batch 10200/25000 - Loss: 1.8110\n",
            "Epoch 3/4 - Batch 10250/25000 - Loss: 1.8775\n",
            "Epoch 3/4 - Batch 10300/25000 - Loss: 2.0656\n",
            "Epoch 3/4 - Batch 10350/25000 - Loss: 1.5177\n",
            "Epoch 3/4 - Batch 10400/25000 - Loss: 1.3840\n",
            "Epoch 3/4 - Batch 10450/25000 - Loss: 1.4363\n",
            "Epoch 3/4 - Batch 10500/25000 - Loss: 1.8563\n",
            "Epoch 3/4 - Batch 10550/25000 - Loss: 1.5944\n",
            "Epoch 3/4 - Batch 10600/25000 - Loss: 2.0267\n",
            "Epoch 3/4 - Batch 10650/25000 - Loss: 1.5860\n",
            "Epoch 3/4 - Batch 10700/25000 - Loss: 1.7461\n",
            "Epoch 3/4 - Batch 10750/25000 - Loss: 1.9124\n",
            "Epoch 3/4 - Batch 10800/25000 - Loss: 1.9208\n",
            "Epoch 3/4 - Batch 10850/25000 - Loss: 1.8951\n",
            "Epoch 3/4 - Batch 10900/25000 - Loss: 2.1891\n",
            "Epoch 3/4 - Batch 10950/25000 - Loss: 1.0855\n",
            "Epoch 3/4 - Batch 11000/25000 - Loss: 1.8659\n",
            "Epoch 3/4 - Batch 11050/25000 - Loss: 1.9710\n",
            "Epoch 3/4 - Batch 11100/25000 - Loss: 1.6387\n",
            "Epoch 3/4 - Batch 11150/25000 - Loss: 1.5453\n",
            "Epoch 3/4 - Batch 11200/25000 - Loss: 0.8776\n",
            "Epoch 3/4 - Batch 11250/25000 - Loss: 1.6414\n",
            "Epoch 3/4 - Batch 11300/25000 - Loss: 1.6474\n",
            "Epoch 3/4 - Batch 11350/25000 - Loss: 1.8848\n",
            "Epoch 3/4 - Batch 11400/25000 - Loss: 0.8125\n",
            "Epoch 3/4 - Batch 11450/25000 - Loss: 1.5312\n",
            "Epoch 3/4 - Batch 11500/25000 - Loss: 1.5548\n",
            "Epoch 3/4 - Batch 11550/25000 - Loss: 1.7123\n",
            "Epoch 3/4 - Batch 11600/25000 - Loss: 1.6272\n",
            "Epoch 3/4 - Batch 11650/25000 - Loss: 1.7920\n",
            "Epoch 3/4 - Batch 11700/25000 - Loss: 1.7030\n",
            "Epoch 3/4 - Batch 11750/25000 - Loss: 1.6991\n",
            "Epoch 3/4 - Batch 11800/25000 - Loss: 1.8003\n",
            "Epoch 3/4 - Batch 11850/25000 - Loss: 1.5968\n",
            "Epoch 3/4 - Batch 11900/25000 - Loss: 1.9451\n",
            "Epoch 3/4 - Batch 11950/25000 - Loss: 1.8204\n",
            "Epoch 3/4 - Batch 12000/25000 - Loss: 1.4484\n",
            "Epoch 3/4 - Batch 12050/25000 - Loss: 1.8906\n",
            "Epoch 3/4 - Batch 12100/25000 - Loss: 1.5038\n",
            "Epoch 3/4 - Batch 12150/25000 - Loss: 1.8652\n",
            "Epoch 3/4 - Batch 12200/25000 - Loss: 1.9434\n",
            "Epoch 3/4 - Batch 12250/25000 - Loss: 1.2559\n",
            "Epoch 3/4 - Batch 12300/25000 - Loss: 1.7520\n",
            "Epoch 3/4 - Batch 12350/25000 - Loss: 1.9210\n",
            "Epoch 3/4 - Batch 12400/25000 - Loss: 1.5903\n",
            "Epoch 3/4 - Batch 12450/25000 - Loss: 1.8066\n",
            "Epoch 3/4 - Batch 12500/25000 - Loss: 1.5816\n",
            "Epoch 3/4 - Batch 12550/25000 - Loss: 1.7585\n",
            "Epoch 3/4 - Batch 12600/25000 - Loss: 0.9649\n",
            "Epoch 3/4 - Batch 12650/25000 - Loss: 1.7941\n",
            "Epoch 3/4 - Batch 12700/25000 - Loss: 1.6785\n",
            "Epoch 3/4 - Batch 12750/25000 - Loss: 1.5651\n",
            "Epoch 3/4 - Batch 12800/25000 - Loss: 1.6287\n",
            "Epoch 3/4 - Batch 12850/25000 - Loss: 1.6435\n",
            "Epoch 3/4 - Batch 12900/25000 - Loss: 1.5154\n",
            "Epoch 3/4 - Batch 12950/25000 - Loss: 1.7863\n",
            "Epoch 3/4 - Batch 13000/25000 - Loss: 1.4094\n",
            "Epoch 3/4 - Batch 13050/25000 - Loss: 1.7118\n",
            "Epoch 3/4 - Batch 13100/25000 - Loss: 1.6425\n",
            "Epoch 3/4 - Batch 13150/25000 - Loss: 1.5877\n",
            "Epoch 3/4 - Batch 13200/25000 - Loss: 1.7137\n",
            "Epoch 3/4 - Batch 13250/25000 - Loss: 1.7907\n",
            "Epoch 3/4 - Batch 13300/25000 - Loss: 1.0516\n",
            "Epoch 3/4 - Batch 13350/25000 - Loss: 1.5437\n",
            "Epoch 3/4 - Batch 13400/25000 - Loss: 1.2394\n",
            "Epoch 3/4 - Batch 13450/25000 - Loss: 1.9207\n",
            "Epoch 3/4 - Batch 13500/25000 - Loss: 1.5691\n",
            "Epoch 3/4 - Batch 13550/25000 - Loss: 1.2750\n",
            "Epoch 3/4 - Batch 13600/25000 - Loss: 1.4928\n",
            "Epoch 3/4 - Batch 13650/25000 - Loss: 1.8341\n",
            "Epoch 3/4 - Batch 13700/25000 - Loss: 1.1411\n",
            "Epoch 3/4 - Batch 13750/25000 - Loss: 1.5083\n",
            "Epoch 3/4 - Batch 13800/25000 - Loss: 1.6673\n",
            "Epoch 3/4 - Batch 13850/25000 - Loss: 1.7602\n",
            "Epoch 3/4 - Batch 13900/25000 - Loss: 1.0322\n",
            "Epoch 3/4 - Batch 13950/25000 - Loss: 1.7747\n",
            "Epoch 3/4 - Batch 14000/25000 - Loss: 2.0730\n",
            "Epoch 3/4 - Batch 14050/25000 - Loss: 2.1946\n",
            "Epoch 3/4 - Batch 14100/25000 - Loss: 1.8353\n",
            "Epoch 3/4 - Batch 14150/25000 - Loss: 2.0636\n",
            "Epoch 3/4 - Batch 14200/25000 - Loss: 1.7604\n",
            "Epoch 3/4 - Batch 14250/25000 - Loss: 1.7090\n",
            "Epoch 3/4 - Batch 14300/25000 - Loss: 2.0823\n",
            "Epoch 3/4 - Batch 14350/25000 - Loss: 1.6734\n",
            "Epoch 3/4 - Batch 14400/25000 - Loss: 1.7260\n",
            "Epoch 3/4 - Batch 14450/25000 - Loss: 2.0365\n",
            "Epoch 3/4 - Batch 14500/25000 - Loss: 1.5731\n",
            "Epoch 3/4 - Batch 14550/25000 - Loss: 1.5701\n",
            "Epoch 3/4 - Batch 14600/25000 - Loss: 1.7371\n",
            "Epoch 3/4 - Batch 14650/25000 - Loss: 1.6862\n",
            "Epoch 3/4 - Batch 14700/25000 - Loss: 1.9318\n",
            "Epoch 3/4 - Batch 14750/25000 - Loss: 1.8774\n",
            "Epoch 3/4 - Batch 14800/25000 - Loss: 1.9179\n",
            "Epoch 3/4 - Batch 14850/25000 - Loss: 1.8309\n",
            "Epoch 3/4 - Batch 14900/25000 - Loss: 1.3608\n",
            "Epoch 3/4 - Batch 14950/25000 - Loss: 1.7560\n",
            "Epoch 3/4 - Batch 15000/25000 - Loss: 2.0063\n",
            "Epoch 3/4 - Batch 15050/25000 - Loss: 1.4241\n",
            "Epoch 3/4 - Batch 15100/25000 - Loss: 1.4070\n",
            "Epoch 3/4 - Batch 15150/25000 - Loss: 1.5690\n",
            "Epoch 3/4 - Batch 15200/25000 - Loss: 2.1195\n",
            "Epoch 3/4 - Batch 15250/25000 - Loss: 1.8572\n",
            "Epoch 3/4 - Batch 15300/25000 - Loss: 2.0077\n",
            "Epoch 3/4 - Batch 15350/25000 - Loss: 1.2565\n",
            "Epoch 3/4 - Batch 15400/25000 - Loss: 1.6979\n",
            "Epoch 3/4 - Batch 15450/25000 - Loss: 2.1141\n",
            "Epoch 3/4 - Batch 15500/25000 - Loss: 1.7328\n",
            "Epoch 3/4 - Batch 15550/25000 - Loss: 1.7962\n",
            "Epoch 3/4 - Batch 15600/25000 - Loss: 1.5149\n",
            "Epoch 3/4 - Batch 15650/25000 - Loss: 1.5147\n",
            "Epoch 3/4 - Batch 15700/25000 - Loss: 2.0851\n",
            "Epoch 3/4 - Batch 15750/25000 - Loss: 1.8119\n",
            "Epoch 3/4 - Batch 15800/25000 - Loss: 1.7689\n",
            "Epoch 3/4 - Batch 15850/25000 - Loss: 1.9454\n",
            "Epoch 3/4 - Batch 15900/25000 - Loss: 1.6254\n",
            "Epoch 3/4 - Batch 15950/25000 - Loss: 2.1564\n",
            "Epoch 3/4 - Batch 16000/25000 - Loss: 2.0273\n",
            "Epoch 3/4 - Batch 16050/25000 - Loss: 1.8811\n",
            "Epoch 3/4 - Batch 16100/25000 - Loss: 2.1274\n",
            "Epoch 3/4 - Batch 16150/25000 - Loss: 1.4888\n",
            "Epoch 3/4 - Batch 16200/25000 - Loss: 1.6708\n",
            "Epoch 3/4 - Batch 16250/25000 - Loss: 1.9972\n",
            "Epoch 3/4 - Batch 16300/25000 - Loss: 1.5812\n",
            "Epoch 3/4 - Batch 16350/25000 - Loss: 1.9081\n",
            "Epoch 3/4 - Batch 16400/25000 - Loss: 1.7155\n",
            "Epoch 3/4 - Batch 16450/25000 - Loss: 2.3781\n",
            "Epoch 3/4 - Batch 16500/25000 - Loss: 2.0560\n",
            "Epoch 3/4 - Batch 16550/25000 - Loss: 0.8625\n",
            "Epoch 3/4 - Batch 16600/25000 - Loss: 2.0535\n",
            "Epoch 3/4 - Batch 16650/25000 - Loss: 1.1802\n",
            "Epoch 3/4 - Batch 16700/25000 - Loss: 1.5667\n",
            "Epoch 3/4 - Batch 16750/25000 - Loss: 1.7009\n",
            "Epoch 3/4 - Batch 16800/25000 - Loss: 1.9020\n",
            "Epoch 3/4 - Batch 16850/25000 - Loss: 1.5736\n",
            "Epoch 3/4 - Batch 16900/25000 - Loss: 1.4358\n",
            "Epoch 3/4 - Batch 16950/25000 - Loss: 1.8978\n",
            "Epoch 3/4 - Batch 17000/25000 - Loss: 1.6424\n",
            "Epoch 3/4 - Batch 17050/25000 - Loss: 1.8163\n",
            "Epoch 3/4 - Batch 17100/25000 - Loss: 1.7646\n",
            "Epoch 3/4 - Batch 17150/25000 - Loss: 1.8110\n",
            "Epoch 3/4 - Batch 17200/25000 - Loss: 1.9405\n",
            "Epoch 3/4 - Batch 17250/25000 - Loss: 1.8972\n",
            "Epoch 3/4 - Batch 17300/25000 - Loss: 1.6041\n",
            "Epoch 3/4 - Batch 17350/25000 - Loss: 2.1058\n",
            "Epoch 3/4 - Batch 17400/25000 - Loss: 1.4824\n",
            "Epoch 3/4 - Batch 17450/25000 - Loss: 1.9777\n",
            "Epoch 3/4 - Batch 17500/25000 - Loss: 1.4467\n",
            "Epoch 3/4 - Batch 17550/25000 - Loss: 1.8121\n",
            "Epoch 3/4 - Batch 17600/25000 - Loss: 1.8916\n",
            "Epoch 3/4 - Batch 17650/25000 - Loss: 1.5963\n",
            "Epoch 3/4 - Batch 17700/25000 - Loss: 1.9794\n",
            "Epoch 3/4 - Batch 17750/25000 - Loss: 1.2370\n",
            "Epoch 3/4 - Batch 17800/25000 - Loss: 1.8522\n",
            "Epoch 3/4 - Batch 17850/25000 - Loss: 1.6746\n",
            "Epoch 3/4 - Batch 17900/25000 - Loss: 1.3822\n",
            "Epoch 3/4 - Batch 17950/25000 - Loss: 1.8377\n",
            "Epoch 3/4 - Batch 18000/25000 - Loss: 2.1680\n",
            "Epoch 3/4 - Batch 18050/25000 - Loss: 1.6126\n",
            "Epoch 3/4 - Batch 18100/25000 - Loss: 1.4916\n",
            "Epoch 3/4 - Batch 18150/25000 - Loss: 1.7107\n",
            "Epoch 3/4 - Batch 18200/25000 - Loss: 1.7074\n",
            "Epoch 3/4 - Batch 18250/25000 - Loss: 1.4212\n",
            "Epoch 3/4 - Batch 18300/25000 - Loss: 1.7247\n",
            "Epoch 3/4 - Batch 18350/25000 - Loss: 2.0864\n",
            "Epoch 3/4 - Batch 18400/25000 - Loss: 1.7821\n",
            "Epoch 3/4 - Batch 18450/25000 - Loss: 1.1067\n",
            "Epoch 3/4 - Batch 18500/25000 - Loss: 1.7123\n",
            "Epoch 3/4 - Batch 18550/25000 - Loss: 1.6968\n",
            "Epoch 3/4 - Batch 18600/25000 - Loss: 1.3760\n",
            "Epoch 3/4 - Batch 18650/25000 - Loss: 1.5873\n",
            "Epoch 3/4 - Batch 18700/25000 - Loss: 1.7254\n",
            "Epoch 3/4 - Batch 18750/25000 - Loss: 1.7382\n",
            "Epoch 3/4 - Batch 18800/25000 - Loss: 1.6831\n",
            "Epoch 3/4 - Batch 18850/25000 - Loss: 1.7690\n",
            "Epoch 3/4 - Batch 18900/25000 - Loss: 1.6624\n",
            "Epoch 3/4 - Batch 18950/25000 - Loss: 1.2157\n",
            "Epoch 3/4 - Batch 19000/25000 - Loss: 2.4090\n",
            "Epoch 3/4 - Batch 19050/25000 - Loss: 1.8495\n",
            "Epoch 3/4 - Batch 19100/25000 - Loss: 2.0382\n",
            "Epoch 3/4 - Batch 19150/25000 - Loss: 1.6546\n",
            "Epoch 3/4 - Batch 19200/25000 - Loss: 1.7939\n",
            "Epoch 3/4 - Batch 19250/25000 - Loss: 2.0154\n",
            "Epoch 3/4 - Batch 19300/25000 - Loss: 1.7204\n",
            "Epoch 3/4 - Batch 19350/25000 - Loss: 1.8199\n",
            "Epoch 3/4 - Batch 19400/25000 - Loss: 1.9778\n",
            "Epoch 3/4 - Batch 19450/25000 - Loss: 1.6954\n",
            "Epoch 3/4 - Batch 19500/25000 - Loss: 1.9584\n",
            "Epoch 3/4 - Batch 19550/25000 - Loss: 1.7414\n",
            "Epoch 3/4 - Batch 19600/25000 - Loss: 1.5491\n",
            "Epoch 3/4 - Batch 19650/25000 - Loss: 1.5381\n",
            "Epoch 3/4 - Batch 19700/25000 - Loss: 2.0336\n",
            "Epoch 3/4 - Batch 19750/25000 - Loss: 1.8202\n",
            "Epoch 3/4 - Batch 19800/25000 - Loss: 1.0957\n",
            "Epoch 3/4 - Batch 19850/25000 - Loss: 2.2324\n",
            "Epoch 3/4 - Batch 19900/25000 - Loss: 1.7416\n",
            "Epoch 3/4 - Batch 19950/25000 - Loss: 1.7575\n",
            "Epoch 3/4 - Batch 20000/25000 - Loss: 1.6997\n",
            "Epoch 3/4 - Batch 20050/25000 - Loss: 1.7585\n",
            "Epoch 3/4 - Batch 20100/25000 - Loss: 1.4614\n",
            "Epoch 3/4 - Batch 20150/25000 - Loss: 2.0482\n",
            "Epoch 3/4 - Batch 20200/25000 - Loss: 1.5116\n",
            "Epoch 3/4 - Batch 20250/25000 - Loss: 1.7443\n",
            "Epoch 3/4 - Batch 20300/25000 - Loss: 1.6597\n",
            "Epoch 3/4 - Batch 20350/25000 - Loss: 1.9628\n",
            "Epoch 3/4 - Batch 20400/25000 - Loss: 2.0125\n",
            "Epoch 3/4 - Batch 20450/25000 - Loss: 2.2170\n",
            "Epoch 3/4 - Batch 20500/25000 - Loss: 1.6371\n",
            "Epoch 3/4 - Batch 20550/25000 - Loss: 1.7803\n",
            "Epoch 3/4 - Batch 20600/25000 - Loss: 1.9273\n",
            "Epoch 3/4 - Batch 20650/25000 - Loss: 1.8857\n",
            "Epoch 3/4 - Batch 20700/25000 - Loss: 1.9095\n",
            "Epoch 3/4 - Batch 20750/25000 - Loss: 1.5343\n",
            "Epoch 3/4 - Batch 20800/25000 - Loss: 2.0784\n",
            "Epoch 3/4 - Batch 20850/25000 - Loss: 1.8018\n",
            "Epoch 3/4 - Batch 20900/25000 - Loss: 1.9794\n",
            "Epoch 3/4 - Batch 20950/25000 - Loss: 1.6819\n",
            "Epoch 3/4 - Batch 21000/25000 - Loss: 1.2197\n",
            "Epoch 3/4 - Batch 21050/25000 - Loss: 1.3824\n",
            "Epoch 3/4 - Batch 21100/25000 - Loss: 1.4113\n",
            "Epoch 3/4 - Batch 21150/25000 - Loss: 2.0384\n",
            "Epoch 3/4 - Batch 21200/25000 - Loss: 1.8863\n",
            "Epoch 3/4 - Batch 21250/25000 - Loss: 1.6143\n",
            "Epoch 3/4 - Batch 21300/25000 - Loss: 1.5163\n",
            "Epoch 3/4 - Batch 21350/25000 - Loss: 1.9151\n",
            "Epoch 3/4 - Batch 21400/25000 - Loss: 1.3882\n",
            "Epoch 3/4 - Batch 21450/25000 - Loss: 1.4889\n",
            "Epoch 3/4 - Batch 21500/25000 - Loss: 1.8010\n",
            "Epoch 3/4 - Batch 21550/25000 - Loss: 1.6130\n",
            "Epoch 3/4 - Batch 21600/25000 - Loss: 1.6784\n",
            "Epoch 3/4 - Batch 21650/25000 - Loss: 2.0105\n",
            "Epoch 3/4 - Batch 21700/25000 - Loss: 1.8821\n",
            "Epoch 3/4 - Batch 21750/25000 - Loss: 1.5088\n",
            "Epoch 3/4 - Batch 21800/25000 - Loss: 1.6042\n",
            "Epoch 3/4 - Batch 21850/25000 - Loss: 1.8577\n",
            "Epoch 3/4 - Batch 21900/25000 - Loss: 1.7279\n",
            "Epoch 3/4 - Batch 21950/25000 - Loss: 1.4999\n",
            "Epoch 3/4 - Batch 22000/25000 - Loss: 0.8509\n",
            "Epoch 3/4 - Batch 22050/25000 - Loss: 1.7053\n",
            "Epoch 3/4 - Batch 22100/25000 - Loss: 1.8989\n",
            "Epoch 3/4 - Batch 22150/25000 - Loss: 2.2343\n",
            "Epoch 3/4 - Batch 22200/25000 - Loss: 1.9702\n",
            "Epoch 3/4 - Batch 22250/25000 - Loss: 2.2171\n",
            "Epoch 3/4 - Batch 22300/25000 - Loss: 2.0169\n",
            "Epoch 3/4 - Batch 22350/25000 - Loss: 1.5604\n",
            "Epoch 3/4 - Batch 22400/25000 - Loss: 1.7229\n",
            "Epoch 3/4 - Batch 22450/25000 - Loss: 1.7778\n",
            "Epoch 3/4 - Batch 22500/25000 - Loss: 1.0366\n",
            "Epoch 3/4 - Batch 22550/25000 - Loss: 1.9268\n",
            "Epoch 3/4 - Batch 22600/25000 - Loss: 1.4271\n",
            "Epoch 3/4 - Batch 22650/25000 - Loss: 1.5698\n",
            "Epoch 3/4 - Batch 22700/25000 - Loss: 1.5395\n",
            "Epoch 3/4 - Batch 22750/25000 - Loss: 1.9061\n",
            "Epoch 3/4 - Batch 22800/25000 - Loss: 1.2944\n",
            "Epoch 3/4 - Batch 22850/25000 - Loss: 1.5636\n",
            "Epoch 3/4 - Batch 22900/25000 - Loss: 1.8135\n",
            "Epoch 3/4 - Batch 22950/25000 - Loss: 1.0419\n",
            "Epoch 3/4 - Batch 23000/25000 - Loss: 1.4942\n",
            "Epoch 3/4 - Batch 23050/25000 - Loss: 1.8264\n",
            "Epoch 3/4 - Batch 23100/25000 - Loss: 1.4649\n",
            "Epoch 3/4 - Batch 23150/25000 - Loss: 1.7215\n",
            "Epoch 3/4 - Batch 23200/25000 - Loss: 1.5989\n",
            "Epoch 3/4 - Batch 23250/25000 - Loss: 1.5875\n",
            "Epoch 3/4 - Batch 23300/25000 - Loss: 2.0705\n",
            "Epoch 3/4 - Batch 23350/25000 - Loss: 1.5129\n",
            "Epoch 3/4 - Batch 23400/25000 - Loss: 1.6355\n",
            "Epoch 3/4 - Batch 23450/25000 - Loss: 1.7211\n",
            "Epoch 3/4 - Batch 23500/25000 - Loss: 1.5122\n",
            "Epoch 3/4 - Batch 23550/25000 - Loss: 1.7036\n",
            "Epoch 3/4 - Batch 23600/25000 - Loss: 2.1127\n",
            "Epoch 3/4 - Batch 23650/25000 - Loss: 1.5837\n",
            "Epoch 3/4 - Batch 23700/25000 - Loss: 1.4652\n",
            "Epoch 3/4 - Batch 23750/25000 - Loss: 1.3360\n",
            "Epoch 3/4 - Batch 23800/25000 - Loss: 1.8811\n",
            "Epoch 3/4 - Batch 23850/25000 - Loss: 1.6654\n",
            "Epoch 3/4 - Batch 23900/25000 - Loss: 1.2434\n",
            "Epoch 3/4 - Batch 23950/25000 - Loss: 2.1148\n",
            "Epoch 3/4 - Batch 24000/25000 - Loss: 1.4536\n",
            "Epoch 3/4 - Batch 24050/25000 - Loss: 1.5722\n",
            "Epoch 3/4 - Batch 24100/25000 - Loss: 1.6684\n",
            "Epoch 3/4 - Batch 24150/25000 - Loss: 2.0291\n",
            "Epoch 3/4 - Batch 24200/25000 - Loss: 1.6906\n",
            "Epoch 3/4 - Batch 24250/25000 - Loss: 1.9795\n",
            "Epoch 3/4 - Batch 24300/25000 - Loss: 1.5502\n",
            "Epoch 3/4 - Batch 24350/25000 - Loss: 1.7060\n",
            "Epoch 3/4 - Batch 24400/25000 - Loss: 1.5909\n",
            "Epoch 3/4 - Batch 24450/25000 - Loss: 1.7653\n",
            "Epoch 3/4 - Batch 24500/25000 - Loss: 1.6446\n",
            "Epoch 3/4 - Batch 24550/25000 - Loss: 1.5969\n",
            "Epoch 3/4 - Batch 24600/25000 - Loss: 1.6895\n",
            "Epoch 3/4 - Batch 24650/25000 - Loss: 1.5455\n",
            "Epoch 3/4 - Batch 24700/25000 - Loss: 1.6219\n",
            "Epoch 3/4 - Batch 24750/25000 - Loss: 1.5393\n",
            "Epoch 3/4 - Batch 24800/25000 - Loss: 2.2166\n",
            "Epoch 3/4 - Batch 24850/25000 - Loss: 1.3824\n",
            "Epoch 3/4 - Batch 24900/25000 - Loss: 1.7998\n",
            "Epoch 3/4 - Batch 24950/25000 - Loss: 1.5171\n",
            "Epoch 3/4 - Average loss: 1.7098 - Duration: 15966.72s\n",
            "Achieved privacy budget: Îµ = 12.92\n",
            "Epoch 4/4 - Batch 0/25000 - Loss: 1.6641\n",
            "Epoch 4/4 - Batch 50/25000 - Loss: 1.3900\n",
            "Epoch 4/4 - Batch 100/25000 - Loss: 1.7212\n",
            "Epoch 4/4 - Batch 150/25000 - Loss: 1.5805\n",
            "Epoch 4/4 - Batch 200/25000 - Loss: 1.5332\n",
            "Epoch 4/4 - Batch 250/25000 - Loss: 1.1018\n",
            "Epoch 4/4 - Batch 300/25000 - Loss: 2.0882\n",
            "Epoch 4/4 - Batch 350/25000 - Loss: 1.3430\n",
            "Epoch 4/4 - Batch 400/25000 - Loss: 1.5374\n",
            "Epoch 4/4 - Batch 450/25000 - Loss: 1.7875\n",
            "Epoch 4/4 - Batch 500/25000 - Loss: 1.4060\n",
            "Epoch 4/4 - Batch 550/25000 - Loss: 1.0731\n",
            "Epoch 4/4 - Batch 600/25000 - Loss: 1.4415\n",
            "Epoch 4/4 - Batch 650/25000 - Loss: 1.2856\n",
            "Epoch 4/4 - Batch 700/25000 - Loss: 0.9284\n",
            "Epoch 4/4 - Batch 750/25000 - Loss: 2.0464\n",
            "Epoch 4/4 - Batch 800/25000 - Loss: 1.5937\n",
            "Epoch 4/4 - Batch 850/25000 - Loss: 2.1220\n",
            "Epoch 4/4 - Batch 900/25000 - Loss: 1.8487\n",
            "Epoch 4/4 - Batch 950/25000 - Loss: 1.5061\n",
            "Epoch 4/4 - Batch 1000/25000 - Loss: 1.7738\n",
            "Epoch 4/4 - Batch 1050/25000 - Loss: 1.5818\n",
            "Epoch 4/4 - Batch 1100/25000 - Loss: 1.8387\n",
            "Epoch 4/4 - Batch 1150/25000 - Loss: 2.0737\n",
            "Epoch 4/4 - Batch 1200/25000 - Loss: 1.3964\n",
            "Epoch 4/4 - Batch 1250/25000 - Loss: 1.5685\n",
            "Epoch 4/4 - Batch 1300/25000 - Loss: 1.3083\n",
            "Epoch 4/4 - Batch 1350/25000 - Loss: 1.6929\n",
            "Epoch 4/4 - Batch 1400/25000 - Loss: 1.0515\n",
            "Epoch 4/4 - Batch 1450/25000 - Loss: 1.8911\n",
            "Epoch 4/4 - Batch 1500/25000 - Loss: 1.1348\n",
            "Epoch 4/4 - Batch 1550/25000 - Loss: 1.9013\n",
            "Epoch 4/4 - Batch 1600/25000 - Loss: 1.8472\n",
            "Epoch 4/4 - Batch 1650/25000 - Loss: 1.9142\n",
            "Epoch 4/4 - Batch 1700/25000 - Loss: 2.2611\n",
            "Epoch 4/4 - Batch 1750/25000 - Loss: 2.0966\n",
            "Epoch 4/4 - Batch 1800/25000 - Loss: 1.6466\n",
            "Epoch 4/4 - Batch 1850/25000 - Loss: 1.6508\n",
            "Epoch 4/4 - Batch 1900/25000 - Loss: 1.6165\n",
            "Epoch 4/4 - Batch 1950/25000 - Loss: 1.2987\n",
            "Epoch 4/4 - Batch 2000/25000 - Loss: 1.9427\n",
            "Epoch 4/4 - Batch 2050/25000 - Loss: 1.6673\n",
            "Epoch 4/4 - Batch 2100/25000 - Loss: 1.6237\n",
            "Epoch 4/4 - Batch 2150/25000 - Loss: 1.8623\n",
            "Epoch 4/4 - Batch 2200/25000 - Loss: 1.9329\n",
            "Epoch 4/4 - Batch 2250/25000 - Loss: 1.7695\n",
            "Epoch 4/4 - Batch 2300/25000 - Loss: 1.5061\n",
            "Epoch 4/4 - Batch 2350/25000 - Loss: 1.6565\n",
            "Epoch 4/4 - Batch 2400/25000 - Loss: 1.5865\n",
            "Epoch 4/4 - Batch 2450/25000 - Loss: 1.8766\n",
            "Epoch 4/4 - Batch 2500/25000 - Loss: 1.7870\n",
            "Epoch 4/4 - Batch 2550/25000 - Loss: 1.9976\n",
            "Epoch 4/4 - Batch 2600/25000 - Loss: 1.6707\n",
            "Epoch 4/4 - Batch 2650/25000 - Loss: 2.0711\n",
            "Epoch 4/4 - Batch 2700/25000 - Loss: 1.7481\n",
            "Epoch 4/4 - Batch 2750/25000 - Loss: 2.0039\n",
            "Epoch 4/4 - Batch 2800/25000 - Loss: 1.8327\n",
            "Epoch 4/4 - Batch 2850/25000 - Loss: 2.1500\n",
            "Epoch 4/4 - Batch 2900/25000 - Loss: 1.7083\n",
            "Epoch 4/4 - Batch 2950/25000 - Loss: 1.7009\n",
            "Epoch 4/4 - Batch 3000/25000 - Loss: 2.2964\n",
            "Epoch 4/4 - Batch 3050/25000 - Loss: 1.5597\n",
            "Epoch 4/4 - Batch 3100/25000 - Loss: 2.2902\n",
            "Epoch 4/4 - Batch 3150/25000 - Loss: 2.0644\n",
            "Epoch 4/4 - Batch 3200/25000 - Loss: 1.5843\n",
            "Epoch 4/4 - Batch 3250/25000 - Loss: 1.6631\n",
            "Epoch 4/4 - Batch 3300/25000 - Loss: 1.5342\n",
            "Epoch 4/4 - Batch 3350/25000 - Loss: 1.7759\n",
            "Epoch 4/4 - Batch 3400/25000 - Loss: 1.8313\n",
            "Epoch 4/4 - Batch 3450/25000 - Loss: 1.6471\n",
            "Epoch 4/4 - Batch 3500/25000 - Loss: 1.7512\n",
            "Epoch 4/4 - Batch 3550/25000 - Loss: 1.7792\n",
            "Epoch 4/4 - Batch 3600/25000 - Loss: 1.8176\n",
            "Epoch 4/4 - Batch 3650/25000 - Loss: 1.7529\n",
            "Epoch 4/4 - Batch 3700/25000 - Loss: 2.2338\n",
            "Epoch 4/4 - Batch 3750/25000 - Loss: 1.6628\n",
            "Epoch 4/4 - Batch 3800/25000 - Loss: 1.5784\n",
            "Epoch 4/4 - Batch 3850/25000 - Loss: 1.8244\n",
            "Epoch 4/4 - Batch 3900/25000 - Loss: 1.4560\n",
            "Epoch 4/4 - Batch 3950/25000 - Loss: 1.6693\n",
            "Epoch 4/4 - Batch 4000/25000 - Loss: 1.9534\n",
            "Epoch 4/4 - Batch 4050/25000 - Loss: 1.8367\n",
            "Epoch 4/4 - Batch 4100/25000 - Loss: 1.9409\n",
            "Epoch 4/4 - Batch 4150/25000 - Loss: 1.4882\n",
            "Epoch 4/4 - Batch 4200/25000 - Loss: 1.8373\n",
            "Epoch 4/4 - Batch 4250/25000 - Loss: 1.7609\n",
            "Epoch 4/4 - Batch 4300/25000 - Loss: 1.0889\n",
            "Epoch 4/4 - Batch 4350/25000 - Loss: 1.5091\n",
            "Epoch 4/4 - Batch 4400/25000 - Loss: 1.9397\n",
            "Epoch 4/4 - Batch 4450/25000 - Loss: 1.8093\n",
            "Epoch 4/4 - Batch 4500/25000 - Loss: 1.9265\n",
            "Epoch 4/4 - Batch 4550/25000 - Loss: 1.4882\n",
            "Epoch 4/4 - Batch 4600/25000 - Loss: 1.6528\n",
            "Epoch 4/4 - Batch 4650/25000 - Loss: 1.4710\n",
            "Epoch 4/4 - Batch 4700/25000 - Loss: 1.5251\n",
            "Epoch 4/4 - Batch 4750/25000 - Loss: 1.8791\n",
            "Epoch 4/4 - Batch 4800/25000 - Loss: 1.7451\n",
            "Epoch 4/4 - Batch 4850/25000 - Loss: 1.5852\n",
            "Epoch 4/4 - Batch 4900/25000 - Loss: 1.0350\n",
            "Epoch 4/4 - Batch 4950/25000 - Loss: 1.9631\n",
            "Epoch 4/4 - Batch 5000/25000 - Loss: 1.9627\n",
            "Epoch 4/4 - Batch 5050/25000 - Loss: 2.1539\n",
            "Epoch 4/4 - Batch 5100/25000 - Loss: 1.8968\n",
            "Epoch 4/4 - Batch 5150/25000 - Loss: 1.5967\n",
            "Epoch 4/4 - Batch 5200/25000 - Loss: 1.7677\n",
            "Epoch 4/4 - Batch 5250/25000 - Loss: 1.5312\n",
            "Epoch 4/4 - Batch 5300/25000 - Loss: 2.0371\n",
            "Epoch 4/4 - Batch 5350/25000 - Loss: 1.2226\n",
            "Epoch 4/4 - Batch 5400/25000 - Loss: 1.6770\n",
            "Epoch 4/4 - Batch 5450/25000 - Loss: 1.7263\n",
            "Epoch 4/4 - Batch 5500/25000 - Loss: 1.7942\n",
            "Epoch 4/4 - Batch 5550/25000 - Loss: 1.7521\n",
            "Epoch 4/4 - Batch 5600/25000 - Loss: 1.6352\n",
            "Epoch 4/4 - Batch 5650/25000 - Loss: 2.1347\n",
            "Epoch 4/4 - Batch 5700/25000 - Loss: 1.8338\n",
            "Epoch 4/4 - Batch 5750/25000 - Loss: 1.7107\n",
            "Epoch 4/4 - Batch 5800/25000 - Loss: 1.6302\n",
            "Epoch 4/4 - Batch 5850/25000 - Loss: 1.8817\n",
            "Epoch 4/4 - Batch 5900/25000 - Loss: 1.6778\n",
            "Epoch 4/4 - Batch 5950/25000 - Loss: 1.9290\n",
            "Epoch 4/4 - Batch 6000/25000 - Loss: 1.4160\n",
            "Epoch 4/4 - Batch 6050/25000 - Loss: 1.5609\n",
            "Epoch 4/4 - Batch 6100/25000 - Loss: 1.8877\n",
            "Epoch 4/4 - Batch 6150/25000 - Loss: 1.7155\n",
            "Epoch 4/4 - Batch 6200/25000 - Loss: 1.4179\n",
            "Epoch 4/4 - Batch 6250/25000 - Loss: 1.8637\n",
            "Epoch 4/4 - Batch 6300/25000 - Loss: 1.3130\n",
            "Epoch 4/4 - Batch 6350/25000 - Loss: 2.1010\n",
            "Epoch 4/4 - Batch 6400/25000 - Loss: 1.9424\n",
            "Epoch 4/4 - Batch 6450/25000 - Loss: 1.4006\n",
            "Epoch 4/4 - Batch 6500/25000 - Loss: 2.0532\n",
            "Epoch 4/4 - Batch 6550/25000 - Loss: 1.0582\n",
            "Epoch 4/4 - Batch 6600/25000 - Loss: 1.6470\n",
            "Epoch 4/4 - Batch 6650/25000 - Loss: 1.9829\n",
            "Epoch 4/4 - Batch 6700/25000 - Loss: 1.8027\n",
            "Epoch 4/4 - Batch 6750/25000 - Loss: 1.8642\n",
            "Epoch 4/4 - Batch 6800/25000 - Loss: 1.2188\n",
            "Epoch 4/4 - Batch 6850/25000 - Loss: 1.0736\n",
            "Epoch 4/4 - Batch 6900/25000 - Loss: 1.5728\n",
            "Epoch 4/4 - Batch 6950/25000 - Loss: 1.6383\n",
            "Epoch 4/4 - Batch 7000/25000 - Loss: 1.8960\n",
            "Epoch 4/4 - Batch 7050/25000 - Loss: 1.3167\n",
            "Epoch 4/4 - Batch 7100/25000 - Loss: 1.7711\n",
            "Epoch 4/4 - Batch 7150/25000 - Loss: 2.0599\n",
            "Epoch 4/4 - Batch 7200/25000 - Loss: 1.8779\n",
            "Epoch 4/4 - Batch 7250/25000 - Loss: 1.9016\n",
            "Epoch 4/4 - Batch 7300/25000 - Loss: 1.8282\n",
            "Epoch 4/4 - Batch 7350/25000 - Loss: 1.0280\n",
            "Epoch 4/4 - Batch 7400/25000 - Loss: 1.3951\n",
            "Epoch 4/4 - Batch 7450/25000 - Loss: 1.2450\n",
            "Epoch 4/4 - Batch 7500/25000 - Loss: 1.7928\n",
            "Epoch 4/4 - Batch 7550/25000 - Loss: 2.3139\n",
            "Epoch 4/4 - Batch 7600/25000 - Loss: 1.2511\n",
            "Epoch 4/4 - Batch 7650/25000 - Loss: 1.9298\n",
            "Epoch 4/4 - Batch 7700/25000 - Loss: 2.0352\n",
            "Epoch 4/4 - Batch 7750/25000 - Loss: 1.8012\n",
            "Epoch 4/4 - Batch 7800/25000 - Loss: 1.5529\n",
            "Epoch 4/4 - Batch 7850/25000 - Loss: 1.7473\n",
            "Epoch 4/4 - Batch 7900/25000 - Loss: 1.6912\n",
            "Epoch 4/4 - Batch 7950/25000 - Loss: 1.8868\n",
            "Epoch 4/4 - Batch 8000/25000 - Loss: 1.3801\n",
            "Epoch 4/4 - Batch 8050/25000 - Loss: 1.5173\n",
            "Epoch 4/4 - Batch 8100/25000 - Loss: 1.8111\n",
            "Epoch 4/4 - Batch 8150/25000 - Loss: 1.6146\n",
            "Epoch 4/4 - Batch 8200/25000 - Loss: 1.5504\n",
            "Epoch 4/4 - Batch 8250/25000 - Loss: 1.9000\n",
            "Epoch 4/4 - Batch 8300/25000 - Loss: 1.4931\n",
            "Epoch 4/4 - Batch 8350/25000 - Loss: 1.9541\n",
            "Epoch 4/4 - Batch 8400/25000 - Loss: 1.9556\n",
            "Epoch 4/4 - Batch 8450/25000 - Loss: 1.3006\n",
            "Epoch 4/4 - Batch 8500/25000 - Loss: 1.6496\n",
            "Epoch 4/4 - Batch 8550/25000 - Loss: 1.5268\n",
            "Epoch 4/4 - Batch 8600/25000 - Loss: 1.8230\n",
            "Epoch 4/4 - Batch 8650/25000 - Loss: 1.8204\n",
            "Epoch 4/4 - Batch 8700/25000 - Loss: 1.9450\n",
            "Epoch 4/4 - Batch 8750/25000 - Loss: 1.4244\n",
            "Epoch 4/4 - Batch 8800/25000 - Loss: 1.8864\n",
            "Epoch 4/4 - Batch 8850/25000 - Loss: 2.1306\n",
            "Epoch 4/4 - Batch 8900/25000 - Loss: 1.5476\n",
            "Epoch 4/4 - Batch 8950/25000 - Loss: 2.0287\n",
            "Epoch 4/4 - Batch 9000/25000 - Loss: 2.0192\n",
            "Epoch 4/4 - Batch 9050/25000 - Loss: 1.9370\n",
            "Epoch 4/4 - Batch 9100/25000 - Loss: 1.6870\n",
            "Epoch 4/4 - Batch 9150/25000 - Loss: 1.9247\n",
            "Epoch 4/4 - Batch 9200/25000 - Loss: 1.5741\n",
            "Epoch 4/4 - Batch 9250/25000 - Loss: 1.3641\n",
            "Epoch 4/4 - Batch 9300/25000 - Loss: 1.5780\n",
            "Epoch 4/4 - Batch 9350/25000 - Loss: 1.7387\n",
            "Epoch 4/4 - Batch 9400/25000 - Loss: 1.0229\n",
            "Epoch 4/4 - Batch 9450/25000 - Loss: 1.7054\n",
            "Epoch 4/4 - Batch 9500/25000 - Loss: 1.8439\n",
            "Epoch 4/4 - Batch 9550/25000 - Loss: 1.6264\n",
            "Epoch 4/4 - Batch 9600/25000 - Loss: 1.8562\n",
            "Epoch 4/4 - Batch 9650/25000 - Loss: 1.5472\n",
            "Epoch 4/4 - Batch 9700/25000 - Loss: 1.7229\n",
            "Epoch 4/4 - Batch 9750/25000 - Loss: 1.7898\n",
            "Epoch 4/4 - Batch 9800/25000 - Loss: 1.9425\n",
            "Epoch 4/4 - Batch 9850/25000 - Loss: 1.6857\n",
            "Epoch 4/4 - Batch 9900/25000 - Loss: 1.1730\n",
            "Epoch 4/4 - Batch 9950/25000 - Loss: 1.1942\n",
            "Epoch 4/4 - Batch 10000/25000 - Loss: 1.4182\n",
            "Epoch 4/4 - Batch 10050/25000 - Loss: 1.2587\n",
            "Epoch 4/4 - Batch 10100/25000 - Loss: 1.7319\n",
            "Epoch 4/4 - Batch 10150/25000 - Loss: 1.7097\n",
            "Epoch 4/4 - Batch 10200/25000 - Loss: 1.5263\n",
            "Epoch 4/4 - Batch 10250/25000 - Loss: 1.5124\n",
            "Epoch 4/4 - Batch 10300/25000 - Loss: 1.5010\n",
            "Epoch 4/4 - Batch 10350/25000 - Loss: 1.3886\n",
            "Epoch 4/4 - Batch 10400/25000 - Loss: 1.7579\n",
            "Epoch 4/4 - Batch 10450/25000 - Loss: 1.5471\n",
            "Epoch 4/4 - Batch 10500/25000 - Loss: 1.4867\n",
            "Epoch 4/4 - Batch 10550/25000 - Loss: 1.1702\n",
            "Epoch 4/4 - Batch 10600/25000 - Loss: 1.8570\n",
            "Epoch 4/4 - Batch 10650/25000 - Loss: 1.8925\n",
            "Epoch 4/4 - Batch 10700/25000 - Loss: 1.8914\n",
            "Epoch 4/4 - Batch 10750/25000 - Loss: 1.6174\n",
            "Epoch 4/4 - Batch 10800/25000 - Loss: 1.8562\n",
            "Epoch 4/4 - Batch 10850/25000 - Loss: 1.6502\n",
            "Epoch 4/4 - Batch 10900/25000 - Loss: 1.2032\n",
            "Epoch 4/4 - Batch 10950/25000 - Loss: 1.8620\n",
            "Epoch 4/4 - Batch 11000/25000 - Loss: 1.5338\n",
            "Epoch 4/4 - Batch 11050/25000 - Loss: 1.5225\n",
            "Epoch 4/4 - Batch 11100/25000 - Loss: 1.8643\n",
            "Epoch 4/4 - Batch 11150/25000 - Loss: 1.4762\n",
            "Epoch 4/4 - Batch 11200/25000 - Loss: 1.7931\n",
            "Epoch 4/4 - Batch 11250/25000 - Loss: 1.6573\n",
            "Epoch 4/4 - Batch 11300/25000 - Loss: 1.7408\n",
            "Epoch 4/4 - Batch 11350/25000 - Loss: 1.7979\n",
            "Epoch 4/4 - Batch 11400/25000 - Loss: 1.2139\n",
            "Epoch 4/4 - Batch 11450/25000 - Loss: 1.8412\n",
            "Epoch 4/4 - Batch 11500/25000 - Loss: 1.8490\n",
            "Epoch 4/4 - Batch 11550/25000 - Loss: 1.9149\n",
            "Epoch 4/4 - Batch 11600/25000 - Loss: 1.7828\n",
            "Epoch 4/4 - Batch 11650/25000 - Loss: 1.6605\n",
            "Epoch 4/4 - Batch 11700/25000 - Loss: 1.8276\n",
            "Epoch 4/4 - Batch 11750/25000 - Loss: 1.3211\n",
            "Epoch 4/4 - Batch 11800/25000 - Loss: 1.3036\n",
            "Epoch 4/4 - Batch 11850/25000 - Loss: 1.5415\n",
            "Epoch 4/4 - Batch 11900/25000 - Loss: 1.4924\n",
            "Epoch 4/4 - Batch 11950/25000 - Loss: 1.9320\n",
            "Epoch 4/4 - Batch 12000/25000 - Loss: 1.8673\n",
            "Epoch 4/4 - Batch 12050/25000 - Loss: 1.8763\n",
            "Epoch 4/4 - Batch 12100/25000 - Loss: 2.0517\n",
            "Epoch 4/4 - Batch 12150/25000 - Loss: 1.4367\n",
            "Epoch 4/4 - Batch 12200/25000 - Loss: 1.0340\n",
            "Epoch 4/4 - Batch 12250/25000 - Loss: 1.8864\n",
            "Epoch 4/4 - Batch 12300/25000 - Loss: 1.8269\n",
            "Epoch 4/4 - Batch 12350/25000 - Loss: 1.8403\n",
            "Epoch 4/4 - Batch 12400/25000 - Loss: 1.1454\n",
            "Epoch 4/4 - Batch 12450/25000 - Loss: 2.0681\n",
            "Epoch 4/4 - Batch 12500/25000 - Loss: 1.2945\n",
            "Epoch 4/4 - Batch 12550/25000 - Loss: 1.8424\n",
            "Epoch 4/4 - Batch 12600/25000 - Loss: 1.2667\n",
            "Epoch 4/4 - Batch 12650/25000 - Loss: 2.4732\n",
            "Epoch 4/4 - Batch 12700/25000 - Loss: 2.0107\n",
            "Epoch 4/4 - Batch 12750/25000 - Loss: 1.8839\n",
            "Epoch 4/4 - Batch 12800/25000 - Loss: 1.5577\n",
            "Epoch 4/4 - Batch 12850/25000 - Loss: 1.1891\n",
            "Epoch 4/4 - Batch 12900/25000 - Loss: 1.7904\n",
            "Epoch 4/4 - Batch 12950/25000 - Loss: 1.7296\n",
            "Epoch 4/4 - Batch 13000/25000 - Loss: 1.8686\n",
            "Epoch 4/4 - Batch 13050/25000 - Loss: 1.8464\n",
            "Epoch 4/4 - Batch 13100/25000 - Loss: 1.4933\n",
            "Epoch 4/4 - Batch 13150/25000 - Loss: 1.2079\n",
            "Epoch 4/4 - Batch 13200/25000 - Loss: 1.2552\n",
            "Epoch 4/4 - Batch 13250/25000 - Loss: 1.5505\n",
            "Epoch 4/4 - Batch 13300/25000 - Loss: 1.5870\n",
            "Epoch 4/4 - Batch 13350/25000 - Loss: 2.4678\n",
            "Epoch 4/4 - Batch 13400/25000 - Loss: 1.6961\n",
            "Epoch 4/4 - Batch 13450/25000 - Loss: 1.5093\n",
            "Epoch 4/4 - Batch 13500/25000 - Loss: 1.6469\n",
            "Epoch 4/4 - Batch 13550/25000 - Loss: 1.9412\n",
            "Epoch 4/4 - Batch 13600/25000 - Loss: 1.7930\n",
            "Epoch 4/4 - Batch 13650/25000 - Loss: 2.0968\n",
            "Epoch 4/4 - Batch 13700/25000 - Loss: 1.1738\n",
            "Epoch 4/4 - Batch 13750/25000 - Loss: 1.6555\n",
            "Epoch 4/4 - Batch 13800/25000 - Loss: 1.4521\n",
            "Epoch 4/4 - Batch 13850/25000 - Loss: 1.6334\n",
            "Epoch 4/4 - Batch 13900/25000 - Loss: 1.7597\n",
            "Epoch 4/4 - Batch 13950/25000 - Loss: 2.0513\n",
            "Epoch 4/4 - Batch 14000/25000 - Loss: 2.3019\n",
            "Epoch 4/4 - Batch 14050/25000 - Loss: 1.5467\n",
            "Epoch 4/4 - Batch 14100/25000 - Loss: 1.7551\n",
            "Epoch 4/4 - Batch 14150/25000 - Loss: 1.9073\n",
            "Epoch 4/4 - Batch 14200/25000 - Loss: 1.5883\n",
            "Epoch 4/4 - Batch 14250/25000 - Loss: 1.5284\n",
            "Epoch 4/4 - Batch 14300/25000 - Loss: 1.5092\n",
            "Epoch 4/4 - Batch 14350/25000 - Loss: 1.9216\n",
            "Epoch 4/4 - Batch 14400/25000 - Loss: 1.4809\n",
            "Epoch 4/4 - Batch 14450/25000 - Loss: 1.7517\n",
            "Epoch 4/4 - Batch 14500/25000 - Loss: 1.6844\n",
            "Epoch 4/4 - Batch 14550/25000 - Loss: 1.3160\n",
            "Epoch 4/4 - Batch 14600/25000 - Loss: 1.6327\n",
            "Epoch 4/4 - Batch 14650/25000 - Loss: 1.3102\n",
            "Epoch 4/4 - Batch 14700/25000 - Loss: 1.3679\n",
            "Epoch 4/4 - Batch 14750/25000 - Loss: 1.6620\n",
            "Epoch 4/4 - Batch 14800/25000 - Loss: 1.4075\n",
            "Epoch 4/4 - Batch 14850/25000 - Loss: 1.9681\n",
            "Epoch 4/4 - Batch 14900/25000 - Loss: 1.8981\n",
            "Epoch 4/4 - Batch 14950/25000 - Loss: 1.3706\n",
            "Epoch 4/4 - Batch 15000/25000 - Loss: 1.4754\n",
            "Epoch 4/4 - Batch 15050/25000 - Loss: 1.6109\n",
            "Epoch 4/4 - Batch 15100/25000 - Loss: 1.8045\n",
            "Epoch 4/4 - Batch 15150/25000 - Loss: 2.0215\n",
            "Epoch 4/4 - Batch 15200/25000 - Loss: 1.1385\n",
            "Epoch 4/4 - Batch 15250/25000 - Loss: 1.7871\n",
            "Epoch 4/4 - Batch 15300/25000 - Loss: 1.7478\n",
            "Epoch 4/4 - Batch 15350/25000 - Loss: 1.6224\n",
            "Epoch 4/4 - Batch 15400/25000 - Loss: 1.7569\n",
            "Epoch 4/4 - Batch 15450/25000 - Loss: 1.9649\n",
            "Epoch 4/4 - Batch 15500/25000 - Loss: 1.6273\n",
            "Epoch 4/4 - Batch 15550/25000 - Loss: 2.0908\n",
            "Epoch 4/4 - Batch 15600/25000 - Loss: 1.1271\n",
            "Epoch 4/4 - Batch 15650/25000 - Loss: 2.1933\n",
            "Epoch 4/4 - Batch 15700/25000 - Loss: 1.3018\n",
            "Epoch 4/4 - Batch 15750/25000 - Loss: 1.3185\n",
            "Epoch 4/4 - Batch 15800/25000 - Loss: 1.8958\n",
            "Epoch 4/4 - Batch 15850/25000 - Loss: 2.1002\n",
            "Epoch 4/4 - Batch 15900/25000 - Loss: 1.5132\n",
            "Epoch 4/4 - Batch 15950/25000 - Loss: 1.6810\n",
            "Epoch 4/4 - Batch 16000/25000 - Loss: 1.9169\n",
            "Epoch 4/4 - Batch 16050/25000 - Loss: 1.5494\n",
            "Epoch 4/4 - Batch 16100/25000 - Loss: 1.4969\n",
            "Epoch 4/4 - Batch 16150/25000 - Loss: 1.7187\n",
            "Epoch 4/4 - Batch 16200/25000 - Loss: 1.9470\n",
            "Epoch 4/4 - Batch 16250/25000 - Loss: 1.5313\n",
            "Epoch 4/4 - Batch 16300/25000 - Loss: 1.3722\n",
            "Epoch 4/4 - Batch 16350/25000 - Loss: 1.4296\n",
            "Epoch 4/4 - Batch 16400/25000 - Loss: 1.8871\n",
            "Epoch 4/4 - Batch 16450/25000 - Loss: 1.8571\n",
            "Epoch 4/4 - Batch 16500/25000 - Loss: 1.6537\n",
            "Epoch 4/4 - Batch 16550/25000 - Loss: 1.3218\n",
            "Epoch 4/4 - Batch 16600/25000 - Loss: 1.7135\n",
            "Epoch 4/4 - Batch 16650/25000 - Loss: 1.2730\n",
            "Epoch 4/4 - Batch 16700/25000 - Loss: 1.7953\n",
            "Epoch 4/4 - Batch 16750/25000 - Loss: 1.4795\n",
            "Epoch 4/4 - Batch 16800/25000 - Loss: 1.8891\n",
            "Epoch 4/4 - Batch 16850/25000 - Loss: 1.9989\n",
            "Epoch 4/4 - Batch 16900/25000 - Loss: 1.5393\n",
            "Epoch 4/4 - Batch 16950/25000 - Loss: 1.7011\n",
            "Epoch 4/4 - Batch 17000/25000 - Loss: 1.4879\n",
            "Epoch 4/4 - Batch 17050/25000 - Loss: 1.6532\n",
            "Epoch 4/4 - Batch 17100/25000 - Loss: 1.4602\n",
            "Epoch 4/4 - Batch 17150/25000 - Loss: 1.9656\n",
            "Epoch 4/4 - Batch 17200/25000 - Loss: 1.6829\n",
            "Epoch 4/4 - Batch 17250/25000 - Loss: 1.8840\n",
            "Epoch 4/4 - Batch 17300/25000 - Loss: 1.6880\n",
            "Epoch 4/4 - Batch 17350/25000 - Loss: 1.2956\n",
            "Epoch 4/4 - Batch 17400/25000 - Loss: 1.1783\n",
            "Epoch 4/4 - Batch 17450/25000 - Loss: 1.2570\n",
            "Epoch 4/4 - Batch 17500/25000 - Loss: 1.7402\n",
            "Epoch 4/4 - Batch 17550/25000 - Loss: 1.4723\n",
            "Epoch 4/4 - Batch 17600/25000 - Loss: 1.8405\n",
            "Epoch 4/4 - Batch 17650/25000 - Loss: 1.9414\n",
            "Epoch 4/4 - Batch 17700/25000 - Loss: 1.5433\n",
            "Epoch 4/4 - Batch 17750/25000 - Loss: 1.4330\n",
            "Epoch 4/4 - Batch 17800/25000 - Loss: 1.6105\n",
            "Epoch 4/4 - Batch 17850/25000 - Loss: 1.7493\n",
            "Epoch 4/4 - Batch 17900/25000 - Loss: 1.0296\n",
            "Epoch 4/4 - Batch 17950/25000 - Loss: 1.5392\n",
            "Epoch 4/4 - Batch 18000/25000 - Loss: 1.6136\n",
            "Epoch 4/4 - Batch 18050/25000 - Loss: 1.8967\n",
            "Epoch 4/4 - Batch 18100/25000 - Loss: 1.6143\n",
            "Epoch 4/4 - Batch 18150/25000 - Loss: 1.4406\n",
            "Epoch 4/4 - Batch 18200/25000 - Loss: 1.8540\n",
            "Epoch 4/4 - Batch 18250/25000 - Loss: 1.3821\n",
            "Epoch 4/4 - Batch 18300/25000 - Loss: 2.0832\n",
            "Epoch 4/4 - Batch 18350/25000 - Loss: 1.4944\n",
            "Epoch 4/4 - Batch 18400/25000 - Loss: 1.4069\n",
            "Epoch 4/4 - Batch 18450/25000 - Loss: 1.7084\n",
            "Epoch 4/4 - Batch 18500/25000 - Loss: 1.7897\n",
            "Epoch 4/4 - Batch 18550/25000 - Loss: 1.6338\n",
            "Epoch 4/4 - Batch 18600/25000 - Loss: 1.3751\n",
            "Epoch 4/4 - Batch 18650/25000 - Loss: 1.8922\n",
            "Epoch 4/4 - Batch 18700/25000 - Loss: 1.5953\n",
            "Epoch 4/4 - Batch 18750/25000 - Loss: 1.8208\n",
            "Epoch 4/4 - Batch 18800/25000 - Loss: 1.6808\n",
            "Epoch 4/4 - Batch 18850/25000 - Loss: 1.2882\n",
            "Epoch 4/4 - Batch 18900/25000 - Loss: 1.3753\n",
            "Epoch 4/4 - Batch 18950/25000 - Loss: 1.5263\n",
            "Epoch 4/4 - Batch 19000/25000 - Loss: 2.1142\n",
            "Epoch 4/4 - Batch 19050/25000 - Loss: 1.6672\n",
            "Epoch 4/4 - Batch 19100/25000 - Loss: 1.8309\n",
            "Epoch 4/4 - Batch 19150/25000 - Loss: 1.3352\n",
            "Epoch 4/4 - Batch 19200/25000 - Loss: 1.8762\n",
            "Epoch 4/4 - Batch 19250/25000 - Loss: 1.3779\n",
            "Epoch 4/4 - Batch 19300/25000 - Loss: 1.5051\n",
            "Epoch 4/4 - Batch 19350/25000 - Loss: 1.8510\n",
            "Epoch 4/4 - Batch 19400/25000 - Loss: 1.5656\n",
            "Epoch 4/4 - Batch 19450/25000 - Loss: 1.0839\n",
            "Epoch 4/4 - Batch 19500/25000 - Loss: 1.9041\n",
            "Epoch 4/4 - Batch 19550/25000 - Loss: 1.3869\n",
            "Epoch 4/4 - Batch 19600/25000 - Loss: 1.1730\n",
            "Epoch 4/4 - Batch 19650/25000 - Loss: 1.7579\n",
            "Epoch 4/4 - Batch 19700/25000 - Loss: 1.4291\n",
            "Epoch 4/4 - Batch 19750/25000 - Loss: 2.0994\n",
            "Epoch 4/4 - Batch 19800/25000 - Loss: 1.7603\n",
            "Epoch 4/4 - Batch 19850/25000 - Loss: 1.8657\n",
            "Epoch 4/4 - Batch 19900/25000 - Loss: 1.4687\n",
            "Epoch 4/4 - Batch 19950/25000 - Loss: 2.3517\n",
            "Epoch 4/4 - Batch 20000/25000 - Loss: 1.7220\n",
            "Epoch 4/4 - Batch 20050/25000 - Loss: 1.5030\n",
            "Epoch 4/4 - Batch 20100/25000 - Loss: 1.7739\n",
            "Epoch 4/4 - Batch 20150/25000 - Loss: 1.5442\n",
            "Epoch 4/4 - Batch 20200/25000 - Loss: 1.9180\n",
            "Epoch 4/4 - Batch 20250/25000 - Loss: 1.6357\n",
            "Epoch 4/4 - Batch 20300/25000 - Loss: 1.8693\n",
            "Epoch 4/4 - Batch 20350/25000 - Loss: 1.8264\n",
            "Epoch 4/4 - Batch 20400/25000 - Loss: 1.5060\n",
            "Epoch 4/4 - Batch 20450/25000 - Loss: 1.8196\n",
            "Epoch 4/4 - Batch 20500/25000 - Loss: 1.8903\n",
            "Epoch 4/4 - Batch 20550/25000 - Loss: 1.5987\n",
            "Epoch 4/4 - Batch 20600/25000 - Loss: 0.9235\n",
            "Epoch 4/4 - Batch 20650/25000 - Loss: 1.8404\n",
            "Epoch 4/4 - Batch 20700/25000 - Loss: 1.6230\n",
            "Epoch 4/4 - Batch 20750/25000 - Loss: 1.8344\n",
            "Epoch 4/4 - Batch 20800/25000 - Loss: 1.3272\n",
            "Epoch 4/4 - Batch 20850/25000 - Loss: 1.7713\n",
            "Epoch 4/4 - Batch 20900/25000 - Loss: 1.4888\n",
            "Epoch 4/4 - Batch 20950/25000 - Loss: 1.4903\n",
            "Epoch 4/4 - Batch 21000/25000 - Loss: 1.4018\n",
            "Epoch 4/4 - Batch 21050/25000 - Loss: 1.1955\n",
            "Epoch 4/4 - Batch 21100/25000 - Loss: 1.6089\n",
            "Epoch 4/4 - Batch 21150/25000 - Loss: 1.6691\n",
            "Epoch 4/4 - Batch 21200/25000 - Loss: 1.8176\n",
            "Epoch 4/4 - Batch 21250/25000 - Loss: 1.8014\n",
            "Epoch 4/4 - Batch 21300/25000 - Loss: 1.6016\n",
            "Epoch 4/4 - Batch 21350/25000 - Loss: 1.6980\n",
            "Epoch 4/4 - Batch 21400/25000 - Loss: 1.9400\n",
            "Epoch 4/4 - Batch 21450/25000 - Loss: 1.7166\n",
            "Epoch 4/4 - Batch 21500/25000 - Loss: 1.5267\n",
            "Epoch 4/4 - Batch 21550/25000 - Loss: 1.6104\n",
            "Epoch 4/4 - Batch 21600/25000 - Loss: 1.1186\n",
            "Epoch 4/4 - Batch 21650/25000 - Loss: 0.9986\n",
            "Epoch 4/4 - Batch 21700/25000 - Loss: 0.9486\n",
            "Epoch 4/4 - Batch 21750/25000 - Loss: 2.1595\n",
            "Epoch 4/4 - Batch 21800/25000 - Loss: 1.9180\n",
            "Epoch 4/4 - Batch 21850/25000 - Loss: 2.0033\n",
            "Epoch 4/4 - Batch 21900/25000 - Loss: 1.5125\n",
            "Epoch 4/4 - Batch 21950/25000 - Loss: 1.7377\n",
            "Epoch 4/4 - Batch 22000/25000 - Loss: 1.9545\n",
            "Epoch 4/4 - Batch 22050/25000 - Loss: 1.9646\n",
            "Epoch 4/4 - Batch 22100/25000 - Loss: 1.4656\n",
            "Epoch 4/4 - Batch 22150/25000 - Loss: 1.7285\n",
            "Epoch 4/4 - Batch 22200/25000 - Loss: 1.4014\n",
            "Epoch 4/4 - Batch 22250/25000 - Loss: 1.9745\n",
            "Epoch 4/4 - Batch 22300/25000 - Loss: 1.9759\n",
            "Epoch 4/4 - Batch 22350/25000 - Loss: 2.1021\n",
            "Epoch 4/4 - Batch 22400/25000 - Loss: 1.6990\n",
            "Epoch 4/4 - Batch 22450/25000 - Loss: 1.7973\n",
            "Epoch 4/4 - Batch 22500/25000 - Loss: 1.4267\n",
            "Epoch 4/4 - Batch 22550/25000 - Loss: 1.9318\n",
            "Epoch 4/4 - Batch 22600/25000 - Loss: 1.7076\n",
            "Epoch 4/4 - Batch 22650/25000 - Loss: 1.7841\n",
            "Epoch 4/4 - Batch 22700/25000 - Loss: 1.4783\n",
            "Epoch 4/4 - Batch 22750/25000 - Loss: 1.8866\n",
            "Epoch 4/4 - Batch 22800/25000 - Loss: 1.5571\n",
            "Epoch 4/4 - Batch 22850/25000 - Loss: 1.7540\n",
            "Epoch 4/4 - Batch 22900/25000 - Loss: 1.7069\n",
            "Epoch 4/4 - Batch 22950/25000 - Loss: 1.3778\n",
            "Epoch 4/4 - Batch 23000/25000 - Loss: 1.1949\n",
            "Epoch 4/4 - Batch 23050/25000 - Loss: 1.3092\n",
            "Epoch 4/4 - Batch 23100/25000 - Loss: 1.8281\n",
            "Epoch 4/4 - Batch 23150/25000 - Loss: 1.4742\n",
            "Epoch 4/4 - Batch 23200/25000 - Loss: 1.5470\n",
            "Epoch 4/4 - Batch 23250/25000 - Loss: 1.4243\n",
            "Epoch 4/4 - Batch 23300/25000 - Loss: 1.6374\n",
            "Epoch 4/4 - Batch 23350/25000 - Loss: 1.8594\n",
            "Epoch 4/4 - Batch 23400/25000 - Loss: 1.3352\n",
            "Epoch 4/4 - Batch 23450/25000 - Loss: 1.6814\n",
            "Epoch 4/4 - Batch 23500/25000 - Loss: 2.0894\n",
            "Epoch 4/4 - Batch 23550/25000 - Loss: 1.8269\n",
            "Epoch 4/4 - Batch 23600/25000 - Loss: 1.9114\n",
            "Epoch 4/4 - Batch 23650/25000 - Loss: 1.3544\n",
            "Epoch 4/4 - Batch 23700/25000 - Loss: 1.5461\n",
            "Epoch 4/4 - Batch 23750/25000 - Loss: 1.5247\n",
            "Epoch 4/4 - Batch 23800/25000 - Loss: 1.5698\n",
            "Epoch 4/4 - Batch 23850/25000 - Loss: 2.0431\n",
            "Epoch 4/4 - Batch 23900/25000 - Loss: 1.4321\n",
            "Epoch 4/4 - Batch 23950/25000 - Loss: 1.7332\n",
            "Epoch 4/4 - Batch 24000/25000 - Loss: 1.8458\n",
            "Epoch 4/4 - Batch 24050/25000 - Loss: 1.8813\n",
            "Epoch 4/4 - Batch 24100/25000 - Loss: 1.8084\n",
            "Epoch 4/4 - Batch 24150/25000 - Loss: 2.0575\n",
            "Epoch 4/4 - Batch 24200/25000 - Loss: 1.6561\n",
            "Epoch 4/4 - Batch 24250/25000 - Loss: 1.3625\n",
            "Epoch 4/4 - Batch 24300/25000 - Loss: 1.1523\n",
            "Epoch 4/4 - Batch 24350/25000 - Loss: 1.7086\n",
            "Epoch 4/4 - Batch 24400/25000 - Loss: 1.7360\n",
            "Epoch 4/4 - Batch 24450/25000 - Loss: 1.9886\n",
            "Epoch 4/4 - Batch 24500/25000 - Loss: 2.0347\n",
            "Epoch 4/4 - Batch 24550/25000 - Loss: 1.5590\n",
            "Epoch 4/4 - Batch 24600/25000 - Loss: 1.7207\n",
            "Epoch 4/4 - Batch 24650/25000 - Loss: 1.8035\n",
            "Epoch 4/4 - Batch 24700/25000 - Loss: 1.5880\n",
            "Epoch 4/4 - Batch 24750/25000 - Loss: 1.6794\n",
            "Epoch 4/4 - Batch 24800/25000 - Loss: 1.3373\n",
            "Epoch 4/4 - Batch 24850/25000 - Loss: 1.7068\n",
            "Epoch 4/4 - Batch 24900/25000 - Loss: 1.5625\n",
            "Epoch 4/4 - Batch 24950/25000 - Loss: 1.6769\n",
            "Epoch 4/4 - Average loss: 1.6760 - Duration: 15973.87s\n",
            "Achieved privacy budget: Îµ = 13.93\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    batch_times = []\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        batch_start = datetime.datetime.now()\n",
        "\n",
        "\n",
        "        # Move each element of the batch to the device.\n",
        "        # input_ids_batch, attention_mask_batch, labels_batch = [x.to(device) for x in batch]\n",
        "        input_ids_batch, attention_mask_batch, labels_batch = [x.to(device, non_blocking=True) for x in batch]\n",
        "\n",
        "        # Create a position_ids tensor: shape [batch_size, seq_len]\n",
        "        seq_len = input_ids_batch.size(1)\n",
        "        position_ids = torch.arange(seq_len, device=device).unsqueeze(0).repeat(input_ids_batch.size(0), 1)\n",
        "\n",
        "        # Forward pass: compute the loss.\n",
        "        outputs = model(\n",
        "            input_ids=input_ids_batch,\n",
        "            attention_mask=attention_mask_batch,\n",
        "            position_ids=position_ids,\n",
        "            labels=labels_batch\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        # total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        total_loss += loss.item()\n",
        "        batch_end = datetime.datetime.now()\n",
        "        batch_times.append((batch_end - batch_start).total_seconds())\n",
        "\n",
        "        # Print progress\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Batch {i}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Compute epoch metrics\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    end_time = datetime.datetime.now()\n",
        "    epoch_duration = (end_time - start_time).total_seconds()\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Average loss: {avg_loss:.4f} - Duration: {epoch_duration:.2f}s\")\n",
        "\n",
        "    # Record epoch metrics\n",
        "    tracker.record_epoch_metrics(\n",
        "        epoch=epoch+1,\n",
        "        loss=avg_loss,\n",
        "        batch_times=batch_times,\n",
        "        epoch_duration=epoch_duration,\n",
        "        timestamp=datetime.datetime.now().isoformat()\n",
        "    )\n",
        "\n",
        "    # Record privacy budget if using differential privacy\n",
        "    if hasattr(model, \"remove_hooks\"):\n",
        "        epsilon = privacy_engine.accountant.get_epsilon(delta=1e-5)\n",
        "        print(f\"Achieved privacy budget: Îµ = {epsilon:.2f}\")\n",
        "        tracker.record_privacy_budget(epsilon=epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Specify the directory where your model and tokenizer are saved\n",
        "save_directory = \".\"\n",
        "\n",
        "# Load the model and tokenizer from the saved directory\n",
        "model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "\n",
        "# Set up the device: use CUDA if available, otherwise fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the model to the selected device and set it to evaluation mode\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define a sample prompt\n",
        "sample_prompt = (\"System prompt: Given the Rating and Title, you are required to generate the review, \"\n",
        "                 \"Rating: 5, Title: Would definitely buy again, Review:\")\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(sample_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Generate text using the model\n",
        "generated_ids = model.generate(**inputs, max_length=128, do_sample=True, top_k=50)\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated text:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629,
          "referenced_widgets": [
            "fb986c45c98f4f24ad47c6cb6c4b28ac",
            "29dee63c8e7f445cbdfcc6365debdd1b",
            "729f49ea83414ed08fff6968b3aaa617",
            "04687cd0ed9e46789e22e749955bdeef",
            "0fddcbb8d61841beb2d7255b8de59336",
            "a6e784375ad1480f8a8a1a19f7a3f7ef",
            "0f6322e33ff64d6ab58880fef256993b",
            "88101fd544154b4db7b4107d4faab8cf",
            "2e0e7f9688de42dda309b63cf2573d63",
            "91d67aa1354646128302de0851c7904c",
            "26b13df091e341328f3e0b32a9506516"
          ]
        },
        "id": "DBIxXVt2E85I",
        "outputId": "2d908087-f7e6-45f1-d6e1-cadb8a28bf4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb986c45c98f4f24ad47c6cb6c4b28ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 22.17 GiB of which 22.88 MiB is free. Process 232487 has 22.14 GiB memory in use. Of the allocated memory 20.93 GiB is allocated by PyTorch, and 996.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-c765620180ff>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Move the model to the selected device and set it to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3162\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m                 )\n\u001b[0;32m-> 3164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 22.17 GiB of which 22.88 MiB is free. Process 232487 has 22.14 GiB memory in use. Of the allocated memory 20.93 GiB is allocated by PyTorch, and 996.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WRufee97E9Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sjccWWMIFZi"
      },
      "source": [
        "## Model saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNz9DicrIGdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d63e7d7-648d-41c8-f835-30c07f19159d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer saved to ./finetuned_model_dp\n"
          ]
        }
      ],
      "source": [
        "# Remove DP hooks if present\n",
        "if hasattr(model, \"remove_hooks\"):\n",
        "    model.remove_hooks()\n",
        "    model = model._module  # Unwrap the model\n",
        "\n",
        "# # Remove DP hooks to restore the underlying model.\n",
        "# model.remove_hooks()\n",
        "# model = model._module  # Unwrap the model.\n",
        "\n",
        "# Define model type based on whether differential privacy was used\n",
        "model_type = \"with_dp\" if hasattr(privacy_engine, \"accountant\") else \"without_dp\"\n",
        "\n",
        "# Specify the directory where you want to save your fine-tuned model\n",
        "save_directory = \"./finetuned_model_dp\"\n",
        "\n",
        "# Save the model weights and configuration\n",
        "model.save_pretrained(save_directory)\n",
        "\n",
        "# Save the tokenizer (this ensures that any custom tokens are preserved)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "# Record model info\n",
        "tracker.save_model_info(\n",
        "    model_path=save_directory,\n",
        "    model_type=model_type,\n",
        "    tokenizer_info={\n",
        "        \"vocab_size\": len(tokenizer),\n",
        "        \"model_max_length\": tokenizer.model_max_length,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Model and tokenizer saved to {save_directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3pAtkLcIIDl"
      },
      "source": [
        "## Interactive testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUKEFCVeIJ7P"
      },
      "outputs": [],
      "source": [
        "# Evaluate using a sample prompt.\n",
        "while True:\n",
        "    sample_prompt = input(\"Input: \")\n",
        "    if sample_prompt.lower() == \"bye\":\n",
        "        break\n",
        "    enc = tokenizer(sample_prompt, return_tensors='pt', padding=True, truncation=True)\n",
        "    enc = {k: v.to(device, non_blocking=True) for k, v in enc.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**enc, max_length=128, do_sample=True, top_k=50)\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8tihiLKfm-2"
      },
      "outputs": [],
      "source": [
        "# while True:\n",
        "#     sample_prompt = input(\"Input: \")\n",
        "#     if sample_prompt.lower() == \"bye\":\n",
        "#         break\n",
        "#     enc = tokenizer(sample_prompt, return_tensors='pt', padding=True, truncation=True)\n",
        "#     enc = {k: v.to(device) for k, v in enc.items()}\n",
        "#     generated_ids = model.generate(**enc, max_length=512, do_sample=True, top_k=50)\n",
        "#     generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "#     print(\"Generated text:\", generated_text)\n",
        "\n",
        "    #lora_alpha = 4\n",
        "    #reduce loss to 2e-5"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KwPM3hdJr1ez",
        "g8WJhCIZMuRB",
        "ykEN9tloMw_h",
        "RkAUFwP4HM5p",
        "M8h8bWh_HRTv",
        "Yda7cPXHH9kl",
        "9sjccWWMIFZi"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9c3f9b9aa6424fec9c1a51cbfe1a44d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0113e3179d948c395cc90d485c7415d",
              "IPY_MODEL_851ef30e404a49978fd5067c72dda3e0",
              "IPY_MODEL_bbc4590ce08747eba9d9afa0a205304f"
            ],
            "layout": "IPY_MODEL_5e72ef4743a3457da85217110d83ebfc"
          }
        },
        "a0113e3179d948c395cc90d485c7415d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_565bfbb1bd004a1fb35450536f34838f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_29d91c8076f444d09a3e5941cad7caec",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "851ef30e404a49978fd5067c72dda3e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7918ee2aeece48beb5bb99835e94055e",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea280ee5e4fa4d768bf60645ff5dd0f1",
            "value": 4
          }
        },
        "bbc4590ce08747eba9d9afa0a205304f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b15598576834a039a085f6b8a9491c2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e94df0ce5ac746b3be53b9275d8a5558",
            "value": "â€‡4/4â€‡[00:10&lt;00:00,â€‡â€‡2.23s/it]"
          }
        },
        "5e72ef4743a3457da85217110d83ebfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "565bfbb1bd004a1fb35450536f34838f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d91c8076f444d09a3e5941cad7caec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7918ee2aeece48beb5bb99835e94055e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea280ee5e4fa4d768bf60645ff5dd0f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b15598576834a039a085f6b8a9491c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e94df0ce5ac746b3be53b9275d8a5558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb986c45c98f4f24ad47c6cb6c4b28ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29dee63c8e7f445cbdfcc6365debdd1b",
              "IPY_MODEL_729f49ea83414ed08fff6968b3aaa617",
              "IPY_MODEL_04687cd0ed9e46789e22e749955bdeef"
            ],
            "layout": "IPY_MODEL_0fddcbb8d61841beb2d7255b8de59336"
          }
        },
        "29dee63c8e7f445cbdfcc6365debdd1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6e784375ad1480f8a8a1a19f7a3f7ef",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0f6322e33ff64d6ab58880fef256993b",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "729f49ea83414ed08fff6968b3aaa617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88101fd544154b4db7b4107d4faab8cf",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e0e7f9688de42dda309b63cf2573d63",
            "value": 4
          }
        },
        "04687cd0ed9e46789e22e749955bdeef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91d67aa1354646128302de0851c7904c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_26b13df091e341328f3e0b32a9506516",
            "value": "â€‡4/4â€‡[00:52&lt;00:00,â€‡11.25s/it]"
          }
        },
        "0fddcbb8d61841beb2d7255b8de59336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6e784375ad1480f8a8a1a19f7a3f7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f6322e33ff64d6ab58880fef256993b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88101fd544154b4db7b4107d4faab8cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e0e7f9688de42dda309b63cf2573d63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91d67aa1354646128302de0851c7904c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26b13df091e341328f3e0b32a9506516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}