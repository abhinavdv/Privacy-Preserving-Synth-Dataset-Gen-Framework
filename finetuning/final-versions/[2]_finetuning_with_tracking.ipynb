{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements and dependencies\n"
      ],
      "metadata": {
        "id": "KwPM3hdJr1ez"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HZof-WuyHdF2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install opacus\n",
        "# !pip install -U bitsandbytes transformers accelerate\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8Ga73J2NITU",
        "outputId": "c1e6aad1-e917-454c-df9e-cf2263019cb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (11.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCRKTux3HJuH",
        "outputId": "63e5a413-2b21-4a72-cd65-b07d6c68c01a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Using device: cuda\n",
            "GPU Device: Tesla T4\n",
            "Available GPU memory: 14.74 GB\n"
          ]
        }
      ],
      "source": [
        "from random import sample\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)  # Should print \"1.23.5\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.amp import autocast, GradScaler  # Import automatic mixed precision tools\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from opacus import PrivacyEngine\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Set up device - prioritize GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Print GPU info if available\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NSOP969tMhsE"
      },
      "outputs": [],
      "source": [
        "## Clear GPU cache and storage\n",
        "torch.cuda.empty_cache()  # Frees unused memory\n",
        "torch.cuda.ipc_collect()  # Collects shared memory used in multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "06uZqEV3rGAe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve token securely\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"Logged in successfully!\")\n",
        "else:\n",
        "    print(\"Hugging Face token not found. Please set it using `userdata.set`.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5eyCXarG33",
        "outputId": "d9ba7fef-b969-4a72-a3a8-e73453d9c6bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU and GPU util functions"
      ],
      "metadata": {
        "id": "g8WJhCIZMuRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetUtilizationRates, nvmlSystemGetDriverVersion, nvmlDeviceGetName, nvmlShutdown\n",
        "    nvmlInit()\n",
        "    NVML_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NVML_AVAILABLE = False\n",
        "\n",
        "def get_cpu_stats():\n",
        "    \"\"\" Get CPU usage stats \"\"\"\n",
        "    cpu_usage = psutil.cpu_percent(interval=1)  # Get CPU usage %\n",
        "    cpu_freq = psutil.cpu_freq().current if psutil.cpu_freq() else \"Unknown\"  # CPU Frequency\n",
        "    num_cores = psutil.cpu_count(logical=False)  # Physical Cores\n",
        "    num_threads = psutil.cpu_count(logical=True)  # Logical Cores\n",
        "    print(f\"CPU Usage: {cpu_usage}%\")\n",
        "    print(f\"CPU Frequency: {cpu_freq} MHz\")\n",
        "    print(f\"Physical Cores: {num_cores}\")\n",
        "    print(f\"Logical Cores: {num_threads}\")\n",
        "\n",
        "def get_ram_stats():\n",
        "    \"\"\" Get system RAM stats \"\"\"\n",
        "    ram = psutil.virtual_memory()\n",
        "    print(\"Total RAM:\", round(ram.total / 1e9, 2), \"GB\")\n",
        "    print(\"Available RAM:\", round(ram.available / 1e9, 2), \"GB\")\n",
        "    print(\"Used RAM:\", round(ram.used / 1e9, 2), \"GB\")\n",
        "    print(\"RAM Usage:\", ram.percent, \"%\")\n",
        "\n",
        "def get_gpu_stats():\n",
        "    \"\"\" Get GPU stats if available \"\"\"\n",
        "    if not NVML_AVAILABLE:\n",
        "        return {\"Error\": \"pynvml not installed. Run: pip install nvidia-ml-py3\"}\n",
        "\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "\n",
        "    for i in range(num_gpus):\n",
        "        handle = nvmlDeviceGetHandleByIndex(i)\n",
        "        mem_info = nvmlDeviceGetMemoryInfo(handle)\n",
        "        utilization = nvmlDeviceGetUtilizationRates(handle)\n",
        "\n",
        "        print(f\"GPU {i} - {nvmlDeviceGetName(handle)}\")\n",
        "        print(f\"Driver Version: {nvmlSystemGetDriverVersion()}\")\n",
        "        print(f\"Total VRAM: {round(mem_info.total / 1e9, 2)} GB\")\n",
        "        print(f\"Used VRAM: {round(mem_info.used / 1e9, 2)} GB\")\n",
        "        print(f\"Free VRAM: {round(mem_info.free / 1e9, 2)} GB\")\n",
        "        print(f\"GPU Usage: {utilization.gpu}%\")\n",
        "        print()\n",
        "\n",
        "    nvmlShutdown()  # Clean up NVML\n",
        "\n",
        "# Run and print system stats\n",
        "\n",
        "print(\"\\nðŸ”¹ CPU Stats:\", )\n",
        "print(\"\\nðŸ”¹ RAM Stats:\", )\n",
        "print(\"\\nðŸ”¹ GPU Stats:\", )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebizwy4ZMnlk",
        "outputId": "57a11faf-2167-4229-ed00-2544d18e1aea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¹ CPU Stats:\n",
            "\n",
            "ðŸ”¹ RAM Stats:\n",
            "\n",
            "ðŸ”¹ GPU Stats:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU & GPU specs"
      ],
      "metadata": {
        "id": "ykEN9tloMw_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_cpu_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SymnXtWfM024",
        "outputId": "9b7f15b1-d920-4aee-a001-68da6557ef3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Usage: 82.0%\n",
            "CPU Frequency: 2000.134 MHz\n",
            "Physical Cores: 1\n",
            "Logical Cores: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ram_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXzbNF9UM1-a",
        "outputId": "7d0bd5c5-0ee7-4ba8-aeb6-4eec5d4dac35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total RAM: 13.61 GB\n",
            "Available RAM: 11.63 GB\n",
            "Used RAM: 1.63 GB\n",
            "RAM Usage: 14.6 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_gpu_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBp4-KgEM3CQ",
        "outputId": "d6b44320-a938-44a5-e9bb-53abf2ac72d0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0 - b'Tesla T4'\n",
            "Driver Version: b'550.54.15'\n",
            "Total VRAM: 16.11 GB\n",
            "Used VRAM: 0.28 GB\n",
            "Free VRAM: 15.83 GB\n",
            "GPU Usage: 0%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkAUFwP4HM5p"
      },
      "source": [
        "## Model Loading and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPZT02dfHLb5",
        "outputId": "cbc4f3b2-d0bb-4d82-aeb3-dccd8862a7c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pad, token doesnt exists, using EOS token\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(128256, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Load Pretrained Model and Tokenizer\n",
        "# model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "# model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "# model_name = \"meta-llama/Llama-3.1-8B\"\n",
        "\n",
        "# This line downloads (if needed) and initializes a tokenizer using the identifier stored in model_name.\n",
        "# The tokenizer converts text into a numerical format (tokens) that the model can process,\n",
        "# and it also handles the reverse process (converting tokens back to human-readable text).\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# This line loads a pre-trained causal language model (such as GPT-style models) using the same model identifier.\n",
        "# It retrieves the model architecture and its pre-trained weights so you can use it for tasks like text generation.\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,       # Loads model in FP16\n",
        "    device_map=\"auto\"                # Automatically distributes model across devices if needed\n",
        ")\n",
        "\n",
        "# !! NEW\n",
        "# Freeze all model parameters (ensuring no gradients are computed for the base model)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Ensure a pad token exists (set to eos token if not present).\n",
        "# 1. Check for the padding token id. If none, use the eos_token as the padding token\n",
        "if tokenizer.pad_token_id is None:\n",
        "    print(\"pad, token doesnt exists, using EOS token\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Adjusts the model's token embedding matrix to match the size of the tokenizer's vocabulary.\n",
        "# This is important because adding or changing tokens (like defining a pad token)\n",
        "# may change the size of the vocabulary, and the model's embedding layer needs to reflect that change.\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8h8bWh_HRTv"
      },
      "source": [
        "## LoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To get all the intermediate layer config of the model\n",
        "# for name, module in model.named_modules():\n",
        "#     print(name, \":\", module)"
      ],
      "metadata": {
        "id": "PKP9YMKBdIpe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZffBS3pRHUrK",
        "outputId": "23a708b9-98f7-4d39-d1e0-dedc4c9cbfca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA applied. Trainable parameters:\n",
            "trainable params: 4,325,376 || all params: 1,240,139,776 || trainable%: 0.3488\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 2048)\n",
              "        (layers): ModuleList(\n",
              "          (0-15): 16 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Enable gradient checkpointing to save memory.\n",
        "\n",
        "# This technique reduces memory usage during training by not storing all intermediate activations\n",
        "# during the forward pass. Instead, it saves only a subset of them and recomputes the missing ones\n",
        "# during the backward pass.\n",
        "model.config.gradient_checkpointing = True\n",
        "\n",
        "# Configure LoRA: update only a small set of additional parameters.\n",
        "# tried r=4 and lora+alpha = 32. Maybe that destabilized training so modifying to 8 and 16 respectively\n",
        "#initally was 0.1, changing to 0.05\n",
        "\n",
        "# studies say best to apply Lora to all layers\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,  # Fine-tuning for causal language modeling.\n",
        "    inference_mode=False,          # Training mode.\n",
        "    r=8,                           # Rank of low-rank decomposition.\n",
        "    lora_alpha=16,                 # Scaling factor.\n",
        "    lora_dropout=0.05,               # Dropout rate for LoRA layers.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# This function call takes the pre-trained model and applies the LoRA configuration you defined.\n",
        "# It modifies the model so that, instead of updating all parameters during fine-tuning,\n",
        "# only a small subset (the LoRA adapters) is trained.\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"LoRA applied. Trainable parameters:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Move the model to the chosen device and set to training mode.\n",
        "model.to(device)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fulb2eoMHXBD"
      },
      "source": [
        "## Data loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ooqU-XeiHY3Q"
      },
      "outputs": [],
      "source": [
        "# Load and Format Training Data\n",
        "import json\n",
        "\n",
        "formatted_strings = []\n",
        "\n",
        "with open(\"finetuning/train.jsonl\", \"r\") as f:\n",
        "    j = 0\n",
        "    for line in f:\n",
        "        # Parse the JSON data from the line\n",
        "        data = json.loads(line.strip())\n",
        "        # Extract values\n",
        "        rating = data['Rating']\n",
        "        title = data['Title']\n",
        "        review = data['Review']\n",
        "\n",
        "        # Format the string as per the required format\n",
        "        formatted_string = f'\"System prompt : Given the Rating and Title, you are required to generate the review\" | \"Rating\": {rating} | \"Title\": {title} | \"Review\": {review}'\n",
        "\n",
        "        # Add the formatted string to the list\n",
        "        formatted_strings.append(formatted_string)\n",
        "\n",
        "        j+=1\n",
        "        if j == 1000:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now `formatted_strings` contains the list of strings in the desired format\n",
        "print(\"Size: \",len(formatted_strings))\n",
        "print(formatted_strings[0])\n",
        "train_texts = formatted_strings\n",
        "strs = [len(formatted_str) for formatted_str in formatted_strings]\n",
        "print(\"length of largets string is: \",sum(strs) / len(strs))\n",
        "# avg around 328"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPe6k7Dmdtwp",
        "outputId": "f6fd5c40-9501-48ce-9e21-7dd14154c787"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  1000\n",
            "\"System prompt : Given the Rating and Title, you are required to generate the review\" | \"Rating\": 4 | \"Title\": No white background! Itâ€™s clear! | \"Review\": I bought this bc I thought it had the nice white background. Turns out itâ€™s clear & since my phone is blue it doesnâ€™t look anything like this.  If I had known that I would have purchased something else. It works ok.\n",
            "length of largets string is:  408.746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCsd5i7NH17t"
      },
      "source": [
        "## Data tokenization and dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV5ygoW-H6IH",
        "outputId": "6b1f156b-6839-4623-be2d-a740f174ee70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: torch.Size([1000, 128])\n"
          ]
        }
      ],
      "source": [
        "# !! NEW - max_length=512\n",
        "\n",
        "DATA_PARAMS = {\n",
        "  \"max_length\": 128,\n",
        "  \"batch_size\": 4,\n",
        "}\n",
        "\n",
        "# Tokenize training texts with padding and truncation.\n",
        "encodings = tokenizer(train_texts, return_tensors='pt', padding=True, truncation=True, max_length=DATA_PARAMS['max_length'])\n",
        "input_ids = encodings['input_ids']\n",
        "attention_mask = encodings['attention_mask']\n",
        "\n",
        "# For causal language modeling, use input_ids as labels.\n",
        "# Replace pad token positions with -100 so that they are ignored by the loss.\n",
        "\n",
        "#creates a copy of your input IDs, so you can modify them without affecting the original tensor.\n",
        "labels = input_ids.clone()\n",
        "\n",
        "#replaces all padding token positions with -100. This is a common convention (especially with PyTorchâ€™s CrossEntropyLoss)\n",
        "# to indicate that these positions should be ignored during loss computatio\n",
        "labels[input_ids == tokenizer.pad_token_id] = -100\n",
        "\n",
        "print(\"Training data shape:\", input_ids.shape)\n",
        "\n",
        "\n",
        "# !! NEW - num_workers=4, pin_memory=True\n",
        "# Create a TensorDataset and DataLoader with a small batch size.\n",
        "train_dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=DATA_PARAMS['batch_size'], shuffle=True, drop_last=True, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yda7cPXHH9kl"
      },
      "source": [
        "## Optimizer & Privacy engine setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !! NEW\n",
        "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4)"
      ],
      "metadata": {
        "id": "nNoZnmXSeXhv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "P4pqKECaH-7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdc462cb-774f-40fb-b12c-2f88e949c6fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "privacy_engine = PrivacyEngine()\n",
        "model, optimizer, train_loader = privacy_engine.make_private(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    noise_multiplier=0.6,      # Adjust for your desired privacy guarantee.\n",
        "    max_grad_norm=1.5,         # Gradient clipping norm.\n",
        "    batch_first=True,\n",
        "    loss_reduction=\"mean\",\n",
        "    poisson_sampling=False     # Use standard sampling.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tracking"
      ],
      "metadata": {
        "id": "ydDru4H6pMXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "class TrainingTracker:\n",
        "    def __init__(self, base_dir=\"./tracking_results\"):\n",
        "        \"\"\"\n",
        "        Initialize the training tracker.\n",
        "\n",
        "        Args:\n",
        "            base_dir: Directory to save tracking results\n",
        "        \"\"\"\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.base_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        # Generate a unique run ID based on timestamp\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.run_id = f\"run_{timestamp}\"\n",
        "\n",
        "        # Create run directory\n",
        "        self.run_dir = self.base_dir / self.run_id\n",
        "        self.run_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Initialize tracking data structures\n",
        "        self.params = {}\n",
        "        self.epoch_metrics = []\n",
        "        self.generated_samples = []\n",
        "        self.privacy_metrics = {}\n",
        "\n",
        "    def record_parameters(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Record training parameters for the current run.\n",
        "\n",
        "        Args:\n",
        "            **kwargs: Key-value pairs of parameters to record\n",
        "        \"\"\"\n",
        "        self.params.update(kwargs)\n",
        "\n",
        "        # Save parameters to file\n",
        "        with open(self.run_dir / \"parameters.json\", \"w\") as f:\n",
        "            json.dump(self.params, f, indent=4)\n",
        "\n",
        "    def record_epoch_metrics(self, epoch, loss, batch_times=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Record metrics for a training epoch.\n",
        "\n",
        "        Args:\n",
        "            epoch: Current epoch number\n",
        "            loss: Loss value for the epoch\n",
        "            batch_times: Optional list of batch processing times\n",
        "            **kwargs: Additional metrics to record\n",
        "        \"\"\"\n",
        "        metrics = {\n",
        "            \"epoch\": epoch,\n",
        "            \"loss\": loss,\n",
        "            **kwargs\n",
        "        }\n",
        "\n",
        "        if batch_times:\n",
        "            metrics[\"avg_batch_time\"] = sum(batch_times) / len(batch_times)\n",
        "            metrics[\"min_batch_time\"] = min(batch_times)\n",
        "            metrics[\"max_batch_time\"] = max(batch_times)\n",
        "\n",
        "        self.epoch_metrics.append(metrics)\n",
        "\n",
        "        # Save updated metrics to file\n",
        "        with open(self.run_dir / \"epoch_metrics.json\", \"w\") as f:\n",
        "            json.dump(self.epoch_metrics, f, indent=4)\n",
        "\n",
        "        # Also save as CSV for easier analysis\n",
        "        pd.DataFrame(self.epoch_metrics).to_csv(\n",
        "            self.run_dir / \"epoch_metrics.csv\", index=False)\n",
        "\n",
        "    def record_privacy_budget(self, epsilon, delta=1e-5, **kwargs):\n",
        "        \"\"\"\n",
        "        Record privacy budget metrics.\n",
        "\n",
        "        Args:\n",
        "            epsilon: Achieved epsilon value\n",
        "            delta: Delta value used\n",
        "            **kwargs: Additional privacy metrics\n",
        "        \"\"\"\n",
        "        self.privacy_metrics = {\n",
        "            \"epsilon\": epsilon,\n",
        "            \"delta\": delta,\n",
        "            **kwargs\n",
        "        }\n",
        "\n",
        "        # Save privacy metrics to file\n",
        "        with open(self.run_dir / \"privacy_metrics.json\", \"w\") as f:\n",
        "            json.dump(self.privacy_metrics, f, indent=4)\n",
        "\n",
        "    def record_sample(self, prompt, generated_text):\n",
        "        \"\"\"\n",
        "        Record a sample of generated text.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input prompt\n",
        "            generated_text: Generated text output\n",
        "        \"\"\"\n",
        "        sample = {\n",
        "            \"prompt\": prompt,\n",
        "            \"generated_text\": generated_text,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self.generated_samples.append(sample)\n",
        "\n",
        "        # Save samples to file\n",
        "        with open(self.run_dir / \"generated_samples.json\", \"w\") as f:\n",
        "            json.dump(self.generated_samples, f, indent=4)\n",
        "\n",
        "    def save_model_info(self, model_path, model_type, tokenizer_info=None):\n",
        "        \"\"\"\n",
        "        Record information about the saved model.\n",
        "\n",
        "        Args:\n",
        "            model_path: Path where model was saved\n",
        "            model_type: Type of model (e.g., \"with_dp\", \"without_dp\")\n",
        "            tokenizer_info: Additional tokenizer information\n",
        "        \"\"\"\n",
        "        model_info = {\n",
        "            \"model_path\": str(model_path),\n",
        "            \"model_type\": model_type,\n",
        "            \"tokenizer_info\": tokenizer_info or {}\n",
        "        }\n",
        "\n",
        "        # Save model info to file\n",
        "        with open(self.run_dir / \"model_info.json\", \"w\") as f:\n",
        "            json.dump(model_info, f, indent=4)\n",
        "\n",
        "    def generate_summary(self):\n",
        "        \"\"\"\n",
        "        Generate a summary of the training run.\n",
        "\n",
        "        Returns:\n",
        "            str: Summary text\n",
        "        \"\"\"\n",
        "        summary_lines = [\n",
        "            f\"Training Run: {self.run_id}\",\n",
        "            \"=\" * 50,\n",
        "            \"\\nParameters:\",\n",
        "        ]\n",
        "\n",
        "        for key, value in self.params.items():\n",
        "            summary_lines.append(f\"  {key}: {value}\")\n",
        "\n",
        "        if self.epoch_metrics:\n",
        "            summary_lines.extend([\n",
        "                \"\\nTraining Results:\",\n",
        "                f\"  Epochs completed: {len(self.epoch_metrics)}\",\n",
        "                f\"  Final loss: {self.epoch_metrics[-1]['loss']:.6f}\",\n",
        "                f\"  Initial loss: {self.epoch_metrics[0]['loss']:.6f}\",\n",
        "                f\"  Loss reduction: {self.epoch_metrics[0]['loss'] - self.epoch_metrics[-1]['loss']:.6f}\"\n",
        "            ])\n",
        "\n",
        "        if self.privacy_metrics:\n",
        "            summary_lines.extend([\n",
        "                \"\\nPrivacy Budget:\",\n",
        "                f\"  Epsilon: {self.privacy_metrics['epsilon']:.4f}\",\n",
        "                f\"  Delta: {self.privacy_metrics['delta']}\"\n",
        "            ])\n",
        "\n",
        "        summary_text = \"\\n\".join(summary_lines)\n",
        "\n",
        "        # Save summary to file\n",
        "        with open(self.run_dir / \"summary.txt\", \"w\") as f:\n",
        "            f.write(summary_text)\n",
        "\n",
        "        return summary_text\n",
        "\n",
        "    # def compare_with_previous_runs(self, metric=\"loss\"):\n",
        "    #     \"\"\"\n",
        "    #     Compare this run with previous runs based on a specific metric.\n",
        "\n",
        "    #     Args:\n",
        "    #         metric: Metric to compare (default: \"loss\")\n",
        "\n",
        "    #     Returns:\n",
        "    #         DataFrame: Comparison data\n",
        "    #     \"\"\"\n",
        "    #     # Collect data from all previous runs\n",
        "    #     all_runs = []\n",
        "\n",
        "    #     for run_dir in self.base_dir.iterdir():\n",
        "    #         if not run_dir.is_dir() or run_dir == self.run_dir:\n",
        "    #             continue\n",
        "\n",
        "    #         params_file = run_dir / \"parameters.json\"\n",
        "    #         metrics_file = run_dir / \"epoch_metrics.json\"\n",
        "\n",
        "    #         if params_file.exists() and metrics_file.exists():\n",
        "    #             with open(params_file, \"r\") as f:\n",
        "    #                 params = json.load(f)\n",
        "\n",
        "    #             with open(metrics_file, \"r\") as f:\n",
        "    #                 metrics = json.load(f)\n",
        "\n",
        "    #             if metrics:\n",
        "    #                 final_metric = metrics[-1].get(metric)\n",
        "\n",
        "    #                 run_data = {\n",
        "    #                     \"run_id\": run_dir.name,\n",
        "    #                     f\"final_{metric}\": final_metric,\n",
        "    #                     **params\n",
        "    #                 }\n",
        "\n",
        "    #                 all_runs.append(run_data)\n",
        "\n",
        "    #     # Add current run\n",
        "    #     if self.epoch_metrics:\n",
        "    #         current_run_data = {\n",
        "    #             \"run_id\": self.run_id,\n",
        "    #             f\"final_{metric}\": self.epoch_metrics[-1].get(metric),\n",
        "    #             **self.params\n",
        "    #         }\n",
        "    #         all_runs.append(current_run_data)\n",
        "\n",
        "    #     # Convert to DataFrame and sort\n",
        "    #     if all_runs:\n",
        "    #         df = pd.DataFrame(all_runs)\n",
        "    #         df = df.sort_values(by=f\"final_{metric}\")\n",
        "\n",
        "    #         # Save comparison to file\n",
        "    #         df.to_csv(self.run_dir / f\"comparison_{metric}.csv\", index=False)\n",
        "\n",
        "    #         return df\n",
        "\n",
        "    #     return pd.DataFrame()"
      ],
      "metadata": {
        "id": "qLr62h6jpLpO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Record initial parameters after setting them up\n",
        "def record_initial_params():\n",
        "    # Record model configuration\n",
        "    tracker.record_parameters(\n",
        "        model_name=model_name,\n",
        "        device=str(device),\n",
        "        epochs=epochs,\n",
        "        batch_size=train_loader.batch_size,\n",
        "        learning_rate=optimizer.param_groups[0]['lr'],\n",
        "        gradient_accumulation_steps=accumulation_steps,\n",
        "\n",
        "        # LoRA parameters\n",
        "        lora_r=lora_config.r,\n",
        "        lora_alpha=lora_config.lora_alpha,\n",
        "        lora_dropout=lora_config.lora_dropout,\n",
        "        lora_target_modules=list(lora_config.target_modules),\n",
        "\n",
        "        # Privacy parameters (if using Opacus)\n",
        "        using_differential_privacy=hasattr(model, \"remove_hooks\"),\n",
        "        noise_multiplier=0.6 if hasattr(model, \"remove_hooks\") else None,\n",
        "        max_grad_norm=1.5 if hasattr(model, \"remove_hooks\") else None,\n",
        "\n",
        "        # Dataset info\n",
        "        dataset_size=len(formatted_strings),\n",
        "        avg_sample_length=sum(len(s) for s in formatted_strings) / len(formatted_strings),\n",
        "        tokenizer_max_length=DATA_PARAMS['max_length'],  # From tokenization step\n",
        "        data_batch_size=DATA_PARAMS['batch_size'],\n",
        "\n",
        "        # Tokenizer info\n",
        "        tokenizer_vocab_size=len(tokenizer),\n",
        "        tokenizer_model_max_length=tokenizer.model_max_length,\n",
        "\n",
        "\n",
        "        # System info\n",
        "        cuda_available=torch.cuda.is_available(),\n",
        "        gpu_name=torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
        "    )"
      ],
      "metadata": {
        "id": "rNSHqUqOCjWS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjdbmm3wIB0M"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1  # Use more epochs for a real task.\n",
        "\n",
        "# !! NEW\n",
        "scaler = GradScaler('cuda')  # Create a gradient scaler to manage FP16 stability\n",
        "accumulation_steps = 1  # Set gradient accumulation steps; use >1 to simulate larger batch sizes"
      ],
      "metadata": {
        "id": "4iRZnGXfGgij"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tracker before loading the model\n",
        "tracker = TrainingTracker()\n",
        "# Call this after all parameters are set but before training starts\n",
        "record_initial_params()"
      ],
      "metadata": {
        "id": "Fxzk9pcCCacI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aZm-yQCJOdve",
        "outputId": "b08992d4-b6c4-4068-e5e0-dde86d108b94"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradSampleModule(PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 2048)\n",
              "        (layers): ModuleList(\n",
              "          (0-15): 16 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              "))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # FINETUNING, NOT WORKING NOW\n",
        "# for epoch in range(epochs):  # Loop over each epoch\n",
        "#     total_loss = 0.0  # Initialize total loss accumulator for the epoch\n",
        "#     optimizer.zero_grad()  # Zero gradients at the start of the epoch\n",
        "#     for i, batch in enumerate(train_loader):  # Loop over mini-batches from the DataLoader\n",
        "#         # Move each tensor in the batch to the device (GPU) asynchronously if pin_memory is True\n",
        "#         input_ids_batch, attention_mask_batch, labels_batch = [\n",
        "#             x.to(device, non_blocking=True) for x in batch\n",
        "#         ]\n",
        "\n",
        "#         # Determine the sequence length for the current batch and create position IDs accordingly\n",
        "#         seq_len = input_ids_batch.size(1)  # Get the sequence length from the input tensor\n",
        "#         # Create a tensor [0, 1, ..., seq_len-1] and repeat it for each item in the batch\n",
        "#         position_ids = torch.arange(seq_len, device=device).unsqueeze(0).repeat(input_ids_batch.size(0), 1)\n",
        "\n",
        "#         # Use mixed precision context for the forward pass to save memory and speed up computation\n",
        "#         with autocast():\n",
        "#             outputs = model(\n",
        "#                 input_ids=input_ids_batch,        # Input token IDs for the model\n",
        "#                 attention_mask=attention_mask_batch,  # Attention mask to differentiate padded tokens\n",
        "#                 position_ids=position_ids,          # Positional IDs for the tokens\n",
        "#                 labels=labels_batch                 # Labels for computing the loss (typically same as input_ids for causal LM)\n",
        "#             )\n",
        "#             # Compute the loss; if using gradient accumulation, scale down the loss accordingly\n",
        "#             loss = outputs.loss / accumulation_steps\n",
        "\n",
        "#         # Scale the loss and perform the backward pass using the GradScaler for FP16 stability\n",
        "#         scaler.scale(loss).backward()\n",
        "\n",
        "#         # Every 'accumulation_steps' iterations, update the model weights\n",
        "#         if (i + 1) % accumulation_steps == 0:\n",
        "#             scaler.step(optimizer)  # Update parameters using scaled gradients\n",
        "#             scaler.update()         # Update the scale for the next iteration\n",
        "#             optimizer.zero_grad()   # Reset gradients after updating\n",
        "\n",
        "#         # Accumulate the loss (multiply back to undo the earlier division, so total_loss is in original scale)\n",
        "#         total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "#         # Optionally, print progress every 50 batches\n",
        "#         if i % 50 == 0:\n",
        "#             print(f\"Batch {i} processed.\")\n",
        "\n",
        "#     # Compute the average loss over the epoch\n",
        "#     avg_loss = total_loss / len(train_loader)\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} - Average loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "eCilSHLsfGGa"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    batch_times = []\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        batch_start = datetime.datetime.now()\n",
        "\n",
        "\n",
        "        # Move each element of the batch to the device.\n",
        "        # input_ids_batch, attention_mask_batch, labels_batch = [x.to(device) for x in batch]\n",
        "        input_ids_batch, attention_mask_batch, labels_batch = [x.to(device, non_blocking=True) for x in batch]\n",
        "\n",
        "        # Create a position_ids tensor: shape [batch_size, seq_len]\n",
        "        seq_len = input_ids_batch.size(1)\n",
        "        position_ids = torch.arange(seq_len, device=device).unsqueeze(0).repeat(input_ids_batch.size(0), 1)\n",
        "\n",
        "        # Forward pass: compute the loss.\n",
        "        outputs = model(\n",
        "            input_ids=input_ids_batch,\n",
        "            attention_mask=attention_mask_batch,\n",
        "            position_ids=position_ids,\n",
        "            labels=labels_batch\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        # total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        total_loss += loss.item()\n",
        "        batch_end = datetime.datetime.now()\n",
        "        batch_times.append((batch_end - batch_start).total_seconds())\n",
        "\n",
        "        # Print progress\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Batch {i}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Compute epoch metrics\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    end_time = datetime.datetime.now()\n",
        "    epoch_duration = (end_time - start_time).total_seconds()\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Average loss: {avg_loss:.4f} - Duration: {epoch_duration:.2f}s\")\n",
        "\n",
        "    # Record epoch metrics\n",
        "    tracker.record_epoch_metrics(\n",
        "        epoch=epoch+1,\n",
        "        loss=avg_loss,\n",
        "        batch_times=batch_times,\n",
        "        epoch_duration=epoch_duration,\n",
        "        timestamp=datetime.datetime.now().isoformat()\n",
        "    )\n",
        "\n",
        "    # Record privacy budget if using differential privacy\n",
        "    if hasattr(model, \"remove_hooks\"):\n",
        "        epsilon = privacy_engine.accountant.get_epsilon(delta=1e-5)\n",
        "        print(f\"Achieved privacy budget: Îµ = {epsilon:.2f}\")\n",
        "        tracker.record_privacy_budget(epsilon=epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Guo1CVB3fLLr",
        "outputId": "0e3227f5-63a2-4e16-8bb2-93f8093f23a7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 - Batch 0/250 - Loss: 3.6693\n",
            "0\n",
            "Epoch 1/1 - Batch 50/250 - Loss: 3.4663\n",
            "50\n",
            "Epoch 1/1 - Batch 100/250 - Loss: 3.5216\n",
            "100\n",
            "Epoch 1/1 - Batch 150/250 - Loss: 3.4874\n",
            "150\n",
            "Epoch 1/1 - Batch 200/250 - Loss: 3.1492\n",
            "200\n",
            "Epoch 1/1 - Average loss: 3.5054 - Duration: 71.21s\n",
            "Achieved privacy budget: Îµ = 2.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sjccWWMIFZi"
      },
      "source": [
        "## Model saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BNz9DicrIGdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c3cfb5-9ff9-4b0a-b8ca-c67371f58fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer saved to ./finetuned_model_dp\n"
          ]
        }
      ],
      "source": [
        "# Remove DP hooks if present\n",
        "if hasattr(model, \"remove_hooks\"):\n",
        "    model.remove_hooks()\n",
        "    model = model._module  # Unwrap the model\n",
        "\n",
        "# # Remove DP hooks to restore the underlying model.\n",
        "# model.remove_hooks()\n",
        "# model = model._module  # Unwrap the model.\n",
        "\n",
        "# Define model type based on whether differential privacy was used\n",
        "model_type = \"with_dp\" if hasattr(privacy_engine, \"accountant\") else \"without_dp\"\n",
        "\n",
        "# Specify the directory where you want to save your fine-tuned model\n",
        "save_directory = \"./finetuned_model_dp\"\n",
        "\n",
        "# Save the model weights and configuration\n",
        "model.save_pretrained(save_directory)\n",
        "\n",
        "# Save the tokenizer (this ensures that any custom tokens are preserved)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "# Record model info\n",
        "tracker.save_model_info(\n",
        "    model_path=save_directory,\n",
        "    model_type=model_type,\n",
        "    tokenizer_info={\n",
        "        \"vocab_size\": len(tokenizer),\n",
        "        \"model_max_length\": tokenizer.model_max_length,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Model and tokenizer saved to {save_directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3pAtkLcIIDl"
      },
      "source": [
        "## Interactive testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUKEFCVeIJ7P"
      },
      "outputs": [],
      "source": [
        "# Evaluate using a sample prompt.\n",
        "while True:\n",
        "    sample_prompt = input(\"Input: \")\n",
        "    if sample_prompt.lower() == \"bye\":\n",
        "        break\n",
        "    enc = tokenizer(sample_prompt, return_tensors='pt', padding=True, truncation=True)\n",
        "    enc = {k: v.to(device, non_blocking=True) for k, v in enc.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**enc, max_length=128, do_sample=True, top_k=50)\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# while True:\n",
        "#     sample_prompt = input(\"Input: \")\n",
        "#     if sample_prompt.lower() == \"bye\":\n",
        "#         break\n",
        "#     enc = tokenizer(sample_prompt, return_tensors='pt', padding=True, truncation=True)\n",
        "#     enc = {k: v.to(device) for k, v in enc.items()}\n",
        "#     generated_ids = model.generate(**enc, max_length=512, do_sample=True, top_k=50)\n",
        "#     generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "#     print(\"Generated text:\", generated_text)\n",
        "\n",
        "    #lora_alpha = 4\n",
        "    #reduce loss to 2e-5"
      ],
      "metadata": {
        "id": "j8tihiLKfm-2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KwPM3hdJr1ez",
        "g8WJhCIZMuRB",
        "ykEN9tloMw_h",
        "RkAUFwP4HM5p",
        "M8h8bWh_HRTv",
        "Yda7cPXHH9kl",
        "9sjccWWMIFZi"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}