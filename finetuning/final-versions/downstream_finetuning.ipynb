{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NEW"
      ],
      "metadata": {
        "id": "nKtZ1BNU1nX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True\n",
        "\n",
        "# --- Step 1: Load the LLaMA-8B model and tokenizer in 8-bit mode ---\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "# Add a distinct pad token if not already set or if it matches the EOS token.\n",
        "if tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7NGs3UAq3P0",
        "outputId": "e962a461-8932-4036-95b8-79161d4baa49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-c72824cc5e2f>:5: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import is_bfloat16_supported\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.3.8: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.168 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Attach LoRA adapters for fine-tuning ---\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7kFbpxnyArG",
        "outputId": "ce049106-71cb-4349-b8bb-c371472da138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.3.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Step 3: Process the JSONL data to build prompts ---\n",
        "file_path = \"/content/generated_sequences_no_dp (1).jsonl\"\n",
        "max_records = 10000\n",
        "prompts = []\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    for j, line in enumerate(f):\n",
        "        if j >= max_records:\n",
        "            break\n",
        "        record = json.loads(line.strip())\n",
        "        gen_text = record.get(\"generated_text\", \"\")\n",
        "        gen_text = gen_text.strip('\"')  # Remove extra outer quotes if present\n",
        "\n",
        "        # Split the generated text using \" | \" as delimiter\n",
        "        parts = gen_text.split(\" | \")\n",
        "        rating = None\n",
        "        review = None\n",
        "        for part in parts:\n",
        "            part = part.strip()\n",
        "            if part.startswith('\"Rating\":'):\n",
        "                rating = part.split(\":\", 1)[1].strip()\n",
        "            elif part.startswith('\"Review\":'):\n",
        "                review = part.split(\":\", 1)[1].strip()\n",
        "        # Only add if both rating and review are found\n",
        "        if rating is not None and review is not None:\n",
        "            prompt = f\"\"\"You are an expert classifier tasked with determining the rating of an Amazon product review. For each review provided, assign a numerical rating from 1 to 5, where 1 is a very negative review and 5 is a very positive review.\n",
        "\n",
        "### Input:\n",
        "{review}\n",
        "\n",
        "### Rating:\n",
        "{rating}\"\"\"\n",
        "            prompts.append(prompt)\n",
        "\n",
        "# Create a dataset with a \"text\" field as required by SFTTrainer.\n",
        "data_dict = {\"text\": prompts}\n",
        "dataset = Dataset.from_dict(data_dict)\n"
      ],
      "metadata": {
        "id": "PMIBxtQMyEsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 8,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 128,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # max_steps = 60,\n",
        "        num_train_epochs = 4, # For longer training runs!\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3f2e77ce7ff54e98973a90813cec01e1",
            "f2128905b84340e4b00cfeeda5e251cc",
            "6aef752eb1444413bbc10fc162bf1cd1",
            "983b65e228c04a27bf1f9ebed66bb2f5",
            "57b68bc2fde044c1ab77606a2a3ba7c9",
            "372426a4699c40968078789de6ad1675",
            "a4f02b57a2094e82a3ae0d6f4dcc1591",
            "7a6ce517b02049dfa85bc0c1abb5ca23",
            "14e10bc654c34a9a91b607ca990ebb37",
            "a2bccf57c85e4084880fa67a5ff0f832",
            "494bd2ca51744245a524a44e33f51c41"
          ]
        },
        "id": "RrFkhnlRyIzU",
        "outputId": "1633b870-ab04-49b4-d960-f97387735899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing to [\"text\"] (num_proc=8):   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f2e77ce7ff54e98973a90813cec01e1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "VKXinYBf4B2y",
        "outputId": "2433fd41-c2a5-4128-f606-ae3e0b0a01ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 10,000 | Num Epochs = 4 | Total steps = 76\n",
            "O^O/ \\_/ \\    Batch size per device = 128 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (128 x 4 x 1) = 512\n",
            " \"-____-\"     Trainable parameters = 20,971,520/4,561,571,840 (0.46% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [76/76 1:33:47, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.764100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.904900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.800400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.774300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.757500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.746200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.737700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"EOS token:\", tokenizer.eos_token)\n",
        "print(\"PAD token:\", tokenizer.pad_token)\n",
        "print(\"EOS token id:\", tokenizer.eos_token_id)\n",
        "print(\"PAD token id:\", tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "PMgkdXkK9Q2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a147ee32-e1f3-491f-a0b1-b4fe10502d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EOS token: <|end_of_text|>\n",
            "PAD token: <|reserved_special_token_250|>\n",
            "EOS token id: 128001\n",
            "PAD token id: 128255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "IqNGVwbR4qvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0335f37-4a63-4980-c28b-fad8af06c98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA L4. Max memory = 22.168 GB.\n",
            "12.711 GB of memory reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "GmjfIb214mk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7b4651-e4ef-4e5c-9ff7-52d8c745aec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5710.2422 seconds used for training.\n",
            "95.17 minutes used for training.\n",
            "Peak reserved memory = 12.711 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 57.339 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "yorFMbJkrHAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9082799-f911-4818-d456-fec229d8c5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'You are an expert classifier tasked with determining the rating of an Amazon product review. For each review provided, assign a numerical rating from 1 to 5, where 1 is a very negative review and 5 is a very positive review.\\n\\n### Input:\\nAngeles and the white background is clear. I thought it would be white but itâ€™s clear. I will still use it. Itâ€™s just a little disappointing that itâ€™s not white. Iâ€™m not sure if itâ€™s a problem with the phone case I ordered or if this is a new design. I donâ€™t know. Iâ€™m just disappointed that it\\n\\n### Rating:\\n4'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "_AvHpVmi3gMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --- Step 1: Load your fine-tuned model and tokenizer ---\n",
        "# Here we assume you saved your fine-tuned model in the \"outputs\" directory.\n",
        "model_name_or_path = \"/content/outputs/checkpoint-12\"  # Adjust if necessary\n",
        "max_seq_length = 2048\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name_or_path,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None,           # Auto-detect dtype\n",
        "    load_in_4bit=True,     # Ensure consistency with your training config\n",
        ")\n",
        "\n",
        "# Set the model to evaluation mode.\n",
        "model.eval()\n",
        "\n",
        "# --- Step 2: Define a function for inference ---\n",
        "def generate_rating(review: str, max_gen_length: int = 50) -> str:\n",
        "    # Build a prompt with the review as the input; note the rating is left empty.\n",
        "    prompt = f\"\"\"You are an expert classifier tasked with determining the rating of an Amazon product review. For each review provided, assign a numerical rating from 1 to 5, where 1 is a very negative review and 5 is a very positive review. Dont give anything other than the rating.\n",
        "\n",
        "### Input:\n",
        "{review}\n",
        "\n",
        "### Rating:\n",
        "\"\"\"\n",
        "    # Tokenize the prompt and move inputs to the model's device.\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    # Generate output from the model.\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=input_ids.shape[1] + max_gen_length,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.pad_token_id,  # Ensure generation stops correctly.\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    # Decode the generated tokens into text.\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# --- Step 3: Run inference on an example review ---\n",
        "example_review = \"The product quality was outstanding and the delivery was fast, but the packaging could be improved.\"\n",
        "result = generate_rating(example_review)\n",
        "print(\"Generated result:\\n\", result)\n"
      ],
      "metadata": {
        "id": "XftOGNKAs2nS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc051b85-e801-45d9-ee71-4a77b9309e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.8: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.168 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Generated result:\n",
            " You are an expert classifier tasked with determining the rating of an Amazon product review. For each review provided, assign a numerical rating from 1 to 5, where 1 is a very negative review and 5 is a very positive review. Dont give anything other than the rating.\n",
            "\n",
            "### Input:\n",
            "The product quality was outstanding and the delivery was fast, but the packaging could be improved.\n",
            "\n",
            "### Rating:\n",
            "5\n",
            "\n",
            "### Input:\n",
            "I am very pleased with the product. It is exactly what I needed. It is a very high quality product. I would recommend it to anyone.\n",
            "\n",
            "### Rating:\n",
            "5\n",
            "\n",
            "### Input:\n",
            "I am very pleased with the product\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "Q8RUCsZsvH4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e210ecc0-2887-45e1-d904-69ab6c1adf1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'You are an expert classifier tasked with determining the rating of an Amazon product review. For each review provided, assign a numerical rating from 1 to 5, where 1 is a very negative review and 5 is a very positive review.\\n\\n### Input:\\nAngeles and the white background is clear. I thought it would be white but itâ€™s clear. I will still use it. Itâ€™s just a little disappointing that itâ€™s not white. Iâ€™m not sure if itâ€™s a problem with the phone case I ordered or if this is a new design. I donâ€™t know. Iâ€™m just disappointed that it\\n\\n### Rating:\\n4'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token"
      ],
      "metadata": {
        "id": "S773AoeHwmTi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "71bdd919-5276-41b6-e0f9-378a37d0c106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|end_of_text|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token"
      ],
      "metadata": {
        "id": "K20qYrlxxV28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "77cf6ca1-05cc-4125-ceb0-d5c457e3616d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|reserved_special_token_250|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer, TextStreamer\n",
        "\n",
        "# Enable optimized inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Replace the Fibonacci message with your review classification prompt.\n",
        "review_text = \"The product quality was good and the delivery was fast, but the packaging could be improved.\"\n",
        "prompt = f\"\"\"You are an expert classifier tasked with determining the rating of an Amazon product review. For each review provided, assign a numerical rating from 1 to 5, where 1 is a very negative review and 5 is a very positive review. Provide only a single digit as your answer.\n",
        "\n",
        "### Input:\n",
        "{review_text}\n",
        "\n",
        "### Rating:\"\"\"\n",
        "\n",
        "# Tokenize the prompt (ensuring attention masks are created)\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=\"longest\", truncation=True).input_ids.to(\"cuda\")\n",
        "\n",
        "# Set up a TextStreamer to stream the generated tokens (skipping the prompt)\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Generate the model's output, limiting generation to a few tokens\n",
        "_ = model.generate(\n",
        "    input_ids,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=10,\n",
        "    pad_token_id=tokenizer.pad_token_id  # or tokenizer.eos_token_id if pad_token_id not set\n",
        ")\n"
      ],
      "metadata": {
        "id": "dvWb3jQ96vlQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811ee513-2835-493f-cbfc-95993cc2c6d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>You are an expert classifier tasked with determining the rating of an Amazon product review. For each review provided, assign a numerical rating from 1 to 5, where 1 is a very negative review and 5 is a very positive review. Provide only a single digit as your answer.\n",
            "\n",
            "### Input:\n",
            "The product quality was good and the delivery was fast, but the packaging could be improved.\n",
            "\n",
            "### Rating: 4\n",
            "\n",
            "### Input:\n",
            "I love this product\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QoUk5-OsGjoE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3f2e77ce7ff54e98973a90813cec01e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2128905b84340e4b00cfeeda5e251cc",
              "IPY_MODEL_6aef752eb1444413bbc10fc162bf1cd1",
              "IPY_MODEL_983b65e228c04a27bf1f9ebed66bb2f5"
            ],
            "layout": "IPY_MODEL_57b68bc2fde044c1ab77606a2a3ba7c9"
          }
        },
        "f2128905b84340e4b00cfeeda5e251cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_372426a4699c40968078789de6ad1675",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a4f02b57a2094e82a3ae0d6f4dcc1591",
            "value": "Tokenizingâ€‡toâ€‡[&quot;text&quot;]â€‡(num_proc=8):â€‡100%"
          }
        },
        "6aef752eb1444413bbc10fc162bf1cd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a6ce517b02049dfa85bc0c1abb5ca23",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14e10bc654c34a9a91b607ca990ebb37",
            "value": 10000
          }
        },
        "983b65e228c04a27bf1f9ebed66bb2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2bccf57c85e4084880fa67a5ff0f832",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_494bd2ca51744245a524a44e33f51c41",
            "value": "â€‡10000/10000â€‡[00:03&lt;00:00,â€‡3963.89â€‡examples/s]"
          }
        },
        "57b68bc2fde044c1ab77606a2a3ba7c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372426a4699c40968078789de6ad1675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4f02b57a2094e82a3ae0d6f4dcc1591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a6ce517b02049dfa85bc0c1abb5ca23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14e10bc654c34a9a91b607ca990ebb37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2bccf57c85e4084880fa67a5ff0f832": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494bd2ca51744245a524a44e33f51c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}