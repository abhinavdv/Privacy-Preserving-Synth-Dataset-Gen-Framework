{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OThers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete models form cache\n",
    "\n",
    "# https://huggingface.co/docs/huggingface_hub/v0.28.1/guides/manage-cache#clean-cache-from-the-terminal\n",
    "# to get revisions: \n",
    "\n",
    "# from huggingface_hub import scan_cache_dir\n",
    "\n",
    "# delete_strategy = scan_cache_dir().delete_revisions(\n",
    "#     \"81fd1d6e7847c99f5862c9fb81387956d99ec7aa\"\n",
    "#     \"e2983b237dccf3ab4937c97fa717319a9ca1a96d\",\n",
    "#     \"d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\",\n",
    "# )\n",
    "# print(\"Will free: \" + delete_strategy.expected_freed_size_str)\n",
    "\n",
    "# delete_strategy.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavduvvuri/Code/Synthetic Data Generation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device = 'mps'\n",
    "\n",
    "#tokenizer is used to convert from text to tokens (a number representation of a word/subword)\n",
    "#each model has its own tokenizer\n",
    "#padding_side = left means, pad to the left when string is smaller than set length for ear sample\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "#we need to explicitly set the pad_token. We set it to the same as end-of-sentence token here.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#loading the model.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Cricket is a 3-year-old male cat who was surrendered to the shelter due to his owner's allergies. He is a beautiful, sleek black\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is a pipeline in transformers. It executes the model for inference.\n",
    "#will return a text generation pipeline\n",
    "generation_pipeline = pipeline(task=\"text-generation\", model= model, tokenizer=tokenizer)\n",
    "generation_pipeline(\"Cricket is a \", max_new_tokens = 25, temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "input_prompt = [\n",
    "    \"Hi there! ssup?\",\n",
    "    \"Hello, how are you? What is going on?\"\n",
    "]\n",
    "\n",
    "#tokenized = tokenizer(input_prompt, return_tensors='pt').to(device) ## all inputs should be the same size after tokenization\n",
    "tokenized = tokenizer(input_prompt, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "print(tokenized[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#inserted a padding token in the beginning (padding side passed to the tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128009, 128009, 128009, 128009, 128009, 128000,  13347,   1070,      0,\n",
      "          11107,    455,     30],\n",
      "        [128000,   9906,     11,   1268,    527,    499,     30,   3639,    374,\n",
      "           2133,    389,     30]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>Hi there! ssup?',\n",
       " '<|begin_of_text|>Hello, how are you? What is going on?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is this attention mask. Tells what all tokens to give attention to (remove attention for padded text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1627,  10263,    220,   2366,     19,    271,   2675,    527,\n",
      "            459,   5505,  15592,  18328,    889,   3727,    264,  22380,  90456,\n",
      "         128009, 128006,    882, 128007,    271,  15546,   1097,    358,     30,\n",
      "         128009, 128006,  78191, 128007,    271]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an useful AI assistant who makes a joke everytime\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Who am I?\"\n",
    "    },\n",
    "\n",
    "]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt_template,\n",
    "    add_generation_prompt = True,\n",
    "    tokenize = True,\n",
    "    padding = True,\n",
    "    return_tensors = \"pt\"\n",
    ").to(device)\n",
    "\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are an useful AI assistant who makes a joke everytime<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who am I?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "You're a mystery, but don't worry, I won't have to \"Google\" all day to figure it out. Seriously though, I don't know who you are, but I'm here to help you discover yourself or answer any questions you might have. What's on your mind?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(tokenized, max_new_tokens=100)\n",
    "decoded = tokenizer.batch_decode(out)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1627,  10263,    220,   2366,     19,    271,   2675,    527,\n",
      "            459,   5505,  15592,  18328,    889,   3727,    264,  22380,  90456,\n",
      "         128009, 128006,    882, 128007,    271,  15546,   1097,    358,     30,\n",
      "         128009, 128006,  78191, 128007,    271,  69112,      0]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "#when we want the output to start with a given sequence\n",
    "prompt_template_2 = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an useful AI assistant who makes a joke everytime\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Who am I?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hola! \"\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt_template_2,\n",
    "    continue_final_message = True, #newly added, removed add_generation_prompt\n",
    "    tokenize = True,\n",
    "    padding = True,\n",
    "    return_tensors = \"pt\"\n",
    ").to(device)\n",
    "\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are an useful AI assistant who makes a joke everytime<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who am I?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hola! I'm not a mind reader, but I can try to guess. You're a curious human who's asking questions, so I'm going to take a wild guess and say... YOU'RE A GENIUS! (Just kidding, but seriously, I don't know, and that's okay!)\n",
      "\n",
      "On a more serious note, I'd love to chat and get to know you better. What brings you here today?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(tokenized, max_new_tokens=100)\n",
    "decoded = tokenizer.batch_decode(out)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at finetuning data (Amazon dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "meta_elec = pd.read_json(\"Cell_Phones_and_Accessories.jsonl\", lines = True, nrows = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (100_000, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>rating</th><th>title</th><th>text</th><th>images</th><th>asin</th><th>parent_asin</th><th>user_id</th><th>timestamp</th><th>helpful_vote</th><th>verified_purchase</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>list[struct[4]]</td><td>str</td><td>str</td><td>str</td><td>datetime[ns]</td><td>i64</td><td>bool</td></tr></thead><tbody><tr><td>4</td><td>&quot;No white background! It’s clea…</td><td>&quot;I bought this bc I thought it …</td><td>[{&quot;IMAGE&quot;,&quot;https://images-na.ssl-images-amazon.com/images/I/B1PrCo7ZjXS._SL1600_.jpg&quot;,&quot;https://images-na.ssl-images-amazon.com/images/I/B1PrCo7ZjXS._SL800_.jpg&quot;,&quot;https://images-na.ssl-images-amazon.com/images/I/B1PrCo7ZjXS._SL256_.jpg&quot;}]</td><td>&quot;B08L6L3X1S&quot;</td><td>&quot;B08L6L3X1S&quot;</td><td>&quot;AFKZENTNBQ7A7V7UXW5JJI6UGRYQ&quot;</td><td>2021-01-30 22:07:31.196</td><td>0</td><td>true</td></tr><tr><td>5</td><td>&quot;Awesome!&nbsp;&nbsp;Great price!&nbsp;&nbsp;Works …</td><td>&quot;Perfect. How pissed am I that …</td><td>[]</td><td>&quot;B079BPGF6C&quot;</td><td>&quot;B079BPGF6C&quot;</td><td>&quot;AFKZENTNBQ7A7V7UXW5JJI6UGRYQ&quot;</td><td>2018-08-16 18:18:37.349</td><td>2</td><td>true</td></tr><tr><td>5</td><td>&quot;Worked but took an hour to ins…</td><td>&quot;Overall very happy with the en…</td><td>[{&quot;IMAGE&quot;,&quot;https://m.media-amazon.com/images/I/B1+g-o0qHKS._SL1600_.jpg&quot;,&quot;https://m.media-amazon.com/images/I/B1+g-o0qHKS._SL800_.jpg&quot;,&quot;https://m.media-amazon.com/images/I/B1+g-o0qHKS._SL256_.jpg&quot;}]</td><td>&quot;B088DR7Z5B&quot;</td><td>&quot;B0BBGGC8F2&quot;</td><td>&quot;AGCI7FAH4GL5FI65HYLKWTMFZ2CQ&quot;</td><td>2021-08-17 21:21:44.798</td><td>3</td><td>true</td></tr><tr><td>4</td><td>&quot;Decent&quot;</td><td>&quot;Lasted about 9 months then the…</td><td>[{&quot;IMAGE&quot;,&quot;https://images-na.ssl-images-amazon.com/images/I/71RgHzZnX3L.jpg&quot;,&quot;https://images-na.ssl-images-amazon.com/images/I/71RgHzZnX3L._SL800_.jpg&quot;,&quot;https://images-na.ssl-images-amazon.com/images/I/71RgHzZnX3L._SL256_.jpg&quot;}]</td><td>&quot;B07XRDHDNQ&quot;</td><td>&quot;B07XRDHDNQ&quot;</td><td>&quot;AGCI7FAH4GL5FI65HYLKWTMFZ2CQ&quot;</td><td>2020-05-26 05:14:42.910</td><td>0</td><td>true</td></tr><tr><td>5</td><td>&quot;LOVE IT!&quot;</td><td>&quot;LOVE THIS CASE! Works better t…</td><td>[]</td><td>&quot;B00A8ZDL9Y&quot;</td><td>&quot;B00A8ZDL9Y&quot;</td><td>&quot;AGCI7FAH4GL5FI65HYLKWTMFZ2CQ&quot;</td><td>2014-08-25 19:23:08</td><td>0</td><td>true</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2</td><td>&quot;I would not buy again.&nbsp;&nbsp;I hope…</td><td>&quot;It kind of works, but is diffi…</td><td>[]</td><td>&quot;B0078XKTTY&quot;</td><td>&quot;B0078XKTTY&quot;</td><td>&quot;AEGTK2K2VEAX7NWK2DLHZBZIYSIQ&quot;</td><td>2014-07-09 19:16:00</td><td>0</td><td>true</td></tr><tr><td>5</td><td>&quot;Highly recommend&quot;</td><td>&quot;Great product.&nbsp;&nbsp;Base is wide e…</td><td>[]</td><td>&quot;B08HCPHP9W&quot;</td><td>&quot;B08KXNGLKK&quot;</td><td>&quot;AFEUZ4RUAKTWJRJIWZ4FFFTVGT6Q&quot;</td><td>2020-12-01 17:52:42.954</td><td>0</td><td>true</td></tr><tr><td>5</td><td>&quot;Five Stars&quot;</td><td>&quot;Wife loves it.&quot;</td><td>[]</td><td>&quot;B01ISS2F3U&quot;</td><td>&quot;B07CK19G7X&quot;</td><td>&quot;AFZEWW6S4NFQEXQU24MGWSCX2Z6A&quot;</td><td>2017-02-22 00:24:01</td><td>0</td><td>true</td></tr><tr><td>4</td><td>&quot;Good product but ring attachme…</td><td>&quot;There was no fingerprint reade…</td><td>[]</td><td>&quot;B082TQ42Y8&quot;</td><td>&quot;B082TQ42Y8&quot;</td><td>&quot;AH7TDENCUVDOYHIMVPLRBS3G2AKA&quot;</td><td>2022-09-22 22:24:14.068</td><td>0</td><td>true</td></tr><tr><td>1</td><td>&quot;Too big for my iPhone 11.&quot;</td><td>&quot;This screen protector might ha…</td><td>[]</td><td>&quot;B07GZSDHSK&quot;</td><td>&quot;B0BST1WHNG&quot;</td><td>&quot;AH7TDENCUVDOYHIMVPLRBS3G2AKA&quot;</td><td>2020-10-29 16:23:24.430</td><td>1</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (100_000, 10)\n",
       "┌────────┬────────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ rating ┆ title      ┆ text       ┆ images    ┆ … ┆ user_id   ┆ timestamp ┆ helpful_v ┆ verified_ │\n",
       "│ ---    ┆ ---        ┆ ---        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ote       ┆ purchase  │\n",
       "│ i64    ┆ str        ┆ str        ┆ list[stru ┆   ┆ str       ┆ datetime[ ┆ ---       ┆ ---       │\n",
       "│        ┆            ┆            ┆ ct[4]]    ┆   ┆           ┆ ns]       ┆ i64       ┆ bool      │\n",
       "╞════════╪════════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 4      ┆ No white   ┆ I bought   ┆ [{\"IMAGE\" ┆ … ┆ AFKZENTNB ┆ 2021-01-3 ┆ 0         ┆ true      │\n",
       "│        ┆ background ┆ this bc I  ┆ ,\"https:/ ┆   ┆ Q7A7V7UXW ┆ 0 22:07:3 ┆           ┆           │\n",
       "│        ┆ ! It’s     ┆ thought it ┆ /images-n ┆   ┆ 5JJI6UGRY ┆ 1.196     ┆           ┆           │\n",
       "│        ┆ clea…      ┆ …          ┆ a.s…      ┆   ┆ Q         ┆           ┆           ┆           │\n",
       "│ 5      ┆ Awesome!   ┆ Perfect.   ┆ []        ┆ … ┆ AFKZENTNB ┆ 2018-08-1 ┆ 2         ┆ true      │\n",
       "│        ┆ Great      ┆ How pissed ┆           ┆   ┆ Q7A7V7UXW ┆ 6 18:18:3 ┆           ┆           │\n",
       "│        ┆ price!     ┆ am I that  ┆           ┆   ┆ 5JJI6UGRY ┆ 7.349     ┆           ┆           │\n",
       "│        ┆ Works …    ┆ …          ┆           ┆   ┆ Q         ┆           ┆           ┆           │\n",
       "│ 5      ┆ Worked but ┆ Overall    ┆ [{\"IMAGE\" ┆ … ┆ AGCI7FAH4 ┆ 2021-08-1 ┆ 3         ┆ true      │\n",
       "│        ┆ took an    ┆ very happy ┆ ,\"https:/ ┆   ┆ GL5FI65HY ┆ 7 21:21:4 ┆           ┆           │\n",
       "│        ┆ hour to    ┆ with the   ┆ /m.media- ┆   ┆ LKWTMFZ2C ┆ 4.798     ┆           ┆           │\n",
       "│        ┆ ins…       ┆ en…        ┆ ama…      ┆   ┆ Q         ┆           ┆           ┆           │\n",
       "│ 4      ┆ Decent     ┆ Lasted     ┆ [{\"IMAGE\" ┆ … ┆ AGCI7FAH4 ┆ 2020-05-2 ┆ 0         ┆ true      │\n",
       "│        ┆            ┆ about 9    ┆ ,\"https:/ ┆   ┆ GL5FI65HY ┆ 6 05:14:4 ┆           ┆           │\n",
       "│        ┆            ┆ months     ┆ /images-n ┆   ┆ LKWTMFZ2C ┆ 2.910     ┆           ┆           │\n",
       "│        ┆            ┆ then the…  ┆ a.s…      ┆   ┆ Q         ┆           ┆           ┆           │\n",
       "│ 5      ┆ LOVE IT!   ┆ LOVE THIS  ┆ []        ┆ … ┆ AGCI7FAH4 ┆ 2014-08-2 ┆ 0         ┆ true      │\n",
       "│        ┆            ┆ CASE!      ┆           ┆   ┆ GL5FI65HY ┆ 5         ┆           ┆           │\n",
       "│        ┆            ┆ Works      ┆           ┆   ┆ LKWTMFZ2C ┆ 19:23:08  ┆           ┆           │\n",
       "│        ┆            ┆ better t…  ┆           ┆   ┆ Q         ┆           ┆           ┆           │\n",
       "│ …      ┆ …          ┆ …          ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 2      ┆ I would    ┆ It kind of ┆ []        ┆ … ┆ AEGTK2K2V ┆ 2014-07-0 ┆ 0         ┆ true      │\n",
       "│        ┆ not buy    ┆ works, but ┆           ┆   ┆ EAX7NWK2D ┆ 9         ┆           ┆           │\n",
       "│        ┆ again.  I  ┆ is diffi…  ┆           ┆   ┆ LHZBZIYSI ┆ 19:16:00  ┆           ┆           │\n",
       "│        ┆ hope…      ┆            ┆           ┆   ┆ Q         ┆           ┆           ┆           │\n",
       "│ 5      ┆ Highly     ┆ Great      ┆ []        ┆ … ┆ AFEUZ4RUA ┆ 2020-12-0 ┆ 0         ┆ true      │\n",
       "│        ┆ recommend  ┆ product.   ┆           ┆   ┆ KTWJRJIWZ ┆ 1 17:52:4 ┆           ┆           │\n",
       "│        ┆            ┆ Base is    ┆           ┆   ┆ 4FFFTVGT6 ┆ 2.954     ┆           ┆           │\n",
       "│        ┆            ┆ wide e…    ┆           ┆   ┆ Q         ┆           ┆           ┆           │\n",
       "│ 5      ┆ Five Stars ┆ Wife loves ┆ []        ┆ … ┆ AFZEWW6S4 ┆ 2017-02-2 ┆ 0         ┆ true      │\n",
       "│        ┆            ┆ it.        ┆           ┆   ┆ NFQEXQU24 ┆ 2         ┆           ┆           │\n",
       "│        ┆            ┆            ┆           ┆   ┆ MGWSCX2Z6 ┆ 00:24:01  ┆           ┆           │\n",
       "│        ┆            ┆            ┆           ┆   ┆ A         ┆           ┆           ┆           │\n",
       "│ 4      ┆ Good       ┆ There was  ┆ []        ┆ … ┆ AH7TDENCU ┆ 2022-09-2 ┆ 0         ┆ true      │\n",
       "│        ┆ product    ┆ no fingerp ┆           ┆   ┆ VDOYHIMVP ┆ 2 22:24:1 ┆           ┆           │\n",
       "│        ┆ but ring   ┆ rint       ┆           ┆   ┆ LRBS3G2AK ┆ 4.068     ┆           ┆           │\n",
       "│        ┆ attachme…  ┆ reade…     ┆           ┆   ┆ A         ┆           ┆           ┆           │\n",
       "│ 1      ┆ Too big    ┆ This       ┆ []        ┆ … ┆ AH7TDENCU ┆ 2020-10-2 ┆ 1         ┆ true      │\n",
       "│        ┆ for my     ┆ screen     ┆           ┆   ┆ VDOYHIMVP ┆ 9 16:23:2 ┆           ┆           │\n",
       "│        ┆ iPhone 11. ┆ protector  ┆           ┆   ┆ LRBS3G2AK ┆ 4.430     ┆           ┆           │\n",
       "│        ┆            ┆ might ha…  ┆           ┆   ┆ A         ┆           ┆           ┆           │\n",
       "└────────┴────────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_elec_pl = pl.DataFrame(meta_elec)\n",
    "meta_elec_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Assuming meta_elec is a pandas DataFrame with the relevant columns\n",
    "\n",
    "for index, row in meta_elec.iterrows():\n",
    "    rating, text, title = row['rating'], row['text'], row['title']\n",
    "    \n",
    "    # Create the dictionary for each line of JSONL\n",
    "    jsonl_data_format_input = {\n",
    "        'System prompt': 'Given the Rating and Title, you are required to generate the review',\n",
    "        'Rating': rating,\n",
    "        'Title': title,\n",
    "        'Review': text\n",
    "    }\n",
    "    \n",
    "    # Open file in append mode and write the dictionary as a JSON line\n",
    "    with open(\"train.jsonl\", \"a\") as f:\n",
    "        json.dump(jsonl_data_format_input, f)\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a review rating predictor. Based on user's review provided, you predict the rating the user would give out of 5. Return only the number of the rating between 0 and 5.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I bought this bc I thought it had the nice white background. Turns out it’s clear & since my phone is blue it doesn’t look anything like this.  If I had known that I would have purchased something else. It works ok.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Rating is:\"\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt_template,\n",
    "    continue_final_message = True, #newly added, removed add_generation_prompt\n",
    "    tokenize = True,\n",
    "    padding = True,\n",
    "    return_tensors = \"pt\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a review rating predictor. Based on user's review provided, you predict the rating the user would give out of 5. Return only the number of the rating between 0 and 5.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I bought this bc I thought it had the nice white background. Turns out it’s clear & since my phone is blue it doesn’t look anything like this.  If I had known that I would have purchased something else. It works ok.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Rating is: 2<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(tokenized, max_new_tokens=100)\n",
    "decoded = tokenizer.batch_decode(out)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the next word - Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is this causal attention things during inference. We need to give the input tokens, then one token is given as output. We feed the (input + one output token) as the next input to get token2 and then (inp + token1 + token2) -> token3 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"Hi there!\", \"Who are you? What's up?\"]\n",
    "tokenized = tokenizer(sentence, return_tensors=\"pt\", padding=True)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128009, 128009, 128009, 128009, 128009, 128000,  13347,   1070,      0],\n",
      "        [128000,  15546,    527,    499,     30,   3639,    596,    709,     30]])\n",
      "['<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>Hi there!', \"<|begin_of_text|>Who are you? What's up?\"]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized)\n",
    "print(tokenizer.batch_decode(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids:  tensor([[128009, 128009, 128009, 128009, 128009, 128000,  13347,   1070],\n",
      "        [128000,  15546,    527,    499,     30,   3639,    596,    709]])\n",
      "Target ids:  tensor([[128009, 128009, 128009, 128009, 128000,  13347,   1070,      0],\n",
      "        [ 15546,    527,    499,     30,   3639,    596,    709,     30]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenized[:, :-1]\n",
    "target_ids = tokenized[:, 1:]\n",
    "\n",
    "print(\"Input ids: \",input_ids)\n",
    "print(\"Target ids: \",target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 2675, 527, 264, 3477, 10959, 62254, 13, 20817, 389, 1217, 596, 3477, 3984, 11, 499, 7168, 279, 10959, 279, 1217, 1053, 3041, 704, 315, 220, 20, 13, 3494, 1193, 279, 1396, 315, 279, 10959, 1990, 220, 15, 323, 220, 20, 13, 128009, 128006, 882, 128007, 271, 40, 11021, 420, 18399, 358, 3463, 433, 1047, 279, 6555, 4251, 4092, 13, 58334, 704, 433, 753, 2867, 612, 2533, 856, 4641, 374, 6437, 433, 3250, 1431, 1427, 4205, 1093, 420, 13, 220, 1442, 358, 1047, 3967, 430, 358, 1053, 617, 15075, 2555, 775, 13, 1102, 4375, 5509, 13, 128009, 128006, 78191, 128007, 271, 22940, 374, 25]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a review rating predictor. Based on user's review provided, you predict the rating the user would give out of 5. Return only the number of the rating between 0 and 5.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I bought this bc I thought it had the nice white background. Turns out it’s clear & since my phone is blue it doesn’t look anything like this.  If I had known that I would have purchased something else. It works ok.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Rating is:\"\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "answer = \"4.0\"\n",
    "\n",
    "chat_template = tokenizer.apply_chat_template(prompt_template, continue_final_message=True, tokenize = True)\n",
    "print(chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'column_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m      3\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,  \u001b[38;5;66;03m# rank of the low-rank adaptation\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m      5\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Prepare SFTTrainer\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_elec_pl\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Fine-tune\u001b[39;00m\n\u001b[1;32m     16\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Code/Synthetic Data Generation/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/Synthetic Data Generation/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:265\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PartialState()\u001b[38;5;241m.\u001b[39mlocal_main_process_first():\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_of_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchars_per_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m         _multiple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mdict\u001b[39m)\n",
      "File \u001b[0;32m~/Code/Synthetic Data Generation/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:380\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_dataset\u001b[0;34m(self, dataset, processing_class, packing, dataset_text_field, max_seq_length, formatting_func, num_of_sequences, chars_per_token, remove_unused_columns, append_concat_token, add_special_tokens, skip_prepare_dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m packing:\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_non_packed_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_packed_dataloader(\n\u001b[1;32m    392\u001b[0m         processing_class,\n\u001b[1;32m    393\u001b[0m         dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m         add_special_tokens,\n\u001b[1;32m    401\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/Synthetic Data Generation/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:434\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_non_packed_dataloader\u001b[0;34m(self, processing_class, dataset, dataset_text_field, max_seq_length, formatting_func, add_special_tokens, remove_unused_columns)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    432\u001b[0m signature_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# None for IterableDataset\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     extra_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mcolumn_names) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(signature_columns))\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'column_names'"
     ]
    }
   ],
   "source": [
    "# Set up LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # rank of the low-rank adaptation\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "\n",
    "# Prepare SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=meta_elec_pl\n",
    ")\n",
    "\n",
    "# Fine-tune\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No ':' found when decoding object value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/Synthetic Data Generation/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/Synthetic Data Generation/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:1023\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m   1022\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/Code/Synthetic Data Generation/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/Code/Synthetic Data Generation/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/Synthetic Data Generation/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:1403\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1403\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m     )\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1406\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1407\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1409\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: No ':' found when decoding object value"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_json(\"train.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
