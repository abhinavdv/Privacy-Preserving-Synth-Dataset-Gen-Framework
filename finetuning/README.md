This is the file where we explore fine-tuning LLM both with and without Differential Privacy. 

Fine tuning with differntial privacy resues ideas from the paper: [Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe](https://arxiv.org/abs/2210.14348)

and looked code in: [DP-Transformer](https://github.com/microsoft/dp-transformers)

Initial finetuning code credits: https://www.youtube.com/watch?v=bZcKYiwtw1I